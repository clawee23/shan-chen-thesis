# Abstract
*Chapter 1*
*General Introduction, Outline and Glossary*
Recent advances in artificial intelligence are reshaping healthcare, driven by the rapid emergence of large language models (LLMs) that exhibit unprecedented fluency, reasoning, and adaptability. Early generative models such as GPT-2 and GPT-3 demonstrated that scale and unsupervised pre-training could unlock broad generalization capabilities previously unattainable in clinical NLP systems[1,2](https://paperpile.com/c/coPcdt/Lqak+5MlW). These breakthroughs were enabled by decoder-only transformer architectures[3](https://paperpile.com/c/coPcdt/yTOb) and further refined through instruction-following and reinforcement learning from human feedback[4](https://paperpile.com/c/coPcdt/3lDg), which significantly improved model controllability and task alignment. More recently, frontier systems such as GPT-4 continue to expand the scope of clinical tasks LLMs can perform, from summarization to reasoning over complex medical content[5](https://paperpile.com/c/coPcdt/UlT6).

Simultaneously, medicine itself has become increasingly digitized. Electronic health records, clinical trial documents, biomedical literature, and patient-generated data now form vast corpora on which these models are trained and in which they are deployed. As a result, LLMs have transitioned rapidly from research prototypes to operational tools embedded in real clinical workflows. They are already assisting with patient-provider communication[6](https://paperpile.com/c/coPcdt/uPo4), generating and summarizing clinical notes[7–9](https://paperpile.com/c/coPcdt/BsvC+knyy+rVYo), extracting structured information from unstructured data[10](https://paperpile.com/c/coPcdt/83yK), and even encoding substantial clinical knowledge comparable to domain-trained models[11,12](https://paperpile.com/c/coPcdt/DkR2+7xsh). This shift marks a profound change in the epistemic infrastructure of medicine: for the first time, clinicians routinely interact with probabilistic generative systems capable of producing medical text at scale.

However, the same generative fluency that makes LLMs attractive for healthcare applications also introduces substantial risks. Studies consistently show that LLMs can hallucinate nonexistent treatments, misjudge clinical acuity, or propagate biases embedded in their training data[13–19](https://paperpile.com/c/coPcdt/SrkJ+X0L4+IXst+d6qh+RJ2T+k02J+rgaJ). Even small perturbations in phrasing, demographic attributes, or contextual cues can produce significant shifts in output quality \- raising concerns about stability, robustness, and equity. These vulnerabilities are particularly concerning given early evidence that patients may overly trust model-generated text and that health systems are beginning to integrate LLMs into triage, decision support, and documentation pipelines. The risk is not hypothetical: inappropriate advice, misaligned summaries, and sycophantic agreement with false statements have already been documented in controlled evaluations[19,20](https://paperpile.com/c/coPcdt/rgaJ+kiiZ).

These developments make clear that the healthcare community faces a dual imperative: **to rigorously identify where LLMs provide measurable value, and to expose and mitigate the conditions under which they may cause harm**. Overemphasizing capability risks premature deployment of unsafe tools; overemphasizing failure risks forfeiting technologies with genuine potential to reduce clinician burden, surface invisible inequities, and enhance evidence generation. This thesis embraces both dimensions.
Part I** investigates domains where LLMs can augment clinical expertise. Our work has shown that models can encode substantial clinical knowledge, assist in patient communication, and extract valuable signals from narrative text that traditional analytics overlook \- such as social determinants of health or treatment-related toxicities. The studies presented here also extend through rigorous, workflow-embedded evaluations, demonstrating that when aligned, fine-tuned, and rigorously validated, LLMs can surface clinically meaningful information at scale and support more equitable clinical operations.

![][image1]

Part II** interrogates the foundational limitations that emerge when LLMs confront clinically related tasks. Building on recent evidence of bias, misalignment, and brittleness, these chapters reveal how pre-training distributions, lexical fragility, and conversational alignment pressures can systematically distort medical predictions. They also show that even state-of-the-art models remain vulnerable to hallucination and sycophantic compliance, posing risks for clinical deployment.

Together, these investigations argue that LLMs should not be deployed as black-box oracles but rather as **transparent, auditable, and domain-aligned clinical instruments**. Achieving this vision requires interdisciplinary collaboration, last-mile alignment grounded in local data and workflows, and rigorous, open evaluation frameworks such as HealthBench[21](https://paperpile.com/c/coPcdt/ESmj) and model-specific benchmarks for medical tasks[7,22,23](https://paperpile.com/c/coPcdt/BsvC+9dPU+Scnx). By synthesizing both the promise and the pitfalls of clinical LLMs, this thesis outlines a principled path toward AI systems that strengthen the safety, equity, and integrity of global healthcare.
Part I: Potential Utilities of Language Models in Real Clinical Practice
The first part of this thesis investigates how LLMs can be harnessed to solve practical, high-impact problems across the clinical ecosystem: from direct patient communication to surfacing hidden determinants of health.

We begin at the patient interface. **Chapter 2** examines the urgent problem of medical misinformation: cancer patients increasingly turn to general-purpose chatbots like ChatGPT for treatment advice, yet these systems lack clinical grounding. We systematically evaluated ChatGPT’s ability to generate guideline-concordant recommendations across 104 oncologic scenarios, finding that while the model always produced at least one correct suggestion, over one-third of responses included potentially harmful errors and 12% hallucinated entire treatment modalities. This study establishes a baseline risk profile for patient-facing LLMs and underscores why unfiltered deployment in consumer health contexts is dangerous without rigorous validation and safeguards.

Moving to the provider workflow, **Chapter 3** investigates LLMs as clinical scribes for patient portal messages—a use case now live in major EHR platforms. By having oncologists use GPT-4 to draft responses to realistic patient messages, we uncovered a tension between efficiency and safety: while AI assistance shortened documentation time, unedited drafts posed a 7.7% risk of severe harm, primarily from misjudging clinical urgency. This work reveals that human oversight remains non-negotiable; the value of LLMs lies not in autonomous drafting but in augmenting physicians, provided they remain vigilant to subtle but critical errors.

Shifting from communication to data extraction, **Chapter 4** and **Chapter 5** demonstrate how LLMs can unlock information trapped in unstructured clinical notes. Radiotherapy-induced esophagitis is a debilitating toxicity that is routinely documented in free text but rarely captured for research. We developed NLP models that automatically grade esophagitis severity with high fidelity, enabling large-scale real-world evidence generation on treatment complications. Similarly, social determinants of health (SDoH)—powerful predictors of outcomes—are notoriously absent from structured data. In **Chapter 5**, we show that fine-tuned LLMs can extract SDoH from clinical narratives far more accurately than billing codes, identifying nearly 94% of patients with adverse social circumstances versus 2% via traditional methods. Critically, these models exhibit less demographic bias than generalist LLMs, offering a path toward more equitable data-driven care.

These extraction capabilities culminate in **Chapter 6**, which introduces iRAE Agent(AEGIS), an agentic system piloted live at Mass General Brigham for immunotherapy-related adverse event detection in oncology clinical trials. Unlike passive classification, iRAE orchestrates multi-step reasoning, error checking, and ontology-guided reporting within live hospital workflows—addressing a labor-intensive process that directly impacts trial safety and cost. This pilot represents the frontier of clinical LLM deployment: moving from isolated tasks to integrated, autonomous agents that solve real, resource-constrained problems in cancer care.
Part II: Potential Fails of Language Models in Medical Settings
If Part I demonstrates what LLMs can do, Part II investigates what can go wrong—exposing latent vulnerabilities that become dangerous when models are deployed in high-stakes medical environments.
Chapter 7** establishes a performance ceiling by comparing generalist LLMs against traditional biomedical NLP methods. On structured tasks like health advice classification and causal reasoning, few-shot prompting of ChatGPT approached but never exceeded the accuracy of fine-tuned, domain-specific models like BioBERT \- while requiring a thousandfold more computational time. This finding is pivotal: it demonstrates that for many clinical NLP tasks, the “generalist advantage” is a myth. Locally tuned models are not only more accurate but also cheaper, faster to deploy, and easier to integrate into existing workflows, making them the pragmatic choice for most biomedical applications.

The next three chapters peel back layers of hidden fragility rooted in pre-training bias.
Chapter 8** introduces Cross‑Care, a benchmark quantifying discordance between LLM-encoded disease-demographic associations and true epidemiology. Across 89 diseases and multiple demographic and language groups, we found that LLM predictions correlate strongly with how often disease-demographic pairs appear in pre-training corpora \- but show virtually no alignment with actual prevalence. This representational bias persists even after alignment tuning and across languages.
Chapter 9** reveals how this bias manifests as lexical fragility: swapping brand and generic drug names (semantically equivalent variations) caused performance drops of 1–13% across state-of-the-art models. This suggests that even when models “know” A=B, they cannot reliably manipulate this knowledge—they rely on memorized patterns from training data rather than robust reasoning.
Chapter 10** extends this to sycophantic behavior, where models comply with factually incorrect drug equivalence claims up to 100% of the time to appear helpful. Together, these chapters expose a systemic vulnerability: LLMs encode the statistical idiosyncrasies of their training data, not grounded medical knowledge, and they default to helpfulness over truth when these conflict.
Chapter 11** and **Chapter 12** scale evaluation efficiently to track real progress on emerging capabilities using existing resources. **Chapter 11** presents WorldMedQA‑V, a multilingual, multimodal medical exam dataset spanning four countries and languages. Rather than simply exposing disparities, this benchmark provides a scalable tool to rigorously track whether vision-language model improvements truly generalize across diverse linguistic contexts—or merely deepen global health inequities. **Chapter 12** introduces MedBrowseComp, a benchmark for medical deep research requiring multi-hop evidence synthesis from live knowledge bases. By forcing agents to navigate fragmented clinical evidence, MedBrowseComp offers a durable framework to measure genuine progress toward safe, evidence-based AI \- exposing that even state-of-the-art systems achieve only \~10% accuracy on complex questions, far below clinical requirements.
Chapter 13** synthesizes these threads into a cohesive agenda: advancing clinical LLMs requires parallel investment in utility and safety. Our findings reveal that each success in Part I: whether extracting toxicities, surfacing SDoH, or enabling trial workflows—also surfaces deeper, domain-general problems: the brittleness of knowledge manipulation, the persistence of pre-training bias, the misalignment of helpfulness and truthfulness, and the inadequacy of current evaluation paradigms. We outline priorities including prevalence-aware pre-training, robust evaluation frameworks that test for bias and fragility, alignment strategies that prioritize factual correctness over helpfulness, and agent architectures grounded in verifiable evidence. The appendices document broader NLP contributions such as open-source medical LLMs, interpretability methods, and multilingual reasoning that strengthen the foundations of this field.

Together, this thesis provides an empirically grounded, dual-narrative examination of LLMs in medicine: celebrating their potential to transform cancer care while unflinchingly exposing the vulnerabilities that must be resolved before they can be trusted at scale.
