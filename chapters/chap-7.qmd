# Chapter 7

Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification

---***Shan Chen****, Yingya Li*, Sheng Lu, Hoang Van, Hugo JWL Aerts,

Guergana K Savova, Danielle S Bitterman

Journal of the American Medical Informatics Association

**Summary**
**Background** Large language models (LLMs) such as ChatGPT (GPT-3.5, GPT-4) demonstrate strong general question-answering ability but remain under-explored for specialized biomedical applications. Understanding their performance in structured biomedical tasks is essential before clinical or research deployment.**Methods** Using 11,122 samples, our study evaluated ChatGPT-family models on two core biomedical natural language processing tasks:**classification** of health advice in scientific literature (n = 8676) and**reasoning** via detection of causal relations (n = 2446). Prompts were developed under zero-shot and few-shot settings, with and without chain-of-thought (CoT) reasoning. The best prompts from each configuration were compared against two baselines: a simple bag-of-words (BoW) logistic regression model and fine-tuned BioBERT models.**Findings** Fine-tuned BioBERT achieved the highest F1 scores for both classification (0.800–0.902) and reasoning (0.851). Among LLMs, few-shot CoT prompting produced the strongest results—classification F1 0.671–0.770 and reasoning F1 0.682—comparable to the BoW baseline (F1 0.602–0.753 and 0.675, respectively). However, achieving optimal LLM performance required**78 hours**, compared with only**0.078 hours** for BioBERT and**0.008 hours** for BoW.**Interpretation** Despite the popularity of ChatGPT, task-specific fine-tuning of domain-adapted models like BioBERT remains the most effective and efficient strategy for biomedical NLP. LLM prompting can approach—but not surpass—traditional methods, while requiring substantially more computational time and engineering effort.

## Introduction

The advancements in machine learning (ML) methods for natural language process (NLP), such as transformers[1](https://paperpile.com/c/FgFYMN/OA2QZ) and reinforcement learning[2](https://paperpile.com/c/FgFYMN/EWdNo), in combination with abundant digital text and scaled-up hardware capabilities has led to many pretrained large language models (LLMs) — also referred to as foundation models. Coupling some of these LLMs with smart engineering gave the world the viral ChatGPT, which in turn popularized the technology and re-invigorated the artificial intelligence (AI)/artificial general intelligence (AGI) debate. Although most LLMs are trained as chatbots, some of the claims in the mainstream media go as far as stating that the LLMs are sentient, even able to solve tasks that previously required a high level of human expertise and specialized training. On the other hand, the scientific papers describing the LLMs are much more measured[3](https://paperpile.com/c/FgFYMN/yZqXo) outlining limitations: *“… Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement.”*[*4*](https://paperpile.com/c/FgFYMN/Ki4xv) Therefore the scientific community bears the responsibility of understanding the LLMs’ strengths and weaknesses, how their limitations and risks can be managed, and implications for our future.[5,6](https://paperpile.com/c/FgFYMN/bupsp+nlsHV)

Medicine is one of the highest-stakes domains for LLMs. The excitement surrounding the LLMs has penetrated the biomedical and clinical communities motivating various early use-case evaluations. For example, studies have shown that ChatGPT can pass the US Medical Licensing Examination (USMLE).[7,8](https://paperpile.com/c/FgFYMN/6p9xd+Kdn6v) Zuccon and Koopman[9](https://paperpile.com/c/FgFYMN/HuOMB) investigate the effect of prompts on ChatGPT in answering complex health information questions. Chen et al.[10](https://paperpile.com/c/FgFYMN/V4C6) evaluate the performance and robustness of ChatGPT in providing cancer treatment recommendations that align with National Comprehensive Cancer Network (NCCN) guidelines. Lyu et al.[11](https://paperpile.com/c/FgFYMN/LOqlU) research the feasibility of using ChatGPT to translate radiology reports into plain language for patients and healthcare providers. A paper by Google Research and DeepMind[12](https://paperpile.com/c/FgFYMN/Ah0nd) presents experiments with Google’s PaLM family of LLMs, suggesting the potential utility of LLMs in medicine but also revealing important limitations, reinforcing the importance of evaluation frameworks and methods development.

In parallel, the practical use of LLMs is limited by their huge size and computational requirements, limiting accessibility for most healthcare practices and researchers. Thus, researchers have been pursuing broader questions such as the utility of specialized clinical models, especially ones that are smaller and thus computationally affordable, in the LLM era. Lehman et al.[13](https://paperpile.com/c/FgFYMN/lDUlL) show that relatively small specialized clinical models substantially outperform bigger LLMs, even when fine-tuned on limited annotated data. In addition, they show that pretraining on clinical datasets allows for smaller, more parameter-efficient models that either match or outperform the much bigger computationally hungry LLMs. Wang et al.[14](https://paperpile.com/c/FgFYMN/VfLjI) focus on exploring ChatGPT robustness where a medical diagnosis dataset represents out-of-domain distributions. Results are consistent with Lehman at al.[13](https://paperpile.com/c/FgFYMN/lDUlL)

We set out to contribute to the growing understanding of LLMs in the biomedical domain, with a focus on practical end-use. NLP research on the clinical narrative within the Electronic Medical Records (EMR) has direct applications to translational science[15](https://paperpile.com/c/FgFYMN/Q4Xhw), clinical decision support,[16](https://paperpile.com/c/FgFYMN/ECcXS) and healthcare administration[17](https://paperpile.com/c/FgFYMN/9Zs2K) in addition to direct patient care. Two fundamental NLP tasks to support these applications are classification (e.g. patient phenotyping) and reasoning (e.g. adverse events of medications), and understanding how LLMs handle these tasks will provide insight into optimal uses of these methods. Thus, we aim to evaluate the state-of-the-art (SOTA) LLM performance on classification and reasoning tasks requiring understanding of contextual nuances. While numerous LLMs have been trained in recent years, most are proprietary and not publicly available for a local download (e.g., GPT-3, ChatGPT), which precludes their evaluation on clinical datasets containing Protected Health Information (PHI) data, even if the data are de-identified. Therefore, we work with proxy biomedical data. We evaluate LLMs within the constraints of the typical user to understand their real-world utility, using the OpenAI API[18](https://paperpile.com/c/FgFYMN/l2nGi) and LLMs that are computationally feasible for the IT capabilities of most hospitals and clinical practice.

**METHOD**
**2.1 Tasks and datasets**

We examine the performance of LLMs on two fundamental tasks in the clinical domain – classification and reasoning. Specifically, we select two open datasets annotated for health advice and causal language to test the ability of the models to classify and reason over medical literature findings, and their implications for health-related practices.

-**Classification task: HealthAdvice**[19](https://paperpile.com/c/FgFYMN/lfPHu) is a dataset consisting of annotations of 10,000+ sentences extracted from abstracts and discussion/conclusion sections of medical research literature. The dataset adopts a multi-dimensional taxonomy and categorizes each sentence into “no advice”, “weak advice”, and “strong advice” to capture the occurrence and strength of clinical and policy recommendations. As health advice normally appears in either abstracts or discussion/conclusion sections and its language style may vary across different sections, the labels are further separated into three datasets: *advice in discussion sections, advice in unstructured abstracts, and advice in structured abstracts*.
- **Reasoning task: CausalRelation**[20](https://paperpile.com/c/FgFYMN/uPu1p) is a multi-label reasoning dataset with the goal to identify correlational and causal claims in the findings of medical research literature. The annotated corpus includes over 3,000 PubMed research conclusion sentences extracted from abstracts. Each sentence is labeled as “correlational”, “conditional causal”, “direct causal”, and “no relationship” by its certainty and reasoning type.

Appendix Table B1 and B2 show the dataset distributions. We split the two datasets into development and test sets. The development set is a proportionate sample of 20% the original dataset, while the remaining 80% of the dataset is used as the test set. Final evaluation is performed on the test set.***Figure 1. ****Examples of the prompt templates used for the classification and reasoning tasks.**** ****Prompt templates were created for each of the prompting settings evaluated:****(A)**** zero-shot,****(B)****zero-shot with Chain of Thought (CoT),****(C)**** one-shot and few-shot, and****(D)**** one-shot and few-shot with CoT. “…more exemplars…” (highlighted in green) were added only for the few-shot with and without CoT settings. All evaluated prompts are in the Appendix B. BoW = bag-of-words.*

[Original Context] = Georgian public health specialists working in the HIV field should prioritize implementation of such interventions among HIV patients.

[Question] = Is this a 0) no advice, 1) weak advice or 2) strong advice?

[Answer] = 2) Strong advice.

[CoT Solution] =

1) The statement is a directive, so it is a strong advice.

2) The statement is specific and clear, so it is not a weak advice.

3) Therefore, the statement is a strong advice.

Answer: 2) Strong advice.

![](../figures/image26.png)![](../figures/image57.png)

![](../figures/image42.png)![](../figures/image89.png)

**2.1 Baseline models**

We compare the performance of LLMs with classic ML approaches and transformer-based pretrained language models. For the classic ML approach, we train logistic regression fitted with Stochastic Gradient Descent (SGD), using bag-of-words (BoW) representations with tf-idf as the vectorization method. We fine-tune BioBERT models, given that BERT-based pre-trained language models[21](https://paperpile.com/c/FgFYMN/nut9V), particularly BioBERT[22](https://paperpile.com/c/FgFYMN/QLAXk), have exhibited their efficacy on the aforementioned two tasks (Hyperparameter settings are in the Appendix Table A1). The baseline models are trained and fine-tuned on the full development set and tested on the test set. To further examine the effect of the amount of data on model performance, we trained and tested the BoW and BioBERT models with the 20%, 50%, and 100% of the development set. We track the time required to develop and evaluate the BoW and BioBERT models.**2.2 ChatGPT Family of Models**

We evaluate GPT-4 and its predecessors, including GPT-3.5-Turbo (20B) and GPT-Davinci-003 (175B), on the two tasks. For GPT-3.5 models, we consider zero-shot, one-shot, and few-shot prompting with and without Chain-of-Thought (CoT). Given computational cost limits, for GPT-4, we consider zero-shot (the simplest prompting strategy mimicking an average user) and few-show with CoT (the most complex prompting strategy). CoT techniques explicitly outline the intermediate reasoning steps as prompts to LLMs to elicit multi-step reasoning behavior.[23](https://paperpile.com/c/FgFYMN/bHjK3)

To design a prompt, we follow the prompt structure applied in prior studies.[23](https://paperpile.com/c/FgFYMN/bHjK3) Fig 1 shows an example of the prompt templates for the classification task. For the zero-shot settings, five of the authors each independently developed two prompts. For the one- and few-shot settings without CoT, exemplars are chosen directly from the development set. For the one- and few-shot settings with CoT, the same five authors independently wrote CoT prompts for exemplars from each of the datasets. To evaluate model efficiency, we track the total time spent on designing prompts. Given the different number of classes in the health advice (3 classes) and causal language (4 classes) datasets, we apply 3- and 4-shot exemplars for the few-shot settings for the classification and reasoning tasks respectively. We use regular expressions to match the model output to labels in the datasets, and conduct validation to verify the accuracy of the regular expressions. To assess the model’s performance, we compare the prediction results against the gold annotations in the datasets. Performance of the prompts was initially evaluated on the development set, and the best performing prompt was selected for the final evaluation on the test dataset. Performance across exemplars from different classes was also evaluated. The full set of evaluated prompts are included in the Appendix Table B3 and B4.

We use the OpenAI Application Programming Interface (API) to run the prediction and measure the performance of the models based on the averaged macro-F1 score on 4-fold cross validation on the test set. F1 score is a classic NLP metric representing the harmonic mean of recall/sensitivity and precision/positive predictive value. Macro F1 score is computed using the arithmetic mean of all per-class F1 scores. For comparison of model efficiency, we track the time and cost for running the inference using the API (Appendix Table A2-A4).

**2.3 Smaller LLMs**

The same settings (zero- and few-shot with and without CoT) were used to evaluate the performance** **of select smaller LLMs (less than 10B parameters) on the tasks, including GPT-J, GPT-JT, and Galactica[24](https://paperpile.com/c/FgFYMN/DU3ir). GPT-J, built on EleutherAI’s 6B parameter GPT-J-6B, is fine-tuned with 3.5 billion tokens. It performs very similarly to GPT-3 on various zero-shot downstream tasks. GPT-JT, a fork of GPT-J-6B, is fine-tuned on 3.53 billion tokens and has been shown to even outperform GPT-3 at some classification tasks. Galactica is trained on 48 million examples** **of scientific articles, websites, textbooks, lecture notes, and encyclopedias. Same evaluation procedure as for the GPT-family models is applied.

**2.4 Statistical Analysis**

Paired *t* tests are used to compare average macro-F1 score across tasks, and a two-sided *p*<0.05 is considered statistically significant. Analyses are performed using python version 3.9.7 (Python Software Foundation).

## Results

A summary of our findings is in Table 1; Figs 2-4 present multiple comparative analyses to provide context and facilitate interpretation of the outcomes.***Figure 2. (A)**** Comparison of model performance on each dataset. Using fine-tuning BioBERT as a reference, average macro-F1 across all datasets was significantly better than all other models (p<0.05 for all); there was no statistically significant difference in average performance between the other pair-wise model comparisons (Appendix Table A5).****(B)**** Time (hours) required to obtain the best-performing results versus average performance across all datasets. BoW = bag-of-words; CoT = Chain of Thought.*

![](../figures/image23.png)

**3.1 Effect of fine-tuning and prompt development on model performance, run time and time investment**

For the reasoning and classification tasks, fine-tuning BioBERT consistently outperforms the best GPT settings by a considerable margin (Δmacro F1 0.109 - 0.169) (Fig 2(A) and Table 1). Averaging performance on all datasets, BioBERT's macro F1 scores are significantly better than the other models (*p*<0.05 for all, Fig 2(A). There is no statistical difference between the average performance of the BoW, best-performing GPT-3.5s, and GPT-4 models (Appendix Table A3).

***Figure 3.**** (A-D) Performance comparison of fine-tuned BioBERT (green bar) and BoW (yellow bar) models with different proportions (20%, 50% or 100%) of development set data versus the best GPTs settings (few-shot CoT in red-line) among four tasks.****(A)**** Comparison on advice in discussion sections,****(B)**** comparison on advice in unstructured abstracts,****(C)**** comparison on advice in structured abstracts,****(D)**** comparison on causal relation detection. BoW = bag-of-words; CoT = Chain of Thought*

![](../figures/image15.png)

Furthermore, engineering the LLM prompts that yield the best results requires a substantial time investment. Fig 2(B) shows the time needed to achieve the best results, taking into account the time needed to develop and identify the prompting strategies for the few-shot settings. Even for the zero-shot setting, which does not require any prompt development, inference time — an indicator of run time and a consideration for compute budget— is longer and yields worse performance compared to BoW and BioBERT performance. Taken together, training a task-specific neural network through classic fine-tuning methodology is both faster and yields better performance.

**3.2 Effect of amount of training data on model performance**

In this experiment, we investigate the amount of training/fine-tuning data from the development set needed to achieve similar or better performing BoW and BioBERT models compared to the best GPT settings (few-shot CoT). As shown in Fig 3, using only 20% of the development set for the supervised fine-tuning of BioBERT surpasses the best GPTs in three of four datasets. Fine-tuning BioBERT on 50% of the development set outperforms the best GPTs on all datasets. Furthermore, the simple and computationally efficient BoW model using 100% of the development set outperforms the best GPTs in two of four datasets.

**3.3 Effect of number of****exemplar**** prompts and CoT prompts on model performance**

We examine the relationship between the number of exemplars and CoT prompts and their impact on GPT settings performance. As shown in Fig 4(A), we observe a drop in performance when comparing one-shot to few-shot settings in three of the datasets. Reasoning for causal relation detection is the only task that consistently improves by adding prompt examples, without CoT. However, as demonstrated in Fig 4(B), incorporating more CoT exemplars results in consistent improvements across datasets. This observation highlights the value of adding CoT exemplars, albeit at substantial cost due to the time effort needed to create the CoT prompts.

Another observation from the one-shot experiments (both with and without CoT) among the LLMs evaluated is the impact of different exemplar prompt choices on performance. Variations in the text of the exemplar prompts for one-shot prompts led to notable variations in model outcomes (Appendix Table A8-A10).

**3.4**** Error Analysis**

Of the investigated GPTs, GPT-4 with few-shot CoT settings yields the best performance for three of four datasets. Thus, we analyze GPT-4’s generated CoTs and identify common error patterns to better understand their strengths and limitations. To maintain consistency, we randomly select 100 prediction errors of GPT-4 with few-shot CoT setting from the test set of each dataset. The selection of the error examples follows the same error type ratio from the confusion matrices of the prediction result. Two common error patterns are identified. Pattern A is an incorrect reasoning step based on one specific keyword. For example, the model classifies an input text as a strong advice if the word “importance” appears in the text (Appendix Table A13, row 4). Pattern B is a false positive due to the model incorrectly determining that there is health advice or a relationship for the classification and reasoning tasks, respectively, when in fact there is none. For instance, Appendix Table A16, row 3 presents a pattern B error where GPT-4 misclassifies a relationship between extracted entities as a clinical relationship. Appendix Tables A13-A16 show examples of the error patterns.***Figure 4. ****Comparison of**** ****GPT-3.5s performance on each dataset with increasing**exemplars** without****(A)**** and with****(B)**** Chain of Thought (CoT). For both plots, random is shown as a baseline and is the uniform distribution across class labels. Here, one-shot uses the majority class exemplar for each task and few-shot uses one exemplar per class. Few-shot = 3**exemplars** for classification datasets and four**exemplars** for the reasoning dataset, reflecting the number of classes for each task.*![](../figures/image79.png)

## Discussion

In this study, we found that, even with the best in-context learning approaches, fine-tuning BioBERT consistently out-performed LLM performance by macro F1 >0.100 for all datasets. In fact, fine-tuning on just 20% of the development dataset outperformed the best GPT-4 and GPT-3.5 performance for all of the classification datasets, and outperformed the best GPT-3.5 performance for the reasoning dataset. Surprisingly, the simple BoW models out-performed all LLM in-context learning approaches without CoT, and performed similarly to the best performing GPT-4 approach for classification of structured abstracts (macro F1 -0.017), and the reasoning task (macro F1 -0.007).

Our study emphasizes the performance, time, and computational trade-offs that should be taken into account when considering various approaches for clinical NLP tasks. At present, our results suggest that the overall balance is in favor of fine-tuning task-specific smaller models, consistent with Lehman et al16. SOTA LLMs such as GPT-3.5/GPT-4 are orders of magnitude larger than traditional language models and cannot be trained or fine-tuned without significant computational resources. For example, GPT-3 has 175 billion parameters and required several thousand petaflop/s-days for pre-training.[25](https://paperpile.com/c/FgFYMN/Ua2Fh) On the other hand, smaller, more accessible out-of-the-box LLMs without fine-tuning (<10 billion parameters) performed very poorly on our tasks and did not demonstrate improvement with in-context learning, in line with the finding that emergent LLMs abilities scale with model size.[26](https://paperpile.com/c/FgFYMN/95vn0)

However, the computational requirements of LLMs could theoretically be offset if their zero- or few-shot performance was adequate. Our results clearly demonstrate that zero-short performance was poor. While few-shot CoT prompting improved performance, fine-tuning BioBERT, a 110 million parameter pre-trained language model, consistently provided the best performance. Prompt development to identify the best prompting strategies is itself resource-intensive, requiring human effort to design the prompts, and computational and time resources to evaluate. Ultimately, at most 50% of each dataset’s full development set was needed to fine-tune BioBERT models that exceeded LLM performance with the best prompting strategies identified using the full development set. Taking into account prompt development and evaluation, obtaining the best-performing LLM results required 100x the time needed to fine-tune our best performing BioBERT model. Of note, we did not include time needed to annotate the datasets, as all methods required the full development set for model development or prompt engineering, and the full test set for evaluation.

Despite under-performing fine-tuned BioBERT, in-context learning—especially CoT prompting—led to important improvements in performance for the classification and reasoning tasks. The ability of CoT to elicit reasoning and improve LLM performance in the general domain has previously been demonstrated.[23,27](https://paperpile.com/c/FgFYMN/bHjK3+nrYzn) Interestingly, while providing more exemplars with CoT prompting consistently improved performance, this was not always the case for prompting without CoT. For GPT-3.5, one-shot prompting provided the best results for classification, while few-shot prompting provided the best results for reasoning. This could be due to noise provided from including less informative prompts, and highlights the fragility of LLM performance based on the provided prompts.

This lack of robustness is also illustrated by the fact that the choice of exemplar for prompting had major impacts on performance. These findings are in line with Shi et al., who showed that, for arithmetic tasks, prompting may provide irrelevant context that reduces performance.[28](https://paperpile.com/c/FgFYMN/eKzvW) Clearly, simply providing more exemplars does not solve the challenge of LLMs robustness—a major area of concern and future research for the clinical domain, where robust performance is paramount. Concerningly, even small, seemingly non-substantive changes to prompts such as typos have been shown to impact performance.[10,14](https://paperpile.com/c/FgFYMN/VfLjI+V4C6) There is an emerging body of work on developing strategies to improve robustness and self-consistency.[14,29](https://paperpile.com/c/FgFYMN/VfLjI+ktchQ) Methods to improve performance will be especially important in the medical domain, where jargon, typos, abbreviations, and synonyms are common, limiting our ability to develop reliable prompting strategies and robustly assess real-world performance.

It should be noted that our tasks indirectly address the question of how LLMs perform on clinical NLP text, and use biomedical texts as a proxy for essential clinical NLP tasks. SOTA LLMs such as GPT-3.5/GPT-4 cannot be used with PHI, precluding an evaluation on real clinical data. Nevertheless, LLM performance is known to decrease on out-of-domain tasks, i.e. tasks that include text that does not reflect what it was trained on, including synthetic clinical text.[14](https://paperpile.com/c/FgFYMN/VfLjI) At the same time, classic language models trained on clinical text has been shown to out-perform LLMs with in-context learning.[13](https://paperpile.com/c/FgFYMN/lDUlL) Taken together with our finding that BioBERT, which is pre-trained on biomedical text, out-performs LLMs, it is reasonable to anticipate that findings would be similar on clinical datasets, but further evaluation will be needed once GPT-3.5/GPT-4 are safely and widely accessible for HIPAA-protected data. Further, non-clinical biomedical NLP has important implications in its own right, including in processing and transmitting medical information and knowledge outside of EMR contexts.[30](https://paperpile.com/c/FgFYMN/4UG7r)

Most studies evaluating LLMs for clinical applications have focused on question-answering, with mixed results.[7–10,12](https://paperpile.com/c/FgFYMN/6p9xd+Kdn6v+HuOMB+Ah0nd+V4C6) For example, while LLMs have achieved passing scores in the USMLE,[7,8](https://paperpile.com/c/FgFYMN/6p9xd+Kdn6v) several recent studies have shown sub-par performance of ChatGPT, including answering neonatal board exam questions,[31](https://paperpile.com/c/FgFYMN/tvjz) answering questions about cancer treatment,[10](https://paperpile.com/c/FgFYMN/V4C6) and providing assistance to laypeople.[32](https://paperpile.com/c/FgFYMN/K2Wb) However, question-answering is not representative of the range of NLP tasks needed to process clinical texts,[30](https://paperpile.com/c/FgFYMN/4UG7r) and in isolation has limited practical use for clinic and research. Further, such studies to date have largely focused on whether the LLM answers a question correctly but do not provide insight into why models may fail, which is critical to improving LLMs for medicine and moving the field forward. We chose our tasks—classification and reasoning—because they are fundamental to the development of NLP technologies that can support clinical care and research beyond question-answering, and are granular enough to reveal the specific language processing steps where LLMs may not perform well.

NLP for classification entails determining what category an input text belongs in. Classification methods identify if a patient’s EMR includes a characteristic or outcome of interest, which has implications for outcomes research, clinical trial matching, and identifying key events at the point-of-care. Reasoning entails determining the relationships between entities, which is to automatically identify how different events in a patient’s medical history relate to one another. Especially because nuanced information conveying medical reasoning can often only be expressed in free text, NLP methods for reasoning are needed to automate higher-level medical inferencing. Here, we investigated causative relationships, which is needed for tasks that require linking any clinical outcome with causative factors, such as associating adverse drug events with their inciting agent. Other reasoning tasks include temporal reasoning, which is determining the order of medical events over time. Another benefit of our task selection is that, compared to question-answering, they are more straight-forward to objectively evaluate, enabling a more direct evaluation of how LLMs perform in the biomedical domain and providing a clearer understanding into the types of performance gaps that need to be improved with additional research and development. In the future, evaluation of LLMs in the clinical domain for other classification and reasoning tasks, as well as other common NLP tasks such as relation extraction, named entity recognition, coreference resolution, word sense disambiguation, and machine translation, will be needed.

## Conclusion

This study suggests an ongoing role for classic NLP models fine-tuned for specific tasks, while also providing guidance into strategies to optimize the LLMs for the biomedical domain. Fine-tuning BioBERT, a much smaller pretrained language model, out-performed SOTA huge LLMs for biomedical classification and reasoning tasks even after extensive prompt development. CoT prompting with multiple exemplars improved LLM performance compared to zero-shot prompting and prompting without CoT. However, developing CoT prompts was both time- and data-intensive, and BioBERT was more efficient with respect to both measures. In addition, LLMs were very sensitive to prompting strategy and the choice of prompt, raising concerns about the potential to develop LLM methods for medicine that are reliable and safe. Our work provides insight into the potential and pitfalls of these rapidly emerging methods for biomedical text processing. Future research could focus on developing more efficient prompting strategies and fine-tuning techniques for LLMs in the biomedical domain while ensuring their reliability and safety, as well as exploring hybrid approaches that combine the strengths of classic NLP models and LLMs to further enhance performance in biomedical text processing tasks.

## Data Availability Statement

All data generated/analysed during this study are available and can be found at [<u>https://github.com/shan23chen/HealthLLM_Eval</u>](https://github.com/shan23chen/HealthLLM_Eval).

## Acknowledgement



The study was funded by R01GM114355 (Author YL), R01LM013486 (Authors HV, GS), and the Woods Foundation (Authors SC, DB). The funder played no role in study design, data collection, analysis and interpretation of data, or the writing of this manuscript.

## References

1. [Vaswani, A.](http://paperpile.com/b/FgFYMN/OA2QZ)[*et al.*](http://paperpile.com/b/FgFYMN/OA2QZ)[Attention is all you need. in](http://paperpile.com/b/FgFYMN/OA2QZ)[*NeurIPS Proceedings*](http://paperpile.com/b/FgFYMN/OA2QZ)[(2017).](http://paperpile.com/b/FgFYMN/OA2QZ)

2. [Sutton, R. S. & Barto, A. G.](http://paperpile.com/b/FgFYMN/EWdNo)[*Reinforcement Learning: An Introduction, 2nd Edition*](http://paperpile.com/b/FgFYMN/EWdNo)[. (The MIT Press, Cambridge, MA, 2018).](http://paperpile.com/b/FgFYMN/EWdNo)

3. [Ouyang, L.](http://paperpile.com/b/FgFYMN/yZqXo)[*et al.*](http://paperpile.com/b/FgFYMN/yZqXo)[Training language models to follow instructions with human feedback.](http://paperpile.com/b/FgFYMN/yZqXo)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/yZqXo)[(2022).](http://paperpile.com/b/FgFYMN/yZqXo)

4. [Ouyang, L.](http://paperpile.com/b/FgFYMN/Ki4xv)[*et al.*](http://paperpile.com/b/FgFYMN/Ki4xv)[Training language models to follow instructions with human feedback. in](http://paperpile.com/b/FgFYMN/Ki4xv)[*NeurIPS Proceesings*](http://paperpile.com/b/FgFYMN/Ki4xv)[(2022).](http://paperpile.com/b/FgFYMN/Ki4xv)

5. [Lee, P., Bubeck, S. & Petro, J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.](http://paperpile.com/b/FgFYMN/bupsp)[*N. Engl. J. Med.*](http://paperpile.com/b/FgFYMN/bupsp)[http://paperpile.com/b/FgFYMN/bupsp](http://paperpile.com/b/FgFYMN/bupsp)[**388**](http://paperpile.com/b/FgFYMN/bupsp)[, 1233–1239 (2023).](http://paperpile.com/b/FgFYMN/bupsp)

6. [Reardon, S. AI Chatbots Can Diagnose Medical Conditions at Home. How Good Are They?](http://paperpile.com/b/FgFYMN/nlsHV)[*Scientific American*](http://paperpile.com/b/FgFYMN/nlsHV)[(By Sara Reardon on March 31 2023).](http://paperpile.com/b/FgFYMN/nlsHV)

7. [Gilson, A.](http://paperpile.com/b/FgFYMN/6p9xd)[*et al.*](http://paperpile.com/b/FgFYMN/6p9xd)[How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment.](http://paperpile.com/b/FgFYMN/6p9xd)[*JMIR Med Educ*](http://paperpile.com/b/FgFYMN/6p9xd)[http://paperpile.com/b/FgFYMN/6p9xd](http://paperpile.com/b/FgFYMN/6p9xd)[**9**](http://paperpile.com/b/FgFYMN/6p9xd)[, e45312 (2023).](http://paperpile.com/b/FgFYMN/6p9xd)

8. [Liévin, V., Hother, C. E. & Winther, O. Can large language models reason about medical questions?](http://paperpile.com/b/FgFYMN/Kdn6v)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/Kdn6v)[(2022).](http://paperpile.com/b/FgFYMN/Kdn6v)

9. [Zuccon, G. & Koopman, B. Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness.](http://paperpile.com/b/FgFYMN/HuOMB)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/HuOMB)[(2023).](http://paperpile.com/b/FgFYMN/HuOMB)

10. [Chen, S.](http://paperpile.com/b/FgFYMN/V4C6)[*et al.*](http://paperpile.com/b/FgFYMN/V4C6)[Use of Artificial Intelligence Chatbots for Cancer Treatment Information.](http://paperpile.com/b/FgFYMN/V4C6)[*JAMA Oncol*](http://paperpile.com/b/FgFYMN/V4C6)[(2023) doi:](http://paperpile.com/b/FgFYMN/V4C6)[10.1001/jamaoncol.2023.2954](http://dx.doi.org/10.1001/jamaoncol.2023.2954)[.](http://paperpile.com/b/FgFYMN/V4C6)

11. [Lyu, Q.](http://paperpile.com/b/FgFYMN/LOqlU)[*et al.*](http://paperpile.com/b/FgFYMN/LOqlU)[Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential.](http://paperpile.com/b/FgFYMN/LOqlU)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/LOqlU)[(2023).](http://paperpile.com/b/FgFYMN/LOqlU)

12. [Singhal, K.](http://paperpile.com/b/FgFYMN/Ah0nd)[*et al.*](http://paperpile.com/b/FgFYMN/Ah0nd)[Large Language Models Encode Clinical Knowledge.](http://paperpile.com/b/FgFYMN/Ah0nd)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/Ah0nd)[(2022).](http://paperpile.com/b/FgFYMN/Ah0nd)

13. [Lehman, E.](http://paperpile.com/b/FgFYMN/lDUlL)[*et al.*](http://paperpile.com/b/FgFYMN/lDUlL)[Do We Still Need Clinical Language Models?](http://paperpile.com/b/FgFYMN/lDUlL)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/lDUlL)[(2023).](http://paperpile.com/b/FgFYMN/lDUlL)

14. [Wang, J.](http://paperpile.com/b/FgFYMN/VfLjI)[*et al.*](http://paperpile.com/b/FgFYMN/VfLjI)[On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective.](http://paperpile.com/b/FgFYMN/VfLjI)[*arXiv [cs.AI]*](http://paperpile.com/b/FgFYMN/VfLjI)[(2023).](http://paperpile.com/b/FgFYMN/VfLjI)

15. [Liao, K. P.](http://paperpile.com/b/FgFYMN/Q4Xhw)[*et al.*](http://paperpile.com/b/FgFYMN/Q4Xhw)[Development of phenotype algorithms using electronic medical records and incorporating natural language processing.](http://paperpile.com/b/FgFYMN/Q4Xhw)[*BMJ*](http://paperpile.com/b/FgFYMN/Q4Xhw)[http://paperpile.com/b/FgFYMN/Q4Xhw](http://paperpile.com/b/FgFYMN/Q4Xhw)[**350**](http://paperpile.com/b/FgFYMN/Q4Xhw)[, h1885 (2015).](http://paperpile.com/b/FgFYMN/Q4Xhw)

16. [Zhang, Y.](http://paperpile.com/b/FgFYMN/ECcXS)[*et al.*](http://paperpile.com/b/FgFYMN/ECcXS)[Comparison of Chest Radiograph Captions Based on Natural Language Processing vs Completed by Radiologists.](http://paperpile.com/b/FgFYMN/ECcXS)[*JAMA Netw Open*](http://paperpile.com/b/FgFYMN/ECcXS)[http://paperpile.com/b/FgFYMN/ECcXS](http://paperpile.com/b/FgFYMN/ECcXS)[**6**](http://paperpile.com/b/FgFYMN/ECcXS)[, e2255113 (2023).](http://paperpile.com/b/FgFYMN/ECcXS)

17. [Medori, J. & Fairon, C. Machine learning and features selection for semi-automatic ICD-9-CM encoding. in](http://paperpile.com/b/FgFYMN/9Zs2K)[*Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents*](http://paperpile.com/b/FgFYMN/9Zs2K)[84–89 (Association for Computational Linguistics, Los Angeles, California, USA, 2010).](http://paperpile.com/b/FgFYMN/9Zs2K)

18. [OpenAI API.](http://paperpile.com/b/FgFYMN/l2nGi)[http://platform.openai.com](http://platform.openai.com)[.](http://paperpile.com/b/FgFYMN/l2nGi)

19. [Li, Y., Wang, J. & Yu, B. Detecting Health Advice in Medical Research Literature. in](http://paperpile.com/b/FgFYMN/lfPHu)[*Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*](http://paperpile.com/b/FgFYMN/lfPHu)[6018–6029 (Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021).](http://paperpile.com/b/FgFYMN/lfPHu)

20. [Yu, B., Li, Y. & Wang, J. Detecting Causal Language Use in Science Findings. in](http://paperpile.com/b/FgFYMN/uPu1p)[*Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*](http://paperpile.com/b/FgFYMN/uPu1p)[4664–4674 (Association for Computational Linguistics, Hong Kong, China, 2019).](http://paperpile.com/b/FgFYMN/uPu1p)

21. [Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.](http://paperpile.com/b/FgFYMN/nut9V)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/nut9V)[(2018).](http://paperpile.com/b/FgFYMN/nut9V)

22. [Lee, J.](http://paperpile.com/b/FgFYMN/QLAXk)[*et al.*](http://paperpile.com/b/FgFYMN/QLAXk)[BioBERT: a pre-trained biomedical language representation model for biomedical text mining.](http://paperpile.com/b/FgFYMN/QLAXk)[*Bioinformatics*](http://paperpile.com/b/FgFYMN/QLAXk)[http://paperpile.com/b/FgFYMN/QLAXk](http://paperpile.com/b/FgFYMN/QLAXk)[**36**](http://paperpile.com/b/FgFYMN/QLAXk)[, 1234–1240 (2020).](http://paperpile.com/b/FgFYMN/QLAXk)

23. [Wei, J.](http://paperpile.com/b/FgFYMN/bHjK3)[*et al.*](http://paperpile.com/b/FgFYMN/bHjK3)[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.](http://paperpile.com/b/FgFYMN/bHjK3)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/bHjK3)[(2022).](http://paperpile.com/b/FgFYMN/bHjK3)

24. [Taylor, R.](http://paperpile.com/b/FgFYMN/DU3ir)[*et al.*](http://paperpile.com/b/FgFYMN/DU3ir)[Galactica: A Large Language Model for Science.](http://paperpile.com/b/FgFYMN/DU3ir)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/DU3ir)[(2022).](http://paperpile.com/b/FgFYMN/DU3ir)

25. [Brown, T. B.](http://paperpile.com/b/FgFYMN/Ua2Fh)[*et al.*](http://paperpile.com/b/FgFYMN/Ua2Fh)[Language Models are Few-Shot Learners.](http://paperpile.com/b/FgFYMN/Ua2Fh)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/Ua2Fh)[(2020).](http://paperpile.com/b/FgFYMN/Ua2Fh)

26. [Wei, J.](http://paperpile.com/b/FgFYMN/95vn0)[*et al.*](http://paperpile.com/b/FgFYMN/95vn0)[Emergent Abilities of Large Language Models.](http://paperpile.com/b/FgFYMN/95vn0)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/95vn0)[(2022).](http://paperpile.com/b/FgFYMN/95vn0)

27. [Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large Language Models are Zero-Shot Reasoners.](http://paperpile.com/b/FgFYMN/nrYzn)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/nrYzn)[(2022).](http://paperpile.com/b/FgFYMN/nrYzn)

28. [Shi, F.](http://paperpile.com/b/FgFYMN/eKzvW)[*et al.*](http://paperpile.com/b/FgFYMN/eKzvW)[Large Language Models Can Be Easily Distracted by Irrelevant Context.](http://paperpile.com/b/FgFYMN/eKzvW)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/eKzvW)[(2023).](http://paperpile.com/b/FgFYMN/eKzvW)

29. [Wang, X.](http://paperpile.com/b/FgFYMN/ktchQ)[*et al.*](http://paperpile.com/b/FgFYMN/ktchQ)[Self-Consistency Improves Chain of Thought Reasoning in Language Models.](http://paperpile.com/b/FgFYMN/ktchQ)[*arXiv [cs.CL]*](http://paperpile.com/b/FgFYMN/ktchQ)[(2022).](http://paperpile.com/b/FgFYMN/ktchQ)

30. [Savova, G. K.](http://paperpile.com/b/FgFYMN/4UG7r)[*et al.*](http://paperpile.com/b/FgFYMN/4UG7r)[Use of Natural Language Processing to Extract Clinical Cancer Phenotypes from Electronic Medical Records.](http://paperpile.com/b/FgFYMN/4UG7r)[*Cancer Res.*](http://paperpile.com/b/FgFYMN/4UG7r)[http://paperpile.com/b/FgFYMN/4UG7r](http://paperpile.com/b/FgFYMN/4UG7r)[**79**](http://paperpile.com/b/FgFYMN/4UG7r)[, 5463–5470 (2019).](http://paperpile.com/b/FgFYMN/4UG7r)

31. [Beam, K.](http://paperpile.com/b/FgFYMN/tvjz)[*et al.*](http://paperpile.com/b/FgFYMN/tvjz)[Performance of a Large Language Model on Practice Questions for the Neonatal Board Examination.](http://paperpile.com/b/FgFYMN/tvjz)[*JAMA Pediatr.*](http://paperpile.com/b/FgFYMN/tvjz)[http://paperpile.com/b/FgFYMN/tvjz](http://paperpile.com/b/FgFYMN/tvjz)[**177**](http://paperpile.com/b/FgFYMN/tvjz)[, 977–979 (2023).](http://paperpile.com/b/FgFYMN/tvjz)

32. [Murk, W., Goralnick, E., Brownstein, J. S. & Landman, A. B. Quality of Layperson CPR Instructions From Artificial Intelligence Voice Assistants.](http://paperpile.com/b/FgFYMN/K2Wb)[*JAMA Netw Open*](http://paperpile.com/b/FgFYMN/K2Wb)[http://paperpile.com/b/FgFYMN/K2Wb](http://paperpile.com/b/FgFYMN/K2Wb)[**6**](http://paperpile.com/b/FgFYMN/K2Wb)[, e2331205 (2023).](http://paperpile.com/b/FgFYMN/K2Wb)

## Appendix A
**Table A1 Parameters and hyperparameters for fine-tuning BioBERT**

<table class="c576 c1021"><tr class="c218"><td class="c205" colspan="1" rowspan="1"><p class="c14 c37"><span class="c7 c25"></span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c1042" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in unstructured abstracts</span></p></td><td class="c951" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c284" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c13"><td class="c851" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">random seed</span></p></td><td class="c677" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">42</span></p></td><td class="c536" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">42</span></p></td><td class="c1014" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">42</span></p></td><td class="c847" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">42</span></p></td></tr><tr class="c13"><td class="c126" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">batch size (per_device_train)</span></p></td><td class="c720" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">128</span></p></td><td class="c257" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">128</span></p></td><td class="c229" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">128</span></p></td><td class="c400" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">128</span></p></td></tr><tr class="c13"><td class="c126" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">batch size (per_device_eval)</span></p></td><td class="c720" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td><td class="c257" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td><td class="c229" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td><td class="c400" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td></tr><tr class="c13"><td class="c126" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">epoch</span></p></td><td class="c720" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">50</span></p></td><td class="c257" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">50</span></p></td><td class="c229" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">50</span></p></td><td class="c400" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">200</span></p></td></tr><tr class="c13"><td class="c126" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">learning rate</span></p></td><td class="c720" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">2e-5</span></p></td><td class="c257" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">2e-5</span></p></td><td class="c229" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">2e-5</span></p></td><td class="c400" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">2e-5</span></p></td></tr><tr class="c13"><td class="c126" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">max sequence length</span></p></td><td class="c720" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td><td class="c257" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td><td class="c229" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td><td class="c400" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">512</span></p></td></tr><tr class="c13"><td class="c126" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">warmup_steps</span></p></td><td class="c720" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">500</span></p></td><td class="c257" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">500</span></p></td><td class="c229" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">500</span></p></td><td class="c400" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">500</span></p></td></tr><tr class="c13"><td class="c77" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">weight_decay</span></p></td><td class="c966" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.01</span></p></td><td class="c979" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.01</span></p></td><td class="c896" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.01</span></p></td><td class="c825" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.01</span></p></td></tr></table>

Tables A2-A4 provide a comparison of the estimated time required for training and inferencing using different models and settings.

The total spending of inference OpenAI API was $1,299 based on OpenAI pricing for Feb-Mar, 2023[[1]](#ftnt1)**Table A2 Time required for training and inferencing of each method**

<table class="c27"><tr class="c729"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Model</span></p><p class="c14"><span class="c25 c79">&amp;</span></p><p class="c14"><span class="c25 c79">Setting</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Prompt creation time</span></p><p class="c14"><span class="c10 c25">(seconds)</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Inference time for </span></p><p class="c14"><span class="c25 c79">training on dev set</span><span class="c25 c79 c30">c</span><span class="c10 c25"> </span></p><p class="c14"><span class="c10 c25">(seconds)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Inference time </span></p><p class="c14"><span class="c25 c79">on test set</span><span class="c25 c79 c30">c</span><span class="c10 c25"> </span></p><p class="c14"><span class="c10 c25">(seconds)</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Total</span></p><p class="c14"><span class="c10 c25">(hours)</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Performance</span><span class="c182 c25 c79 c30">d</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">GPT zero-shot</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c2 c79">2400.000</span><span class="c182 c2 c79 c30">a</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">28296.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">11322.000</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">11.005</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.458</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">GPT few-shot CoT</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c2 c79">18000.000</span><span class="c182 c2 c79 c30">b</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">94320.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">37740.000</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">36.683</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.709</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">BoW</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">13.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.700</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.004</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.667</span></p></td></tr><tr class="c139"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">FT BioBERT</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">139.427</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.800</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.039</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.844</span></p></td></tr></table>

CoT = Chain-of-Thought, BoW = bag-of-words; FT = fine-tuning.

aIt took roughly 240 seconds to create zero-shot prompts, and a total of 10 prompts were tested.

bIt took roughly 1800 seconds to write a few-shot CoT prompt, and a total of 10 prompts were tested.

c1.2s per query for 0-shot, 4s per query 4-shot CoT.

dPerformance is the average macro F1 among the 4 datasets.**Table A3 Time required for training and inferencing of an Oracle prompter**

<table class="c27"><tr class="c139"><td class="c1008" colspan="1" rowspan="2"><p class="c14"><span class="c25 c79">Model</span></p><p class="c14"><span class="c25 c79">&amp;</span></p><p class="c14"><span class="c25 c79">Setting</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Oracle prompter</span></p></td><td class="c779" colspan="4" rowspan="1"><p class="c14"><span class="c10 c2">Assuming an Oracle prompt writer who creates only one prompt to achieve the best performance on the test set. Thus, no prompts needed for the validation set.</span></p></td></tr><tr class="c139"><td class="c107" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Prompt </span></p><p class="c14"><span class="c10 c25">creation time</span></p><p class="c14"><span class="c25 c79">(in seconds)</span><span class="c182 c25 c79 c30">a</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Inference time for </span></p><p class="c14"><span class="c10 c25">training on dev set</span></p><p class="c14"><span class="c10 c25">(in seconds)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Inference time </span></p><p class="c14"><span class="c25 c79">on test set</span><span class="c25 c79 c30">b</span><span class="c10 c25"> </span></p><p class="c14"><span class="c10 c25">(in seconds)</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Total</span></p><p class="c14"><span class="c10 c25">(in hours)</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Performance</span><span class="c182 c25 c79 c30">c</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">GPT zero-shot</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">120.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">11322.000</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">3.145</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.458</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">GPT few-shot CoT</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">900.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">37740.000</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">10.483</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.709</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">BoW</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">13.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.700</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.004</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.667</span></p></td></tr><tr class="c139"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">FT BioBERT</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">139.427</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.800</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.039</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.844</span></p></td></tr></table>

CoT = Chain-of-Thought, BoW = bag-of-words; FT = fine-tuning.

aWe assume the Oracle prompt writer will also create prompts 100% more efficiently than humans (Table A2).

b1.2s per query for 0-shot, 4s per query 4-shot CoT.

cPerformance is the average macro F1 among the 4 datasets.**Table A4 Time required for training and inferencing of an Oracle prompter with machine that can parallel inference in batch of 512**

<table class="c27"><tr class="c139"><td class="c1008" colspan="1" rowspan="2"><p class="c14"><span class="c25 c79">Model</span></p><p class="c14"><span class="c25 c79">&amp;</span></p><p class="c14"><span class="c25 c79">Setting</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Oracle prompter</span></p></td><td class="c779" colspan="4" rowspan="1"><p class="c14"><span class="c10 c2">Batch mode: 512</span></p></td></tr><tr class="c139"><td class="c107" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Prompt </span></p><p class="c14"><span class="c10 c25">creation time</span></p><p class="c14"><span class="c25 c79">(in seconds)</span><span class="c182 c25 c79 c30">a</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Inference time for </span></p><p class="c14"><span class="c10 c25">training on dev set</span></p><p class="c14"><span class="c10 c25">(in seconds)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Inference time </span></p><p class="c14"><span class="c25 c79">on test set</span><span class="c25 c79 c30">b</span><span class="c10 c25"> </span></p><p class="c14"><span class="c10 c25">(in seconds)</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Total</span></p><p class="c14"><span class="c10 c25">(in hours)</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Performance</span><span class="c182 c25 c79 c30">c</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">GPT zero-shot</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">120.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">22.113</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.006</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.458</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">GPT few-shot CoT</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">900.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">73.711</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.020</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.709</span></p></td></tr><tr class="c330"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">BoW</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">13.000</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.700</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.004</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.667</span></p></td></tr><tr class="c139"><td class="c65" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">FT BioBERT</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.000</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">139.427</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.800</span></p></td><td class="c193" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.039</span></p></td><td class="c250" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.844</span></p></td></tr></table>

CoT = Chain-of-Thought, BoW = bag-of-words; FT = fine-tuning.

aWe assume the Oracle prompt writer will also create prompts 100% more efficiently than humans (Table A2).

b1.2s per query for 0-shot, 4s per query 4-shot CoT.

cPerformance is the average macro F1 among the 4 datasets.**Table A5 *****P*****-values for pairwise comparison of average model performance among the 4 datasets**

<table class="c27"><tr class="c218"><td class="c753" colspan="1" rowspan="1"><p class="c14 c37"><span class="c7 c25"></span></p></td><td class="c875" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">GPT-3.5s</span></p></td><td class="c750" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">BoW</span></p></td><td class="c957" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">FT BioBERT</span></p></td></tr><tr class="c13"><td class="c787" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">GPT-4</span></p></td><td class="c1053" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.4150</span></p></td><td class="c655" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.4228</span></p></td><td class="c833" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.0062*</span></p></td></tr><tr class="c13"><td class="c369" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">GPT-3.5s</span></p></td><td class="c196" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c378" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.7969</span></p></td><td class="c396" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.0008*</span></p></td></tr><tr class="c13"><td class="c968" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">BoW</span></p></td><td class="c739" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c800" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c922" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.0041*</span></p></td></tr></table>

BoW = bag-of-words; FT = fine-tuning.

*: *p*<0.05 considered statistically significant.

**Table A6 Macro F1 scores for GPT-3.5-Davinci vs Turbo(ChatGPT) vs GPT-4 on the test set**

<table class="c27"><tr class="c218"><td class="c945" colspan="1" rowspan="1"><p class="c14 c37"><span class="c7 c2"></span></p></td><td class="c637" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c784" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">unstructured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c452" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c291" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c13"><td class="c722" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Zero-shot Davinci</span></p></td><td class="c365" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.503</span></p></td><td class="c715" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.461</span></p></td><td class="c741" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.548</span></p></td><td class="c596" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.242</span></p></td></tr><tr class="c13"><td class="c491" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Zero-shot Turbo</span></p></td><td class="c886" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.506</span></p></td><td class="c283" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.489</span></p></td><td class="c773" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.484</span></p></td><td class="c848" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.288</span></p></td></tr><tr class="c13"><td class="c868" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Zero-shot GPT-4</span></p></td><td class="c865" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.509</span></p></td><td class="c350" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.478</span></p></td><td class="c827" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.517</span></p></td><td class="c586" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.304</span></p></td></tr><tr class="c13"><td class="c958" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT Davinci</span></p></td><td class="c818" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.671</span></p></td><td class="c826" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.617</span></p></td><td class="c829" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.718</span></p></td><td class="c481" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.649</span></p></td></tr><tr class="c13"><td class="c491" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT Turbo</span></p></td><td class="c886" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.603</span></p></td><td class="c283" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.670</span></p></td><td class="c773" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.699</span></p></td><td class="c848" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.566</span></p></td></tr><tr class="c13"><td class="c1007" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT GPT-4</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.648</span></p></td><td class="c1001" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.712</span></p></td><td class="c857" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.770</span></p></td><td class="c642" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.682</span></p></td></tr></table>

Best results are bolded.

CoT = Chain-of-Thought.

**Table A7 Macro F1 scores for GPT-3.5-Davinci vs Turbo(ChatGPT) on the test set**

<table class="c27"><tr class="c375"><td class="c970" colspan="1" rowspan="1"><p class="c14 c37"><span class="c7 c2"></span></p></td><td class="c1066" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c1036" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">unstructured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c988" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c291" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c139"><td class="c362" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot Davinci</span></p></td><td class="c483" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.503</span></p></td><td class="c526" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.461</span></p></td><td class="c908" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.548</span></p></td><td class="c596" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.242</span></p></td></tr><tr class="c139"><td class="c272" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot Turbo</span></p></td><td class="c167" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.506</span></p></td><td class="c407" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.489</span></p></td><td class="c313" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.484</span></p></td><td class="c586" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.288</span></p></td></tr><tr class="c139"><td class="c81" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">One-shot Davinci</span></p></td><td class="c609" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.513</span></p></td><td class="c354" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.501</span></p></td><td class="c274" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.593</span></p></td><td class="c481" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.210</span></p></td></tr><tr class="c139"><td class="c272" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">One-shot Turbo</span></p></td><td class="c167" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.513</span></p></td><td class="c407" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.554</span></p></td><td class="c313" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.470</span></p></td><td class="c586" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.396</span></p></td></tr><tr class="c139"><td class="c81" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot Davinci</span></p></td><td class="c609" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.468</span></p></td><td class="c354" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.403</span></p></td><td class="c274" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.495</span></p></td><td class="c481" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.542</span></p></td></tr><tr class="c139"><td class="c272" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot Turbo</span></p></td><td class="c167" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.475</span></p></td><td class="c407" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.243</span></p></td><td class="c313" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.436</span></p></td><td class="c586" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.513</span></p></td></tr><tr class="c139"><td class="c81" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-CoT Davinci</span></p></td><td class="c609" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.436</span></p></td><td class="c354" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.383</span></p></td><td class="c274" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.515</span></p></td><td class="c481" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.332</span></p></td></tr><tr class="c139"><td class="c272" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-CoT Turbo</span></p></td><td class="c167" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.418</span></p></td><td class="c407" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.424</span></p></td><td class="c313" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.486</span></p></td><td class="c586" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.345</span></p></td></tr><tr class="c139"><td class="c81" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">One-CoT Davinci</span></p></td><td class="c609" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.418</span></p></td><td class="c354" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.423</span></p></td><td class="c274" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.470</span></p></td><td class="c481" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.489</span></p></td></tr><tr class="c139"><td class="c272" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">One-CoT Turbo</span></p></td><td class="c167" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.430</span></p></td><td class="c407" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.410</span></p></td><td class="c313" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.552</span></p></td><td class="c586" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.553</span></p></td></tr><tr class="c139"><td class="c81" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-CoT Davinci</span></p></td><td class="c609" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.671</span></p></td><td class="c354" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.617</span></p></td><td class="c274" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.718</span></p></td><td class="c481" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.649</span></p></td></tr><tr class="c139"><td class="c329" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-CoT Turbo</span></p></td><td class="c281" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.603</span></p></td><td class="c359" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.670</span></p></td><td class="c881" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.699</span></p></td><td class="c642" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.566</span></p></td></tr></table>

Best results are bolded.

CoT = Chain-of-Thought.

Tables A8-A10 present a comparison of development/test set performance when exemplars from different classes are used for GPT-3.5-Davinci under the one-shot and one-shot with Chain-of-Thought settings. One-shot results on the test set were not evaluated due to cost limitations.

**Table A8 GPT-3.5-Davinci, one-shot CoT results on the test set**

<table class="c27"><tr class="c218"><td class="c894" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Class of Exemplar </span><span class="c10 c2">(Classification/Reasoning)</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c1012" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in unstructured abstracts</span></p></td><td class="c616" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c708" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c13"><td class="c394" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79">None/None</span><span class="c2 c79 c30">a</span></p></td><td class="c621" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.421</span></p></td><td class="c1056" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.410</span></p></td><td class="c485" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.552</span></p></td><td class="c623" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.526</span></p></td></tr><tr class="c13"><td class="c607" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Weak/</span><span class="c2 c79 c48">Correlational</span></p></td><td class="c390" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.430</span></p></td><td class="c521" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.283</span></p></td><td class="c1024" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.381</span></p></td><td class="c893" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.455</span></p></td></tr><tr class="c13"><td class="c1022" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Strong/Conditional Causal</span></p></td><td class="c578" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.343</span></p></td><td class="c798" colspan="1" rowspan="1"><p class="c12"><span class="c2 c10">0.319</span></p></td><td class="c664" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.447</span></p></td><td class="c942" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.553</span></p></td></tr><tr class="c13"><td class="c437" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Direct Causal</span></p></td><td class="c737" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c936" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c459" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c374" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.386</span></p></td></tr></table>

Best results are bolded.

aNone class is the majority class for all datasets.

**Table A9 GPT-3.5-Davinci, one-shot results on the development set**

<table class="c27"><tr class="c218"><td class="c725" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Class of Exemplar </span><span class="c10 c2">(Classification/Reasoning)</span></p></td><td class="c316" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c762" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in unstructured abstracts</span></p></td><td class="c616" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c648" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c13"><td class="c795" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79">None/None</span><span class="c2 c79 c30">a</span></p></td><td class="c662" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.462</span></p></td><td class="c560" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.510</span></p></td><td class="c485" colspan="1" rowspan="1"><p class="c12"><span class="c25 c79">0.552</span></p></td><td class="c1002" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.521</span></p></td></tr><tr class="c13"><td class="c454" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Weak/</span><span class="c2 c79 c48">Correlational</span></p></td><td class="c628" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.460</span></p></td><td class="c816" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.333</span></p></td><td class="c1024" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.391</span></p></td><td class="c727" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.495</span></p></td></tr><tr class="c13"><td class="c647" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Strong/Conditional Causal</span></p></td><td class="c446" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.393</span></p></td><td class="c269" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.339</span></p></td><td class="c664" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.448</span></p></td><td class="c1065" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.453</span></p></td></tr><tr class="c13"><td class="c802" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Direct Causal</span></p></td><td class="c1026" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c418" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c459" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c808" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.456</span></p></td></tr></table>

Best results are bolded.

aNone class is the majority class for all datasets.**Table A10 GPT3.5-Davinci, one-shot CoT results on the development set **
**Bold indicates best performance **

<table class="c27"><tr class="c218"><td class="c894" colspan="1" rowspan="1"><p class="c14"><span class="c25 c79">Class of Exemplar </span><span class="c10 c2">(Classification/Reasoning)</span></p></td><td class="c55" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c762" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in unstructured abstracts</span></p></td><td class="c1057" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c1009" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c13"><td class="c394" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79">None/None</span><span class="c2 c79 c30">a</span></p></td><td class="c621" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.411</span></p></td><td class="c560" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.390</span></p></td><td class="c982" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.423</span></p></td><td class="c941" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.526</span></p></td></tr><tr class="c13"><td class="c607" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Weak/</span><span class="c2 c79 c48">Correlational</span></p></td><td class="c390" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.340</span></p></td><td class="c816" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.283</span></p></td><td class="c364" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.361</span></p></td><td class="c1005" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.455</span></p></td></tr><tr class="c13"><td class="c1022" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Strong/Conditional Causal</span></p></td><td class="c578" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.343</span></p></td><td class="c269" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.430</span></p></td><td class="c680" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.350</span></p></td><td class="c323" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.553</span></p></td></tr><tr class="c13"><td class="c437" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79 c48">Direct Causal</span></p></td><td class="c737" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c418" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c991" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">–</span></p></td><td class="c792" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.456</span></p></td></tr></table>

Best results are bolded.

aNone class is the majority class for all datasets.**Table A11 Performance of smaller GPT models (< 10B parameters) on the test set**

<table class="c27"><tr class="c375"><td class="c267" colspan="1" rowspan="1"><p class="c14 c37"><span class="c7 c25"></span></p></td><td class="c233" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c387" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in unstructured abstracts</span></p></td><td class="c738" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c852" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c139"><td class="c764" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot GPT-J</span></p></td><td class="c498" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.248</span></p></td><td class="c431" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.230</span></p></td><td class="c489" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.302</span></p></td><td class="c408" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.211</span></p></td></tr><tr class="c139"><td class="c632" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot GPT-JT</span></p></td><td class="c168" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.272</span></p></td><td class="c906" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.262</span></p></td><td class="c262" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.321</span></p></td><td class="c517" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.226</span></p></td></tr><tr class="c139"><td class="c556" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot Galactica</span></p></td><td class="c214" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.234</span></p></td><td class="c811" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.218</span></p></td><td class="c542" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.294</span></p></td><td class="c766" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.252</span></p></td></tr><tr class="c139"><td class="c764" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot GPT-J</span></p></td><td class="c1048" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.157</span></p></td><td class="c828" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.142</span></p></td><td class="c765" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.292</span></p></td><td class="c527" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.191</span></p></td></tr><tr class="c139"><td class="c632" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot GPT-JT</span></p></td><td class="c168" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.276</span></p></td><td class="c906" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.284</span></p></td><td class="c262" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.358</span></p></td><td class="c517" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.216</span></p></td></tr><tr class="c139"><td class="c695" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot Galactica</span></p></td><td class="c912" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.246</span></p></td><td class="c1015" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.205</span></p></td><td class="c1050" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.293</span></p></td><td class="c836" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.244</span></p></td></tr></table>

Best results are bolded.** **
**Table A12 Performance of smaller GPT models (< 10B parameters) on the dev set **

<table class="c27"><tr class="c375"><td class="c649" colspan="1" rowspan="1"><p class="c14 c37"><span class="c7 c25"></span></p></td><td class="c305" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">discussion </span></p><p class="c14"><span class="c10 c25">sections</span></p></td><td class="c783" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in unstructured abstracts</span></p></td><td class="c1040" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Classification: </span></p><p class="c14"><span class="c10 c25">Advice in </span></p><p class="c14"><span class="c10 c25">structured </span></p><p class="c14"><span class="c10 c25">abstracts</span></p></td><td class="c673" colspan="1" rowspan="1"><p class="c14"><span class="c10 c25">Reasoning: </span></p><p class="c14"><span class="c10 c25">Causal </span></p><p class="c14"><span class="c10 c25">relation </span></p><p class="c14"><span class="c10 c25">detection</span></p></td></tr><tr class="c139"><td class="c347" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot GPT-J</span></p></td><td class="c449 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.247</span></p></td><td class="c434 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.246</span></p></td><td class="c765 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.296</span></p></td><td class="c48 c931" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.210</span></p></td></tr><tr class="c139"><td class="c454" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot GPT-JT</span></p></td><td class="c48 c390" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.285</span></p></td><td class="c344 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.262</span></p></td><td class="c262 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.331</span></p></td><td class="c529 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.226</span></p></td></tr><tr class="c139"><td class="c700" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Zero-shot Galactica</span></p></td><td class="c287" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.233</span></p></td><td class="c370" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.209</span></p></td><td class="c542" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.278</span></p></td><td class="c503" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.250</span></p></td></tr><tr class="c139"><td class="c347" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot GPT-J</span></p></td><td class="c449" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.155</span></p></td><td class="c434" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.143</span></p></td><td class="c765" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.286</span></p></td><td class="c931 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.192</span></p></td></tr><tr class="c139"><td class="c454" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot GPT-JT</span></p></td><td class="c810" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.284</span></p></td><td class="c984" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.282</span></p></td><td class="c933" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.348</span></p></td><td class="c529" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.217</span></p></td></tr><tr class="c139"><td class="c581" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-shot Galactica</span></p></td><td class="c633" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.257</span></p></td><td class="c713" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.222</span></p></td><td class="c278" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.308</span></p></td><td class="c484" colspan="1" rowspan="1"><p class="c12"><span class="c10 c25">0.257</span></p></td></tr><tr class="c139"><td class="c48 c978" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-CoT GPT-J</span></p></td><td class="c449 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.211</span></p></td><td class="c434 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.200</span></p></td><td class="c48 c256" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.323</span></p></td><td class="c995" colspan="1" rowspan="1"><p class="c12"><span class="c2 c79">–</span></p></td></tr><tr class="c139"><td class="c454 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-CoT GPT-JT</span></p></td><td class="c390 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.172</span></p></td><td class="c344 c48" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.129</span></p></td><td class="c48 c917" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.269</span></p></td><td class="c993" colspan="1" rowspan="1"><p class="c12"><span class="c2 c79">–</span></p></td></tr><tr class="c139"><td class="c854" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">Few-CoT Galactica</span></p></td><td class="c940" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.241</span></p></td><td class="c964" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.195</span></p></td><td class="c926" colspan="1" rowspan="1"><p class="c12"><span class="c10 c2">0.368</span></p></td><td class="c417" colspan="1" rowspan="1"><p class="c12"><span class="c2 c79">–</span></p></td></tr></table>

Best results are bolded.

CoT = Chain-of-Thought.**Table A13 Sample of generated three-shot Chain-of-Thought from GPT-4 for advice in the structured abstracts in the HealthAdvice test set.**

<table class="c576 c949"><tr class="c13"><td class="c686" colspan="2" rowspan="1"><p class="c94"><span class="c10 c25">Sentence</span></p></td><td class="c351" colspan="3" rowspan="1"><p class="c94"><span class="c10 c25">GPT–4 Answer</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Gold</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Pred</span></p></td></tr><tr class="c137"><td class="c571" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">Testing this hypothesis may represent a direction of future research.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2 c251">1. The term "may represent" indicates that there is an advice in the context.</span></p><p class="c14"><span class="c10 c2 c251">2. "May represent" suggests a possibility, which is not a strong opinion.</span></p><p class="c14"><span class="c10 c2 c251">3. The answer is weak advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak </span></p><p class="c14"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c571 c48" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">Future research should attempt to replicate these findings in a larger-scale trial using a more sophisticated, user-friendly program and additional measures of improvement in more diverse samples.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2 c251">1. The term "should attempt" indicates that there is an advice in the context.</span></p><p class="c14"><span class="c10 c2 c251">2. "Should" indicates a directive opinion.</span></p><p class="c14"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c571 c48" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">Our results suggest that pregnant women with an HbA1c of 5.4% (36\u2005mmol/mol) should proceed with an OGTT.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1. The term "suggest" indicates that there is an advice in the context.</span></p><p class="c14"><span class="c2 c79">2. </span><span class="c2 c79 c219">"Suggest"</span><span class="c10 c2"> indicates a directive opinion, but not as strong as a recommendation.</span></p><p class="c14"><span class="c10 c2">3. The answer is weak advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak </span></p><p class="c14"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c571 c48" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">This highlights the importance of considering subgroups of patients with LRTI to develop RP evidence-based practice.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1. The context mentions the importance of considering subgroups, which implies a suggestion or advice.</span></p><p class="c14"><span class="c2 c79">2. The term </span><span class="c2 c79 c219">"importance"</span><span class="c10 c2"> indicates that the opinion is not weak.</span></p><p class="c14"><span class="c10 c2">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak</span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c571" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">Surgery done as soon as possible is helpful to reduce the force caused by the obstruction and helpful to the normal facial growth.</span></p></td><td class="c513" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1. The context states that surgery is helpful, which implies a suggestion or advice.</span></p><p class="c14"><span class="c2 c79">2. The phrase </span><span class="c2 c79 c219">"as soon as possible"</span><span class="c10 c2"> indicates a sense of urgency, making the advice stronger.</span></p><p class="c14"><span class="c10 c2">3. The answer is strong advice.</span></p></td><td class="c482" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c482" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c63"><td class="c605" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">This study shows the lack of normative consensus about ACP and highlights the need for consistent educational programs regarding the role of the CBLTC provider in the ACP process.</span></p></td><td class="c1046" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1. The context states the need for consistent educational programs, which is an advice.</span></p><p class="c14"><span class="c2 c79">2. The term </span><span class="c2 c79 c219">"highlights the need"</span><span class="c10 c2"> indicates that the opinion is not strong.</span></p><p class="c14"><span class="c10 c2">3. The answer is weak advice.</span></p></td><td class="c339" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td><td class="c339" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak </span></p><p class="c14"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c948" colspan="7" rowspan="3"><p class="c14"><span class="c2 c56">Pattern A</span><span class="c10 c2">     Incorrect reasoning step based on specific keywords</span></p><p class="c14"><span class="c2 c79 c251">Pattern B</span><span class="c10 c2">     Incorrect classification of health advice</span></p></td></tr><tr class="c63"></tr><tr class="c8"></tr></table>**Table A14 Sample of generated three-shot Chain-of-Thought from GPT-4 for advice in the unstructured abstracts in the HealthAdvice test set.**

<table class="c150"><tr class="c13"><td class="c1044" colspan="2" rowspan="1"><p class="c94"><span class="c10 c25">Sentence</span></p></td><td class="c754" colspan="3" rowspan="1"><p class="c94"><span class="c10 c25">GPT–4 Answer</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Gold</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Pred</span></p></td></tr><tr class="c137"><td class="c624" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">Future research should seek to implement additional evidence-based actions to improve QoL in this population.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1. The term "should seek to implement" indicates that there is an advice in the context.</span></p><p class="c50"><span class="c10 c2 c251">2. "Should" indicates a strong opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c624 c48" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c2 c79">1. The context states a </span><span class="c2 c56">conclusion</span><span class="c10 c2">, which is not an advice.</span></p><p class="c50"><span class="c10 c2">2. The answer is no advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c624 c48" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1. The context mentions "a larger study will be needed," which indicates an advice.</span></p><p class="c50"><span class="c10 c2 c251">2. "Will be needed" suggests a strong opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c624 c48" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1. The context states a recommendation, which is a form of advice.</span></p><p class="c50"><span class="c10 c2 c251">2. The term "recommends" indicates a strong opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c624" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">Health promotion in transport industry workplaces should be an important topic for future research.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1. The term "should be" indicates that there is an advice in the context.</span></p><p class="c50"><span class="c10 c2 c251">2. "Should be" indicates a strong opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c63"><td class="c635" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">Treatment with iodoform would be beneficial.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2">1. The term "would be beneficial" indicates an advice in the context.</span></p><p class="c50"><span class="c2 c79">2. </span><span class="c2 c56">"Would be"</span><span class="c10 c2"> indicates a strong opinion.</span></p><p class="c50"><span class="c10 c2">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">weak </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c624" colspan="2" rowspan="3"><p class="c50"><span class="c10 c2">A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value.</span></p></td><td class="c946" colspan="3" rowspan="3"><p class="c50"><span class="c10 c2 c251">1. The term "may have" indicates an advice in the context.</span></p><p class="c50"><span class="c10 c2 c251">2. "May have" indicates a weak opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is weak advice.</span></p></td><td class="c482" colspan="1" rowspan="3"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c535" colspan="1" rowspan="3"><p class="c14"><span class="c10 c2">weak</span></p><p class="c14"><span class="c10 c2">advice</span></p></td></tr><tr class="c63"></tr><tr class="c8"></tr><tr class="c13"><td class="c624" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1. The phrase "replication will be required" indicates an advice in the context.</span></p><p class="c50"><span class="c10 c2 c251">2. "Will be required" indicates a strong opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c624" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">Replication tests in more generalizable samples and of longer duration are indicated.</span></p></td><td class="c307" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1. The term "are indicated" suggests that there is an advice in the context.</span></p><p class="c50"><span class="c10 c2 c251">2. "Are indicated" implies a strong opinion.</span></p><p class="c50"><span class="c10 c2 c251">3. The answer is strong advice.</span></p></td><td class="c45" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">strong advice</span></p></td></tr><tr class="c13"><td class="c763" colspan="7" rowspan="3"><p class="c14"><span class="c2 c56">Pattern A</span><span class="c10 c2">     Incorrect reasoning step based on specific keywords</span></p><p class="c14"><span class="c2 c79 c251">Pattern B</span><span class="c10 c2">     Incorrect classification of health advice</span></p></td></tr><tr class="c63"></tr><tr class="c8"></tr></table>**Table A15 Sample of generated three-shot Chain-of-Thought from GPT-4 for advice in the discussion in the HealthAdvice test set.**

<table class="c150"><tr class="c13"><td class="c882" colspan="2" rowspan="1"><p class="c94"><span class="c10 c25">Sentence</span></p></td><td class="c351" colspan="3" rowspan="1"><p class="c94"><span class="c10 c25">GPT–4 Answer</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Gold</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Pred</span></p></td></tr><tr class="c137"><td class="c223" colspan="2" rowspan="1"><p class="c50"><span class="c2 c79">In this Saudi sample, the mean arch widths were narrower when compared to a Turkish sample,</span><span class="c2 c79 c30">19</span><span class="c2 c79"> a North American sample, a South American sample, a Korean sample,</span><span class="c2 c79 c30">20</span><span class="c2 c79"> and an Egyptian sample;</span><span class="c2 c79 c30">21</span><span class="c2 c79"> on the other hand, it was close to a Malay sample.</span><span class="c2 c79 c30">22</span><span class="c10 c2"> These ethnic groups’ differences in arch dimensions explain the need for specific orthodontic arch wire for each patient, based on the initial arch form.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2 c251">1) The claim is discussing the differences in arch dimensions among various ethnic groups.</span></p><p class="c50"><span class="c10 c2 c251">2) The claim suggests the need for specific orthodontic arch wires for each patient based on their initial arch form.</span></p><p class="c50"><span class="c10 c2 c251">3) The claim does not use strong or hedged language, but it does imply an actionable suggestion for orthodontic treatment.</span></p><p class="c50"><span class="c10 c2 c251">4) Therefore, the claim has weak advice.</span></p><p class="c50"><span class="c10 c2 c251">Answer: (2) weak advice.</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">no </span></p><p class="c50"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c48 c223" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">In comparison, Patient 3 demonstrated significant PR3-ANCA antibody production without evidence of peripheral B cell repopulation, which emphasizes the need to consider granulomata in GPA as B cell containing tertiary lymphoid structures pivotal to driving the relapsing disease course.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2">1) The claim is discussing the importance of considering granulomata in GPA as B cell containing tertiary lymphoid structures.</span></p><p class="c50"><span class="c2 c79">2) The claim uses the phrase </span><span class="c2 c56">"emphasizes the need to consider,"</span><span class="c10 c2"> which indicates a suggestion or recommendation.</span></p><p class="c50"><span class="c10 c2">3) However, the strength of the advice is not explicitly strong or weak, but it leans more towards a weak advice due to the use of the word "emphasizes."</span></p><p class="c50"><span class="c10 c2">Answer: (2) weak advice.</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">strong advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c223 c48" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">Despite considerable challenges that face the interpretation of clinicoanatomical correlations in neurodegenerative diseases, the current results suggest that at least parts of the left anterior temporal neocortex should be inserted into the temporosylvian language network where they are likely to play critical roles in linking words to their object referents and in sharpening the taxonomic acuity of naming and word comprehension.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2">1) The claim has an actionable suggestion related to the insertion of parts of the left anterior temporal neocortex into the temporosylvian language network.</span></p><p class="c50"><span class="c2 c79">2) When describing the suggestion, the authors used the phrase "suggest that" and </span><span class="c2 c56">"likely to play critical roles"</span><span class="c10 c2">, which indicates the strength of the claim is not strong but weak.</span></p><p class="c50"><span class="c10 c2">3) Therefore, the claim has weak advice.</span></p><p class="c50"><span class="c10 c2">Answer: (2) weak advice.</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">strong advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c223 c48" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">The results, coupled with other correlational and quasiexperimental results, stress the importance of providing high quality evidence-based reading instruction throughout the early grades.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2">1) The claim has an actionable suggestion related to providing high-quality, evidence-based reading instruction in early grades.</span></p><p class="c50"><span class="c2 c79">2) When describing the suggestion, the authors used the phrase </span><span class="c2 c56">"stress the importance of"</span><span class="c10 c2">, which indicates the strength of the claim is strong.</span></p><p class="c50"><span class="c10 c2">3) Therefore, the claim has strong advice.</span></p><p class="c50"><span class="c10 c2">Answer: (1) strong advice.</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">strong </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c223" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">It is important for nurses to be aware of rescue analgesia strategies, such as PCA analgesia to protect against breakthrough pain on ambulation, end-of-dose failure on controlled-release opioids, or spontaneous pain with no obvious pathology (Chiaretti &amp; Langer, 2005).</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2">1) The claim has an actionable suggestion for nurses to be aware of rescue analgesia strategies.</span></p><p class="c50"><span class="c2 c79">2) When describing the suggestion, the authors used the phrase </span><span class="c2 c56">"it is important for"</span><span class="c10 c2">, which indicates the strength of the suggestion is strong.</span></p><p class="c50"><span class="c10 c2">3) Therefore, the claim has strong advice.</span></p><p class="c50"><span class="c10 c2">Answer: (1) strong advice.</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">strong </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c63"><td class="c747" colspan="2" rowspan="1"><p class="c50"><span class="c10 c2">It is reasonable for IVF clinics to use low oxygen concentration from embryo culture beginning, and it will optimize the patients’ treatment outcome in a long-term perspective.</span></p></td><td class="c237" colspan="3" rowspan="1"><p class="c50"><span class="c10 c2">1) The claim has an actionable suggestion related to the use of low oxygen concentration in IVF clinics.</span></p><p class="c50"><span class="c2 c79">2) When describing the suggestion, the authors used the word</span><span class="c2 c79 c48"> </span><span class="c2 c56">"reasonable"</span><span class="c10 c2">, which indicates a certain level of confidence in the suggestion, making it strong advice.</span></p><p class="c50"><span class="c10 c2">3) Therefore, the claim has strong advice.</span></p><p class="c50"><span class="c10 c2">Answer: (1) strong advice.</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td><td class="c236" colspan="1" rowspan="1"><p class="c50"><span class="c10 c2">strong </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c13"><td class="c223" colspan="2" rowspan="3"><p class="c50"><span class="c10 c2">While the study findings provide information about QoL and some influencing factors among PLWHIV in Georgia, the limitations of the current study can be considered and minimized in future studies.</span></p></td><td class="c513" colspan="3" rowspan="3"><p class="c50"><span class="c10 c2 c251">1) The claim is talking about the study findings and their limitations.</span></p><p class="c50"><span class="c10 c2 c251">2) The claim suggests that the limitations can be considered and minimized in future studies, which is an actionable suggestion for future research.</span></p><p class="c50"><span class="c10 c2 c251">3) However, the claim does not use strong modal words or phrases, making the advice weak.</span></p><p class="c50"><span class="c10 c2 c251">Answer: (2) weak advice.</span></p></td><td class="c815" colspan="1" rowspan="3"><p class="c50"><span class="c10 c2">no </span></p><p class="c50"><span class="c10 c2">advice</span></p></td><td class="c535" colspan="1" rowspan="3"><p class="c50"><span class="c10 c2">weak </span></p><p class="c50"><span class="c10 c2">advice</span></p></td></tr><tr class="c63"></tr><tr class="c63"></tr><tr class="c13"><td class="c763" colspan="7" rowspan="3"><p class="c14"><span class="c2 c56">Pattern A</span><span class="c10 c2">     Incorrect reasoning step based on specific keywords</span></p><p class="c14"><span class="c2 c79 c251">Pattern B</span><span class="c10 c2">    Incorrect classification of health advice</span></p></td></tr><tr class="c63"></tr><tr class="c8"></tr></table>**Table A16 Sample of generated four-shot Chain-of-Thought from GPT-4 for reasoning in the CausalRelation**** ****test set.**

<table class="c150"><tr class="c599"><td class="c1006" colspan="2" rowspan="1"><p class="c94"><span class="c10 c25">Sentence</span></p></td><td class="c1054" colspan="3" rowspan="1"><p class="c94"><span class="c10 c25">GPT–4 Answer</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Gold</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Pred</span></p></td></tr><tr class="c137"><td class="c57" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">The present literature supports the idea that authoritative parenting may be protective against later overweight and obesity, although findings are mixed.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1) The sentence describes the relationship between "authoritative parenting" and "later overweight and obesity".</span></p><p class="c14"><span class="c10 c2">2) The authors used the language cue "protective against", which suggests a relationship between the two entities.</span></p><p class="c14"><span class="c2 c79">3) However, the authors also mentioned that </span><span class="c2 c56">"findings are mixed"</span><span class="c10 c2">, which indicates that the relationship is not definitive or certain.</span></p><p class="c14"><span class="c10 c2">4) Therefore, the sentence expresses a correlation, but with some uncertainty.</span></p><p class="c14"><span class="c10 c2">Answer: 2 - correlation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">conditional causation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">correlation</span></p></td></tr><tr class="c13"><td class="c57 c48" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">The bidirectional association between AN diagnosis and CD warrants attention in the initial assessment and follow-up of these conditions because underdiagnosis and misdiagnosis of these disorders likely cause protracted and unnecessary morbidity.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1) The sentence describes the relationship between "AN diagnosis" and "CD".</span></p><p class="c14"><span class="c2 c79">2) The authors used the phrase </span><span class="c2 c56">"bidirectional association"</span><span class="c10 c2">, which suggests a correlation between the two conditions.</span></p><p class="c14"><span class="c10 c2">3) Therefore, the sentence expresses correlation.</span></p><p class="c14"><span class="c10 c2">Answer: 2 - correlation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">conditional causation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">correlation</span></p></td></tr><tr class="c13"><td class="c57 c48" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">This figure would be below accepted international thresholds to undertake screening mammography and raises radiation protection issues.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c2 c79">1) The sentence describes the relationship between </span><span class="c2 c79 c251">"the figure being below accepted international thresholds"</span><span class="c2 c79"> and </span><span class="c2 c79 c251">"raising radiation protection issues"</span><span class="c10 c2">.</span></p><p class="c14"><span class="c10 c2">2) The authors used the verb "raises", which suggests that there is a direct effect between them.</span></p><p class="c14"><span class="c10 c2">3) When describing the effect, the authors did not use any hedge words or modal verbs to tone down the certainty of the effect.</span></p><p class="c14"><span class="c10 c2">4) Therefore, the sentence expresses direct causation.</span></p><p class="c14"><span class="c10 c2">Answer: 4 - causation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no relation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">causation</span></p></td></tr><tr class="c13"><td class="c57 c48" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">High-risk populations may accept CESM as a screening exam and may prefer it over screening MRI if ongoing trials demonstrate screening CESM to be clinically non-inferior MRI.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1) The sentence describes the relationship between "accepting CESM as a screening exam" and "ongoing trials demonstrating screening CESM to be clinically non-inferior MRI".</span></p><p class="c14"><span class="c2 c79">2) The authors used the conditional language </span><span class="c2 c56">"may accept"</span><span class="c10 c2"> and "if ongoing trials demonstrate", which suggests that there is a conditional effect between them.</span></p><p class="c14"><span class="c10 c2">3) Therefore, the sentence expresses conditional causation.</span></p><p class="c14"><span class="c10 c2">Answer: 3 - conditional causation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no relation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">conditional causation</span></p></td></tr><tr class="c13"><td class="c57" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">3D MRI reconstruction after NAC could accurately detect the residual tumors after neoadjuvant chemotherapy, and contribute to select patients who received breast conserving therapy after NAC with tumor downstaging.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1) The sentence describes the relationship between "3D MRI reconstruction after NAC" and "accurately detecting residual tumors" and "contributing to selecting patients for breast conserving therapy after NAC with tumor downstaging".</span></p><p class="c14"><span class="c10 c2">2) The authors used the verbs "could accurately detect" and "contribute to select", which suggest that there is a direct effect between them.</span></p><p class="c14"><span class="c2 c79">3) When describing the effect, the authors also used a hedge word </span><span class="c2 c56">"could"</span><span class="c10 c2"> to tone down the certainty of the effect.</span></p><p class="c14"><span class="c10 c2">4) Therefore, the sentence expresses conditional causation only.</span></p><p class="c14"><span class="c10 c2">Answer: 3 - conditional causation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">direct causation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">conditional causation</span></p></td></tr><tr class="c63"><td class="c15" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">The results showed no effects of aromatherapy on QOL, sleep quality, and vital signs.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c10 c2">1) The sentence describes the relationship between "aromatherapy" and "QOL, sleep quality, and vital sign".</span></p><p class="c14"><span class="c2 c79">2) The authors used the phrase </span><span class="c2 c56">"no effects"</span><span class="c10 c2">, which suggests that there is no relationship between the two entities.</span></p><p class="c14"><span class="c10 c2">3) Therefore, the sentence expresses no relationship.</span></p><p class="c14"><span class="c10 c2">Answer: 1 - no relationship</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">direct causation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">relation</span></p></td></tr><tr class="c13"><td class="c57" colspan="2" rowspan="3"><p class="c14"><span class="c10 c2">The rs7903146 (C/T) polymorphism of the TCF7L2 gene might not be associated with obesity in the Cameroonian population.</span></p></td><td class="c855" colspan="3" rowspan="3"><p class="c14"><span class="c10 c2">1) The sentence describes the relationship between "rs7903146 (C/T) polymorphism of the TCF7L2 gene" and "obesity in the Cameroonian population".</span></p><p class="c14"><span class="c10 c2">2) The authors used the language cue "might not be associated with" to describe the relationship, which suggests a lack of correlation.</span></p><p class="c14"><span class="c2 c79">3) The use of </span><span class="c2 c56">"might not"</span><span class="c10 c2"> indicates uncertainty, but the focus is still on the potential absence of a relationship.</span></p><p class="c14"><span class="c10 c2">4) Therefore, the sentence expresses no relationship.</span></p><p class="c14"><span class="c10 c2">Answer: 1 - no relationship</span></p></td><td class="c977" colspan="1" rowspan="3"><p class="c14"><span class="c10 c2">correlation</span></p></td><td class="c815" colspan="1" rowspan="3"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">relation</span></p></td></tr><tr class="c63"></tr><tr class="c63"></tr><tr class="c13"><td class="c57" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">This protocol will save money, time, and workload without compromising patients safety and future management.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c2 c79">1) The sentence describes the relationship between </span><span class="c2 c79 c251">"this protocol"</span><span class="c2 c79"> and </span><span class="c2 c79 c251">"saving money, time, and workload"</span><span class="c2 c79"> as well as </span><span class="c2 c79 c251">"not compromising patient safety and future management"</span><span class="c10 c2">.</span></p><p class="c14"><span class="c10 c2">2) The authors used the verb "will save" and "without compromising", which suggests that there is a direct effect between the protocol and the outcomes mentioned.</span></p><p class="c14"><span class="c10 c2">3) The certainty of the effect is strong and direct, as the authors used the word "will" to describe the effect.</span></p><p class="c14"><span class="c10 c2">4) Therefore, the sentence expresses causation.</span></p><p class="c14"><span class="c10 c2">Answer: 4 - causation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">relation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">causation</span></p></td></tr><tr class="c13"><td class="c57" colspan="2" rowspan="1"><p class="c14"><span class="c10 c2">We should have major policy changes to increase future screening programs and health education programs which would have an overall positive impact on reducing the disease burden.</span></p></td><td class="c189" colspan="3" rowspan="1"><p class="c14"><span class="c2 c79">1) The sentence describes the relationship between </span><span class="c2 c79 c251">"major policy changes"</span><span class="c2 c79"> and </span><span class="c2 c79 c251">"reducing the disease burden"</span><span class="c10 c2">.</span></p><p class="c14"><span class="c10 c2">2) The authors used the language cue "would have an overall positive impact on", which suggests that there is a direct effect between them.</span></p><p class="c14"><span class="c10 c2">3) When describing the effect, the authors used a hedge word "would" to tone down the certainty of the effect.</span></p><p class="c14"><span class="c10 c2">4) Therefore, the sentence expresses conditional causation only.</span></p><p class="c14"><span class="c10 c2">Answer: 3 - conditional causation</span></p></td><td class="c524" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">no </span></p><p class="c14"><span class="c10 c2">relation</span></p></td><td class="c181" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">conditional causation</span></p></td></tr><tr class="c13"><td class="c552" colspan="7" rowspan="3"><p class="c14"><span class="c2 c56">Pattern A</span><span class="c10 c2">     Incorrect reasoning step based on specific keywords</span></p><p class="c14"><span class="c2 c79 c251">Pattern B</span><span class="c10 c2">     Incorrect classification of reasoning relation</span></p></td></tr><tr class="c63"></tr><tr class="c8"></tr></table>

---

## Appendix B
**Table B1 HealthAdvice dataset distributions**

<table class="c576 c960"><tr class="c218"><td class="c1039" colspan="1" rowspan="1"><p class="c32"><span class="c10 c25">Task</span></p></td><td class="c903" colspan="1" rowspan="1"><p class="c32"><span class="c10 c25">Label</span></p></td><td class="c275" colspan="1" rowspan="1"><p class="c24"><span class="c10 c25">Advice in discussion sections (n)</span></p></td><td class="c600" colspan="1" rowspan="1"><p class="c24"><span class="c10 c25">Advice in unstructured abstracts (n)</span></p></td><td class="c705" colspan="1" rowspan="1"><p class="c24"><span class="c10 c25">Advice in structured abstracts (n)</span></p></td></tr><tr class="c13"><td class="c1027" colspan="1" rowspan="4"><p class="c32"><span class="c10 c2">Classification:</span></p><p class="c32"><span class="c10 c2">HealthAdvice</span></p></td><td class="c312" colspan="1" rowspan="1"><p class="c32"><span class="c10 c2">Weak advice</span></p></td><td class="c169" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">162</span></p></td><td class="c879" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">28</span></p></td><td class="c838" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">1482</span></p></td></tr><tr class="c13"><td class="c464" colspan="1" rowspan="1"><p class="c32"><span class="c10 c2">Strong advice</span></p></td><td class="c636" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">135</span></p></td><td class="c467" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">16</span></p></td><td class="c360" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">925</span></p></td></tr><tr class="c13"><td class="c464" colspan="1" rowspan="1"><p class="c24"><span class="c10 c2">No advice</span></p></td><td class="c636" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">3635</span></p></td><td class="c467" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">890</span></p></td><td class="c360" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">3575</span></p></td></tr><tr class="c13"><td class="c666" colspan="1" rowspan="1"><p class="c24"><span class="c10 c2">Total</span></p></td><td class="c402" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">3932</span></p></td><td class="c582" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">934</span></p></td><td class="c996" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">5982</span></p></td></tr></table>**Table B2 CausalRelation dataset distributions**

<table class="c576 c1011"><thead><tr class="c218"><td class="c967" colspan="1" rowspan="1"><p class="c32"><span class="c10 c25">Task</span></p></td><td class="c732" colspan="1" rowspan="1"><p class="c32"><span class="c10 c25">Label</span></p></td><td class="c790" colspan="1" rowspan="1"><p class="c32"><span class="c10 c25">Count (n)</span></p></td><tbody></tbody></tr><tr class="c13"><td class="c950" colspan="1" rowspan="5"><p class="c32"><span class="c182 c2 c210">Reasoning:</span></p><p class="c32"><span class="c182 c2 c210">CausalRelation</span></p></td><td class="c682" colspan="1" rowspan="1"><p class="c32"><span class="c10 c2">Correlational</span></p></td><td class="c1051" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">998</span></p></td></tr><tr class="c13"><td class="c998" colspan="1" rowspan="1"><p class="c32"><span class="c10 c2">Conditional causal</span></p></td><td class="c626" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">213</span></p></td></tr><tr class="c13"><td class="c998" colspan="1" rowspan="1"><p class="c24"><span class="c2 c79">Causal</span></p></td><td class="c626" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">494</span></p></td></tr><tr class="c13"><td class="c998" colspan="1" rowspan="1"><p class="c24"><span class="c10 c2">No relationship</span></p></td><td class="c626" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">1356</span></p></td></tr><tr class="c13"><td class="c1033" colspan="1" rowspan="1"><p class="c24"><span class="c10 c2">Total</span></p></td><td class="c608" colspan="1" rowspan="1"><p class="c16"><span class="c10 c2">3061</span></p></td></tr></thead></table>**Table B3 Prompt templates for zero-shot, one-shot, few-shot, and zero-shot Chain-of-Thought**

<table class="c260 c576"><tr class="c13"><td class="c888" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Prompt Template</span></p></td><td class="c602" colspan="1" rowspan="1"><p class="c94"><span class="c25 c79"> </span><span class="c25 c79">Setting</span><span class="c25 c79 c30">a</span></p></td><td class="c702" colspan="1" rowspan="1"><p class="c94"><span class="c25 c79">ID</span><span class="c182 c25 c79 c30">b</span></p></td><td class="c176" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Dataset</span></p></td></tr><tr class="c13"><td class="c880" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">"Context: {context} \n Question: is this a 2) strong advice, 1) weak advice 0) no advice?"</span></p></td><td class="c434" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Zero-shot</span></p></td><td class="c646" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">cs</span></p></td><td class="c157" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">"Context: {context} \n Question: does the context contain any medical advice? : 2: yes, 1: maybe 0: no"</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">cs</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: Is it a (0) no advice, (1) strong advice, or (2) weak advice statement?</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">yy</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: Does this claim have (1) strong advice, (2) weak advice, or there is (0) no advice?</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">One-shot</span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">yy</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: Is this (0) no advice, (1) strong advice, or (2) weak advice?</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot</span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">db</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: What type of advice is this? Select only one from: 0 – no advice, 1 - strong advice, or 2 - weak advice.</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">db</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Label the sentence as strong medical advice, weak medical advice or no medical advice</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79">Zero-shot CoT</span><span class="c182 c2 c79 c30">c</span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">gs</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c13"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n The strength of the medical advice if any in this sentence is ....</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">gs</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c218"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} -- Question: What type of relationship is this describing? Select only one from: 1) no relationship, 2) correlation, 3) conditional causation, or 4) direct causation.</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">hv</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c218"><td class="c48 c744" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} -- Question: Is the previous statement describing a (1) directly correlative relationship, (2) conditionally causative relationship, (3) causative relationship, or (0) no relationship.</span></p></td><td class="c370" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c801" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">hv</span></p></td><td class="c215" colspan="1" rowspan="1"><p class="c14 c43"><span class="c10 c2">HealthAdvice</span></p></td></tr><tr class="c218"><td class="c48 c880" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: choose from the following causal relationships: 0: None, 1: Correlational, 2: Conditional causal, 3: Direct causal?</span></p></td><td class="c434" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c646" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">cs</span></p></td><td class="c157" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c13"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: Is this a: 0) None, 1) Correlational, 2) Conditional causal, 3) Direct causal?"</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">cs</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c706"><td class="c90" colspan="1" rowspan="1"><p class="c14"><span class="c2 c79">Context: {context} \n Question: Does 1 - correlation, 2 - conditional causation, or 3 – direct causation expressed in the sentence, or </span><span class="c2 c79">it is</span><span class="c10 c2"> a 0 - no relationship sentence?</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">Few-shot</span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">yy</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c13"><td class="c90" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: Is it 1 - no relationship, 2 - correlation, 3 - conditional causation, or 4 – direct causation?</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">One-shot</span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">yy</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c218"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: What type of relationship is this describing? Select only one from: 0 - no relationship, 2 - correlation, 3 - conditional causation, or 4 – direct causation.</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Zero-shot</span></p><p class="c14"><span class="c10 c2">Zero-shot CoT</span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">db</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c218"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Question: Is this describing a (1) directly correlative relationship, (2) conditionally causative relationship, (3) causative relationship, or (0) no relationship.</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">db</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c13"><td class="c48 c90" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n Label the relation expressed in the sentence as one of correlation, conditional causation, causation or other</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">gs</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c13"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} \n The relation in the sentence is of type...</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">gs</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c218"><td class="c90 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} -- Question: What type of relationship is this describing? Select only one from: 1) no relationship, 2) correlation, 3) conditional causation, or 4) direct causation.</span></p></td><td class="c344" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c497" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">hv</span></p></td><td class="c242" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr><tr class="c218"><td class="c744 c48" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Context: {context} -- Question: Is the previous statement describing a (1) directly correlative relationship, (2) conditionally causative relationship, (3) causative relationship, or (0) no relationship.</span></p></td><td class="c370" colspan="1" rowspan="1"><p class="c14 c37"><span class="c10 c2"></span></p></td><td class="c801" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">hv</span></p></td><td class="c215" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CausalRelation</span></p></td></tr></table>

CoT = Chain-of-Thought.

aThe 2 best-performing prompts for a given setting used on the test set. Blank cell means that the prompt was not selected for evaluation on the test set.

bInitials of the author who wrote the prompt.

cThe string “Let’s think step-by-step.” was appended to the end of the prompt for zero-shot CoT**Table B4 Prompt templates for one- and few-shot Chain-of-Thought Used on the test set**

<table class="c576 c1025"><tr class="c834"><td class="c83" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Prompt Template</span></p></td><td class="c625" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Dataset</span></p></td><td class="c538" colspan="1" rowspan="1"><p class="c94"><span class="c10 c25">Setting</span></p></td></tr><tr class="c992"><td class="c83" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">"CONTEXT: Third, although our study have taken a large number of potential confounders into consideration, we could not completely rule out the possibility of unmeasured confounding.</span></p><p class="c14"><span class="c10 c2">QUESTION: Does this claim have (1) strong advice, (2) weak advice, or there is (0) no advice?</span></p><p class="c14"><span class="c10 c2">Let’s think step by step:</span></p><p class="c14"><span class="c10 c2">1) The claim is talking about limitations of the current study.</span></p><p class="c14"><span class="c10 c2">2) The claim does not have any actionable suggestions for health-related clinical or policy changes.</span></p><p class="c14"><span class="c10 c2">3) Therefore, the claim does not have strong or weak advice. It is a no advice statement.</span></p><p class="c14"><span class="c10 c2">Answer: (0) no advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: Georgian public health specialists working in the HIV field should prioritize implementation of such interventions among HIV patients.</span></p><p class="c14"><span class="c10 c2">QUESTION: Does this claim have (1) strong advice, (2) weak advice, or there is (0) no advice?</span></p><p class="c14"><span class="c10 c2">Let’s think step by step:</span></p><p class="c14"><span class="c10 c2">1) The claim has an actionable suggestion related to the implementation of an intervention.</span></p><p class="c14"><span class="c10 c2">2) When describing the suggestion, the authors used modal word "should", which indicates the strength of the suggestion and it is strong.</span></p><p class="c14"><span class="c10 c2">3) Therefore, the claim has strong advice.</span></p><p class="c14"><span class="c10 c2">Answer: (1) strong advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients, suggesting that ANP could potentially be used to prevent cancer recurrence after surgery.</span></p><p class="c14"><span class="c10 c2">QUESTION: Does this claim have (1) strong advice, (2) weak advice, or there is (0) no advice?</span></p><p class="c14"><span class="c10 c2">Let’s think step by step:</span></p><p class="c14"><span class="c10 c2">1) The claim has an actionable suggestion for the use of ANP to prevent cancer recurrence after surgery.</span></p><p class="c14"><span class="c10 c2">2) When describing the suggestion, the authors used a hedged phrase "could potentially be used to", which indicates the strength of the claim is not strong but weak.</span></p><p class="c14"><span class="c10 c2">3) Therefore, the claim has weak advice.</span></p><p class="c14"><span class="c10 c2">Answer: (2) weak advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: {context}</span></p><p class="c14"><span class="c10 c2">QUESTION: Does this claim have (1) strong advice, (2) weak advice, or there is (0) no advice?</span></p><p class="c14"><span class="c10 c2">Let’s think step by step:</span></p></td><td class="c625" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Classification: </span></p><p class="c14"><span class="c10 c2">Advice in </span></p><p class="c14"><span class="c10 c2">discussion </span></p><p class="c14"><span class="c10 c2">sections</span></p></td><td class="c538" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT</span></p></td></tr><tr class="c627"><td class="c83" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CONTEXT: Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening.</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p><p class="c14"><span class="c10 c2">ANSWER:</span></p><p class="c14"><span class="c10 c2">1. The context states a fact, which is not an advice.</span></p><p class="c14"><span class="c10 c2">2. The answer is no advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p><p class="c14"><span class="c10 c2">ANSWER:</span></p><p class="c14"><span class="c10 c2">1. The term "should be included" indicate that there is an advice in the context.</span></p><p class="c14"><span class="c10 c2">2. "Should be" indicates a strong opinion.</span></p><p class="c14"><span class="c10 c2">3. The answer is strong advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p><p class="c14"><span class="c10 c2">ANSWER:</span></p><p class="c14"><span class="c10 c2">1. The term "may not be indicated" indicates an advice in the context.</span></p><p class="c14"><span class="c10 c2">2. "May not" indicates a weak opinion.</span></p><p class="c14"><span class="c10 c2">3. The answer is weak advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: {context}</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p></td><td class="c625" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Classification: </span></p><p class="c14"><span class="c10 c2">Advice in </span></p><p class="c14"><span class="c10 c2">unstructured </span></p><p class="c14"><span class="c10 c2">abstracts</span></p></td><td class="c538" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT</span></p></td></tr><tr class="c572"><td class="c83" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">CONTEXT: Further mechanistic research in larger cohorts is necessary to reconcile the potential role of T2D in UF risk.</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p><p class="c14"><span class="c10 c2">ANSWER:</span></p><p class="c14"><span class="c10 c2">1. The context states the necessity of further research, which is not an advice.</span></p><p class="c14"><span class="c10 c2">2. The answer is no advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: Since blood pressure problems run a worse course in Blacks, we recommend encouragement of night-time intake in those preferring it and suggest that in those requiring two or more drugs one should be taken at night.</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p><p class="c14"><span class="c10 c2">ANSWER:</span></p><p class="c14"><span class="c10 c2">1. The terms "we recommend" and "suggest" indicate that there is an advice in the context.</span></p><p class="c14"><span class="c10 c2">2. "Recommend" and "suggest" indicate a directive opinion.</span></p><p class="c14"><span class="c10 c2">3. The answer is strong advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: Therefore, this regimen would be a viable option for acne treatments either as monotherapy or as combination therapy.</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p><p class="c14"><span class="c10 c2">ANSWER:</span></p><p class="c14"><span class="c10 c2">1. The term "would be a viable option" indicates that there is an advice in the context.</span></p><p class="c14"><span class="c10 c2">2. "Would be" indicates that the opinion is not strong.</span></p><p class="c14"><span class="c10 c2">3. The answer is weak advice.</span></p><p class="c14 c37"><span class="c10 c2"></span></p><p class="c14"><span class="c10 c2">CONTEXT: {context}</span></p><p class="c14"><span class="c10 c2">QUESTION: Is this a (0) no advice, (1) weak advice, or (2) strong advice? Let's think step by step.</span></p></td><td class="c625" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Classification: </span></p><p class="c14"><span class="c10 c2">Advice in </span></p><p class="c14"><span class="c10 c2">structured </span></p><p class="c14"><span class="c10 c2">abstracts</span></p></td><td class="c538" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT</span></p></td></tr><tr class="c1041"><td class="c83" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2 c48">"CONTEXT: The high rate of text message usage makes it feasible to recruit YAMs for a prospective study in which personalized text messages are used to promote healthy behaviors.</span></p><p class="c14"><span class="c10 c2 c48">QUESTION: Does 2 - correlation, 3 - conditional causation, or 4 – direct causation expressed in the sentence, or it is a 1 - no relationship sentence?</span></p><p class="c14"><span class="c10 c2 c48">Let’s think step by step: </span></p><p class="c14"><span class="c10 c2 c48">1) The sentence does not describe any correlation or causation relation between two entities.</span></p><p class="c14"><span class="c10 c2 c48">2) It is a no relationship sentence.</span></p><p class="c14"><span class="c10 c2 c48">Answer: 1 - no relationship</span></p><p class="c14 c37"><span class="c10 c2 c48"></span></p><p class="c14"><span class="c10 c2 c48">CONTEXT: The incidence of falls and poor quality of life may be partially associated with the presence of depression.</span></p><p class="c14"><span class="c10 c2 c48">QUESTION: Does 2 - correlation, 3 - conditional causation, or 4 – direct causation expressed in the sentence, or it is a 1 - no relationship sentence?</span></p><p class="c14"><span class="c10 c2 c48">Let’s think step by step: </span></p><p class="c14"><span class="c10 c2 c48">1) The sentence describes the relationship between "the incidence of falls and poor quality of life" and "depression".</span></p><p class="c14"><span class="c10 c2 c48">2) The relationship between them is correlation, as the authors used the language cue "associated with" to describe it. "Associated with" is a commonly used expression to indicate correlation.</span></p><p class="c14"><span class="c10 c2 c48">3) Therefore, the sentence expresses correlation.</span></p><p class="c14"><span class="c10 c2 c48">Answer: 2 - correlation</span></p><p class="c14 c37"><span class="c10 c2 c48"></span></p><p class="c14"><span class="c10 c2 c48">CONTEXT: Our study provides preliminary evidence that mothers who consume diets higher in fruit and lower in fried foods and cured meats during pregnancy may reduce the risk of unilateral retinoblastoma in their offspring.</span></p><p class="c14"><span class="c10 c2 c48">QUESTION: Does 2 - correlation, 3 - conditional causation, or 4 – direct causation expressed in the sentence, or it is a 1 - no relationship sentence?</span></p><p class="c14"><span class="c10 c2 c48">Let’s think step by step: </span></p><p class="c14"><span class="c10 c2 c48">1) The sentence describes the relationship between "diets higher in fruit and lower in fried foods and cured meats" and "the risk of "unilateral retinoblastoma in offspring".</span></p><p class="c14"><span class="c10 c2 c48">2) The authors used the verb "reduce", which suggests that there is a direct effect between them.</span></p><p class="c14"><span class="c10 c2 c48">3) When describing the effect, the authors also used a hedge word "may" to tone down the certainty of the effect.</span></p><p class="c14"><span class="c10 c2 c48">3) Therefore, the sentence expresses conditional causation only.</span></p><p class="c14"><span class="c10 c2 c48">Answer: 3 - conditional causation</span></p><p class="c14 c37"><span class="c10 c2 c48"></span></p><p class="c14"><span class="c10 c2 c48">CONTEXT: The nutritional course for patients undergoing colon surgery can be improved by implementing early oral nutritional supplements in the PACU.</span></p><p class="c14"><span class="c10 c2 c48">QUESTION: Does 2 - correlation, 3 - conditional causation, or 4 – direct causation expressed in the sentence, or it is a 1 - no relationship sentence?</span></p><p class="c14"><span class="c10 c2 c48">Let’s think step by step: </span></p><p class="c14"><span class="c10 c2 c48">1) The sentence describes the relationship between "nutritional course" and "the risk of "oral nutritional supplements in the PACU".</span></p><p class="c14"><span class="c10 c2 c48">2) The authors used the verb "can be improved by", which suggests that there is a direct effect between them.</span></p><p class="c14"><span class="c10 c2 c48">3) When describing the effect, the authors also used a modal verb "can", suggesting that the certainty of the effect is strong and direct. And no hedge word is used to tone down the certainty.</span></p><p class="c14"><span class="c10 c2 c48">3) Therefore, the sentence expresses causation.</span></p><p class="c14"><span class="c2 c79 c48">Answer: 4 - causation</span></p></td><td class="c625" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Reasoning: </span></p><p class="c14"><span class="c10 c2">Causal relation detection</span></p></td><td class="c538" colspan="1" rowspan="1"><p class="c14"><span class="c10 c2">Few-shot CoT</span></p></td></tr></table>
