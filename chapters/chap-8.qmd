# Chapter 8

# **<u>Chapter 8</u>**

Cross-care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias

---***Shan Chen****, Jack Gallifant*, Mingye Gao, Pedro Moreira, Nikolaj Munch, Ajay Muthukkumar, Arvind Rajan, Jaya Kolluri, Amelia Fiske, Janna Hastings, Hugo Aerts, Brian Anthony,

Leo Anthony Celi, William La Cava, Danielle Bitterman

Advances in neural information processing systems

**Summary** **Background** The rapid adoption of large language models (LLMs) in healthcare prompts critical questions about their grounding in real-world medical data. Many benchmarks focus on general reasoning or narrow tasks, but few systematically assess how LLMs represent disease prevalence across demographic groups. This gap is especially concerning given the potential for biased or mis-represented outputs in high-stakes clinical settings.**Methods** We introduce Cross‑Care, a benchmark framework designed to evaluate biases and real-world knowledge of LLMs regarding disease prevalence by demographic subgroup (gender and race/ethnicity) and across multiple languages (English, Spanish, French, Chinese). We first quantify disease-demographic co-occurrences in a large pre-training corpus (The Pile) and then compare LLM logit rankings for disease-subgroup pairs against epidemiological prevalence data sourced from U.S. national surveys. Our evaluation covers “controlled” models trained solely on The Pile (e.g., Pythia, Mamba) and “models in the wild” (publicly deployed LLMs of varying size, alignment status and language-pretraining). And we also analyze the effects of alignment/fine-tuning strategies on these bias metrics.**Findings** Across 89 diseases and multiple demographics, the ranking of subgroup disease prevalence in LLM logits correlated strongly with co-occurrence frequencies in The Pile but showed nearly zero correlation with actual epidemiological prevalences. Models consistently favoured White and male subgroups for many diseases, even when real-world data would indicate Indigenous or female populations as more affected. Alignment methods and multilingual evaluation revealed that biases persisted across languages and tuning types; none of the alignment strategies materially improved grounding in real-world prevalence data.**Interpretation** The findings expose a profound disconnect: LLMs in current form appear to encode demographic-disease associations primarily from training-corpus frequencies, not from true epidemiological reality. This suggests serious risks for using LLMs in clinical or policy contexts without addressing representational bias and real-world grounding. Future efforts must incorporate prevalence-aware pre-training, more diverse multilingual data, and robust subgroup analysis to ensure fairness and validity in healthcare AI.**INTRODUCTION**

Large language models (LLMs) enabled transformative progress in many applications[1–5](https://paperpile.com/c/coPcdt/rn7n+EbvY+o7lT+JxIE+h6P0). Benchmarks to assess language models, such as GLUE[6](https://paperpile.com/c/coPcdt/AOzs) and SuperGLUE[7](https://paperpile.com/c/coPcdt/bUun), are instrumental in evaluating general language understanding and complex task performance. However, as LLMs are increasingly applied in diverse domains, challenges of domain knowledge grounding[8–12](https://paperpile.com/c/coPcdt/V1vd+rh6z+xbIg+zMVp+v2iZ), safety[13–17](https://paperpile.com/c/coPcdt/9Bi9+CLhj+LMNd+wt4h+cNDA), hallucinations[18,19](https://paperpile.com/c/coPcdt/mWz4+lt4w), and bias[20–23](https://paperpile.com/c/coPcdt/4oGx+RImu+rIap+jsIO) have emerged as important issues that are inadequately assessed by existing benchmarks. These problems are magnified in high-stakes domains like healthcare, given the potential for biased or inaccurate outputs[24–26](https://paperpile.com/c/coPcdt/uPo4+SrkJ+94S0) to influence disparities in healthcare and outcomes.

This paper investigates**representational biases in LLMs, focusing on medical information**. Our research explores the interplay between biases in pretraining datasets and their manifestation in LLMs’ perceptions of disease demographics. Existing bias metrics in the general domain have currently relied on human-annotated examples and focused on overt stigmatization and prejudices. In contrast, our work examines bias through a different paradigm rooted in real-world data to provide a domain-specific framework for assessing model biases and grounding. We demonstrate this gap using sub-populations defined by United States census categories for gender and race/ethnicity, and normalized disease codes. While these categorizations are necessarily simplistic and imperfect, we contend that the fact that inconsistency is consistently observed across model architectures, model sizes, subgroups, and diseases means that these findings are meaningful and broadly relevant. We aim to provide a foundation for future research that evaluates the subgroup robustness of LLM associations and equip researchers and practitioners with tools to uncover and understand the biases inherent in their models, thereby facilitating the development of more equitable and effective NLP systems for healthcare. Our full workflow can be found in Figure 1.

Specifically, our work makes the following key contributions:

1.**We conduct a quantitative analysis of the co-occurrences between demographic subgroups and disease keywords** in prominent pretraining datasets like *The Pile*, releasing their counts publicly.
2. **We evaluate model logits across various architectures, sizes, and alignment methods** using ten prompt template variants to test robustness to disease-demographic subgroup pairs. Our findings reveal that representational differences in pretraining datasets across diseases align with these logits, irrespective of model size and architecture.
3.**We benchmark model-derived associations against real-world disease prevalences** to highlight discrepancies between model perceptions and actual epidemiological data. Additionally, we compare these associations across different languages (Chinese, English, French, and Spanish) to emphasize discrepancies across languages.
4.**We provide a publicly accessible web app**,[https://www.crosscare.net/](https://www.crosscare.net/)[<u>https://www.crosscare.net</u>](https://www.crosscare.net/), for exploring these data and downloading specific counts, logits, and associations for further research in interpretability, robustness, and fairness. The full appendix of this work is at: [<u>https://www.proceedings.com/079017-0749.html</u>](https://www.proceedings.com/079017-0749.html)

![](../figures/image53.png)

*Figure 1. Overall workflow of Cross-Care.*

**Related Work**
**Language model biases arise from pretraining data**

The sheer breadth of data sources consumed by LLMs enables the emergence of impressive capabilities across a wide range of tasks[27](https://paperpile.com/c/coPcdt/nFS4). However, this expansive data consumption has its pitfalls, as while LLM performance generally improves as models are scaled, this improvement is not uniformly distributed across all domains[28](https://paperpile.com/c/coPcdt/UVGC). Furthermore, it can lead to the phenomenon of “bias exhaust”—the inadvertent propagation of biases present in the pretraining data. The propensity of LLMs to inherit and perpetuate societal biases observed in their training datasets is a well-documented concern in current LLM training methodologies[29–31](https://paperpile.com/c/coPcdt/K3wJ+kzHl+Eekj). Efforts to mitigate this issue through the careful selection of “clean” data have significantly reduced toxicity and biases[32,33](https://paperpile.com/c/coPcdt/5nHk+vl8a), underscoring the link between the choice of pretraining corpora and the resultant behaviors of the models. Furthermore, recent studies have elucidated the impact of pretraining data selection on the manifestation of political biases at the task level[34](https://paperpile.com/c/coPcdt/UzXQ).

###**Evaluating language model biases**

The evaluation of biases in NLP has evolved to distinguish between intrinsic and extrinsic assessments[35](https://paperpile.com/c/coPcdt/yteq). Intrinsic evaluations focus on the inherent properties of the model, while extrinsic evaluations measure biases in the context of specific tasks. This distinction has become increasingly blurred with advancements in language modeling, such as fine-tuning and in-context learning, expanding the scope of what is considered “intrinsic” [36](https://paperpile.com/c/coPcdt/pTvN).

In the era of static word embedding models, such as word2vec[37](https://paperpile.com/c/coPcdt/z3Ih) and fastText[38](https://paperpile.com/c/coPcdt/MMre), intrinsic evaluations were confined to metrics over the embedding space. Unlike static word embedding models, LLMs feature dynamic embeddings that change with context and are inherently capable of next-word prediction, a task that can be applied to numerous objectives. To evaluate bias in LLMs, Guo and Caliskan[39](https://paperpile.com/c/coPcdt/xyIK) developed the Contextualized Embedding Association Test, an extension of the Word Embedding Association Test. Other intrinsic metrics for LLMs include StereoSet[40](https://paperpile.com/c/coPcdt/QfiW) and ILPS[41](https://paperpile.com/c/coPcdt/kPjs), which are based on the log probabilities of words in text that can evoke stereotypes.

Probability-based bias evaluations such as CrowS-Pairs[22](https://paperpile.com/c/coPcdt/rIap) and tasks in the BIG-bench benchmarking suite[42](https://paperpile.com/c/coPcdt/DAS6) compare the probabilities of stereotype-related tokens conditional on the presence of identity-related tokens. These evaluations provide insights into the model’s biases by examining the likelihood of generating stereotype-associated content. Downstream, various benchmarks evaluate LLM bias with respect to languages[43–45](https://paperpile.com/c/coPcdt/WOTi+Q8T9+YEEb), genders and ethnicity[23,46–48](https://paperpile.com/c/coPcdt/uvck+jsIO+rAJt+M3HN), culture[49](https://paperpile.com/c/coPcdt/rhc0) and beyond[40,50](https://paperpile.com/c/coPcdt/CuaR+QfiW). To the best of our knowledge[51–53](https://paperpile.com/c/coPcdt/ZGgZ+2r7Z+QQaD), our work is the first to bridge gender & ethnicity biases with real-world knowledge and multi-language evaluation.**METHODS**
**Datasets** This study used The Pile dataset (deduplicated version), an 825 GB English text corpus created specifically for pre-training autoregressive LLMs[54](https://paperpile.com/c/coPcdt/Dq83), such as open-source LLMs pythia[33](https://paperpile.com/c/coPcdt/vl8a) and mamba[55](https://paperpile.com/c/coPcdt/cux2). The open access to training data and resulting model weights makes it ideal for studying how biomedical keyword co-occurrences in pre-training data affect model outputs.**Co-occurrence pipeline updates** Our co-occurrence analysis methodology builds upon the approach outlined in our previous work[56](https://paperpile.com/c/coPcdt/na2B), incorporating three key modifications: updated and verified keywords, multithread support, and real-world prevalence calculation. Full methodological details are available in the preprint and the associated GitHub repository. All co-occurrences were calculated using a single machine with 64 cores and 512 GB RAM; each checkpoint took approximately 72 hours with a total of 26 checkpoints.

-**Modification 1: Updated Keywords.** Two physician authors (JG and DB) expanded and updated keywords to cover a broad range of conditions and demographics based on PubMed MeSH terms and SNOMED CT headers. The keywords for demographic groups were adapted from the HolisticBias dataset[57](https://paperpile.com/c/coPcdt/co6w), aiming to align with previous studies investigating representational harms in biomedical LLMs. A hierarchical keyword definition strategy was used, including primary terms, variations, and synonyms for each disease and demographic group. The resulting dictionaries include 89 diseases, 6 race/ethnicity subgroup categories, and 3 gender subgroup categories. The list of dictionaries was proofread and expanded by a cultural anthropologist.
-**Modification 2: Multithreading.** Text pre-processing was completed as in the original workflow; however, it was parallelized using multithreading to enable scaling of the number of keywords utilized to maximize robustness and collection of results. Named entity recognition (NER) tagger methods that could aid delineation of the use of specific keywords in a specific context (e.g., “white” or “black” referring to race, versus in other use cases, e.g., “white blood cells”) were initially trialed. However, it became ineffective at this scale due to computational and time constraints and was not used in the final analysis. We used windows of 50–250 tokens to capture co-occurrences between disease and demographic keywords. This range was chosen based on the intuition that, if in relation to one another, disease and demographic keywords should appear within one sentence to a short paragraph of one another and that longer distances would tend to capture spurious co-occurrences.
-**Modification 3: Real-world prevalence.** To estimate the prevalence of diseases across subgroups, we used a standardized process to review the literature for each disease listed in our dictionary, focusing on prevalence and incidence within the USA across various subgroups. A detailed explanation of the approach and search strategy employed is available in Appendix. Over two-thirds of the diseases encountered significant heterogeneity in reporting standards, compromising data consistency and reliability. Only 15 out of the 89 diseases had prevalence data readily available from official CDC statistics sourced from the National Health Interview Survey. Given these constraints, our analysis focused on these 15 diseases, each with data available for at least five of the six race/ethnicity subgroups. Data for only male and female gender subgroups were available. Age-adjusted prevalences were normalized to rates per 10,000 for a consistent scale, facilitating preliminary benchmarking. These data are intended to provide a baseline for initial comparisons and relative ranking among subgroups rather than granular prevalence statistics for population health applications.**Validation of Keyword Frequency and Document Co-Occurrence** To contrast our methods with the current state of the art, we utilized Infini-gram, an engine designed for processing n-grams of any length[58](https://paperpile.com/c/coPcdt/x6hn). This is a publicly accessible API that has precomputed tokenized text across multiple large text corpora. The overall counts were then aggregated using the same dictionary mapping as above to compute the co-occurrence counts. [Footnote] Infini-gram counts are available with the Pile counts online at www.crosscare.net/downloads.**Mathematical Description of Prevalence Calculation Using Average Logits**
**Definitions and Variables**
**Models - **Let**M = {m₀, m₁, …, mₙ}** denote the collection of models.**Languages - **Let**L = {l₁, l₂, …, lₖ}** represent the set of languages.**Diseases - **Let**D** be the complete set of diseases under study.**Demographic subgroups - **Let**S** denote the set of all demographic subgroups.**Templates.** For each disease**d**, demographic subgroup**s**, and language**l**, define**T₍d,s,l₎ = {t₀ … t₉} **as the set of ten templates describing disease prevalence.

##**Logits Definition**

In this context, a**logit** refers to the raw output score from the model’s final layer before any normalization (such as a softmax) is applied. These values represent the model’s unnormalized preference for predicting different tokens.

##**Average Logits per Disease–Subgroup–Language–Model**

For each model**m**, language**l**, disease**d**, and demographic subgroup**s**, we compute the average logit as:

![](../figures/image19.png)

This produces a single scalar value per disease–subgroup–language–model combination, enabling direct comparison across demographic groups.**Model’s Disease–Demographic Ranking**

For each model**m** and disease**d**, we define**Rᵐ₍d,l₎(s) ∈ {1, |S|} **as the rank assigned to subgroup**s**, under language**l. **based on the average logits (higher logit = higher rank). (For simplicity, we drop the language distinction below.) This ranking was determined based on the average logit values, which reflect the model’s predicted disease prevalence within those demographic subgroups. This model-centric approach sheds light on the inherent biases in model predictions and facilitates comparisons with empirical data distributions.

Additionally, we propose an alternative ranking method that analyzes disease subgroups based on their co-occurrences within The Pile, as well as our “gold" subset derived from real-world data. This empirical method bypasses model outputs, directly measuring disease representation across different demographic contexts.**Comparing Rank Order Lists**

To compare subgroup rankings across different models or baselines, we use**Kendall’s τ** correlation coefficient.**Variance / Drift in Disease Ranking**

Given a sequence of models**M = {m₀, m₁, …, mₙ}**, where**m₀** is the base model and the remaining models are alignment variants, we quantify how rankings shift across models.

Let**Rᵐ₍d₎(s)** denote the rank of subgroup**s** for disease**d** under model**m**. This approach allows us to track the progression and impacts of algorithmic adjustments over multiple iterations.**Ranking Variance Analysis: **To understand how disease rankings vary as models undergo finetuning or alignment with different strategies, we quantified the drift in disease rankings from a base model to its aligned iterations, assessing the impact of alignment interventions.

###**1. Compute Kendall’s τ per disease**

For each disease**d**, compare the ranking under the base model**m₀** with the ranking under a model**m**:

![](../figures/image82.png)

where the sum is taken over all unordered pairs of distinct subgroups**sᵢ, s****j**** ∈ S**. with**i < j** denoting that each pair of elements is only compared once. Secondly, we computed the average Kendall’s tau for all diseases and demographic subgroups between the two models, evaluating the overall drift from the base model’s ranking. A τ value of 1 means perfect agreement; −1 means complete reversal.

###**2. Compute overall ranking stability across diseases**

We then average τ values across all diseases:

![](../figures/image70.png)

This quantity δᵐ summarizes how much the aligned model**m** drifts away from the base model**m₀** in terms of disease-demographic ranking patterns.

Higher values indicate greater stability; lower values indicate stronger alignment-induced shifts.

###**Definition of Controlled and In-the-Wild**

-**The controlled group** includes Mamba and Pythia models, strictly pre-trained on The Pile only. We compare these models’ representations of disease prevalence against real-world prevalence and Pile co-occurrence prevalence.
-**Models in the wild** are publicly accessible models with varied training/tuning datasets, including base models (Llama2, Llama3, Mistral, Qwen1.5; 7B and 70B) and aligned variants (e.g., RLHF[59](https://paperpile.com/c/coPcdt/3lDg) , SFT[60](https://paperpile.com/c/coPcdt/B012), DPO[61](https://paperpile.com/c/coPcdt/ts44)), plus biomedical continued pre-training. We evaluated logits in four languages (English, Spanish, French, Chinese). This dual setup enables comprehensive analysis of how models represent disease prevalence across languages and how alignment might alter it.**Experimental Framework** We designed a controlled framework to probe logit differences when changing only demographics or disease keywords. We created**10** templates that pair a demographic relation with a disease term (e.g., “*[Disease] patients are usually [Demographic Group] in America*”). We used GPT-4 to translate English templates into Chinese, French, and Spanish, then refined translations with native speakers. To ensure robustness, we explored template variants and evaluated averages, ranks, and individual-template results.

### **RESULTS**
**Variation Across Windows** We evaluated ranks across token windows of 50, 100, and 250 for each disease–demographic pair. No differences were observed in the top disease rank across window sizes. For simplicity, subsequent analyses use the 250-token window; raw counts for all window sizes are available on our website.**Demographic Distributions** We collected all 89 disease co-occurrences in The Pile and 15 real-world prevalences from CDC. In The Pile, the White subgroup was most frequently represented (87/89), followed by Black and Hispanic with relatively lower counts; Pacific Islanders and Indigenous were least represented. By contrast, real-world statistics often rank Indigenous highest, followed by White and Black. For gender in The Pile, the male subgroup appears more frequently than the female subgroup for the reported diseases, with non-binary least represented.

Figure 2 shows demographic subgroup ranks according to real-world prevalence, The Pile co-occurrence counts, and Llama3 logits for the 15 diseases with real-world prevalence available. This highlights both discrepancies and alignments between dataset co-occurrence representations and actual demographic prevalence. Raw counts and rankings (The Pile vs. NHIS) are detailed in Appendix A.3 from the original paper: [<u>https://www.proceedings.com/079017-0749.html</u>](https://www.proceedings.com/079017-0749.html)

![](../figures/image10.png)***Figure 2. Overall comparison*** *Comparison of disease rankings between The Pile (Blue), Llama3’s logits (Green), and real-world data (Red). Marker position indicates ranking for a given disease–demographic pair (1 = most prevalent, 5 = least prevalent). For example, for “Perforated Ulcer,” The Pile ranked White as most prevalent, Llama3 logits second, and real prevalence third.*

#### **Models in the Controlled Group**
**Logits Rank vs Co-occurrence** For each Pythia/Mamba model in the controlled group, we calculated model logits for all disease–demographic subgroup pairs to obtain the demographic rank of each disease; then we counted each demographic subgroup at the target position (top, bottom, and second bottom) across 89 disease-specific ranks. We also obtained similar rankings based on disease–demographic co-occurrence in The Pile with a 250-token window.

In Figure 3, the stacked bars show the variation of top demographic subgroup counts across 89 diseases along with increasing size of Pythia (left) and Mamba (right) models, while the black line shows the number of diseases for which the top-ranked demographic subgroup based on model logits matched that based on co-occurrence counts. For gender, male was the top subgroup in The Pile for 59/89 diseases. In general, for both Pythia and Mamba models, the larger the model was, the less the demographic distribution from model results followed the distribution in the pretraining dataset. For both the logits and co-occurrence counts, non-binary was never the top gender subgroup.

For race/ethnicity, we observed variation across models and model sizes in the concordance of logit ranking compared to rankings in The Pile pretraining data (Figure 3). Black and White subgroups were consistently ranked highly in the likelihood of disease across a wide range of conditions. In contrast, there were limited occurrences of ranking other subgroups in the top position. Overall, the agreement between co-occurrence rank in The Pile and the model logits rank for the highest-ranking demographic subgroup was generally poor.

The discrepancy between model logits and co-occurrence was also apparent in the second-lowest ranked race/ethnicity subgroup. As shown in the Appendix (bottom subplots in Figure 7, Hispanic was the second-bottom ranked subgroup for almost all 89 diseases based on Pythia and Mamba logits, while disease–demographic co-occurrence in The Pile indicated that Indigenous was the second-bottom subgroup for 86/89 diseases. In contrast, there was strong agreement between model logits and co-occurrence in the bottom rank counts, where Pacific Islander was ranked lowest based on both model logits and co-occurrence.

![](../figures/image56.png) Figure 3*. ****(a)**** Top-ranked gender (top) and race/ethnicity (bottom) subgroups across 89 diseases and the suite of Pythia and Mamba models according to logits (stacked bars). The black line shows the number of diseases where the top-ranked subgroup matches between logits and co-occurrences.****(b)**** Kendall’s tau of Mamba and Pythia logits vs. co-occurrence and real prevalence for gender (top) and race/ethnicity (bottom). Overlap of green and blue lines indicates consistency across the subset and the full 89 diseases; gaps to the red line highlight stronger association with co-occurrences than real-world prevalence.*

**Logits Rank vs Co-occurrence vs Real Prevalence** The Kendall’s tau scores comparing logit rankings against real-world prevalence rankings were near zero across all model sizes, indicating no correlation for both race/ethnicity and gender (Figure 3). This suggests that the logit rankings of diseases by demographic subgroups did not align with real-world prevalence and demonstrates a lack of grounding in real-world medical knowledge. However, most of the time, Mamba and Pythia showed stronger correlation with The Pile co-occurrence than with real-world prevalence rankings, especially among gender subgroups.**Rank vs Co-occurrence Counts** The analysis of Kendall’s tau scores across quartiles of overall disease co-occurrence counts in The Pile revealed consistent relationships for both race/ethnicity. Notably, the relationship between frequency of co-occurrences and logit correlations did not vary significantly across quartiles. This indicates that diseases most frequently mentioned in the dataset did not demonstrate a corresponding improvement in the correlation of logits, suggesting that model performance did not scale with mention frequency in pretraining data.

####**Models in the Wild**

For all models tested across size, alignment method, and language, no model’s disease-logits rankings had τ > 0.35 (Min = −0.73, Max = 0.33, Median = −0.05, Avg = −0.06, Var = 0.03) for gender or race/ethnicity, suggesting none had good knowledge of real-world prevalence. Figure 2 illustrates discrepancies between Llama3 logits compared to The Pile and real-world prevalences. These discrepancies might lead to incorrect and/or biased judgments in healthcare settings.**Variation across Alignment Strategies** The impact of different alignment strategies on the Llama2-70B series for both race/ethnicity and gender is shown in Figure 4. None of the alignment methods nor in-domain continued pretraining corrected the base model toward more accurate reflections of real-world prevalence. In fact, we observed some debiasing strategies during alignment adversely impacting decisions. All Llama2-70B alignment methods increased preference for female over male subgroups and decreased preference for the Black subgroup, in English. A similar observation was seen for the Mistral family. For Qwen1.5-7B base vs. Qwen1.5-7B chat in English, PPO+DPO shifted preference to the Indigenous instead of Asian subgroup.

For the Llama2-70B series, models tuned by SFT or DPO did not change the rank-ordering of race/ethnicity subgroups (δ ≥ 0.8). Models that showed noticeable variation were Meditron (continued pretraining on medical domain data) and the chat version with RLHF. Similar trends were observed for Mistral’s gender results, where Bio-Mistral underwent continued biomedical pretraining.

![](../figures/image2.png) *Figure 4. Top-ranked gender and race/ethnicity subgroups across each of the 89 diseases and different alignment methods for Llama2 models (stacked bars). Changes from base to tuned models illustrate the varying impact of alignment. Variation is not uniform across languages.*

**Models’ Representation Across Different Languages** We also observed differences across languages (Figure 4). For all Mistral and Llama series models, there was a preference toward the female subgroup in Chinese but male in French. In Qwen, there was an overall preference toward male in Chinese, Spanish, and French but female in English.

For race/ethnicity, English and Spanish templates showed preference for the Black subgroup, while Chinese and French templates showed preference for the White subgroup. Interestingly, for Qwen1.5 models (pretrained mostly on English and Chinese), we observed strong bias toward the Asian subgroup in Chinese and English templates, and toward the Black subgroup in Spanish and French templates.

We do not have a definitive explanation for this finding, but prior work suggests models hold different representations across languages[45,62,63](https://paperpile.com/c/coPcdt/YEEb+AiWF+2Zw4). As alignment methods mostly altered choices within the language of preference data, we theorize that the pretraining data mixture is a more important determinant of internal beliefs. This suggests that continuing pretraining on in-domain text might not alleviate the problem.**Limitations** This study has limitations that should be considered when interpreting the findings:

1.**Lack of NER Tagger:** Without integrating NER taggers, there is a risk of misclassifying terms or missing context. However, we were limited by the computational requirements of NER tagging over the entire The Pile dataset.
2.**Selection of Diseases:** The chosen diseases and keywords are based on normalized concepts and standard disease classification terms. This selection, though extensive, does not encompass the entire spectrum of medical knowledge, which could skew findings.
3.**Subgroup Selection:** To demonstrate variation across subgroups, we used terms from CDC national and U.S. surveys as grouping categories for quantifying subgroup robustness. While these terms reflect surface-level attributes, they can be overly simplistic and may perpetuate negative stereotypes if used to polarize. Our objective is to showcase variation using commonly recognized terms, though we hope future work will expand on our approach to delve deeper into the complexities of subgroup robustness. This should be driven by locally designed and governed frameworks. The demographic categories were constrained by the granularity of data available in national statistics, which inherently limits their precision. This approach overlooks more nuanced biases, such as intersectionality, and may unintentionally contribute to stereotyping. Addressing these real-world biases requires better data collection and distribution to empower future efforts in tackling these challenges effectively.
4.**Real-World Data Constraints:** The datasets used to determine real-world disease prevalence are limited by their availability, completeness, and collection biases. This may hinder the assessment of the broader impacts of findings.
5.**Template Sensitivity:** The model’s output sensitivity to semantic nuances in template design means the set number of templates may not capture all linguistic or contextual variations influencing model logits and bias assessment. At least one native speaker for each language verifies all translations of our templates. However, the authors acknowledge the translations can be subjective.
6.**API access model evaluation:** Because most API providers do not provide logit access to models nor model weights, these models cannot be evaluated the same way as we evaluated open-weight models. Therefore, we did not include any API-only access model research.
7.**Assessing knowledge in pretraining data and models:** Other ways to assess knowledge represented in pretraining data and model representations include investigating direct statements about prevalence in the pretraining data and querying prevalence rates from the model. However, we were interested in the more general question of how general distributions in pretraining data contribute to model biases concerning medical reasoning, more broadly, beyond factoid knowledge.**Future Work** Future research will prioritize:

1.**Development of Comprehensive Datasets:** Efforts will be made to create and employ datasets that provide more accurate and exhaustive real-world prevalence data for diseases, especially those poorly represented in existing datasets.
2.**Impact on Clinical Decision-Making:** We plan to investigate the effects of model biases on downstream tasks and clinical decision-making to improve model training and evaluation to mitigate negative impacts.
3.**Ability of Information Provided In Context to Update Prevalence Estimates:** Future work will explore the potential of providing information in context, for example using retrieval-augmented generation (RAG) to adjust prevalence estimates more effectively compared to traditional fine-tuning methods.
4.**Use of Real-World Data-Aware Synthetic Data:** We also aim to leverage continued pretraining or fine-tuning with real-world data-aware synthetic data to explicitly incorporate real-world prevalence statistics, aligning model predictions with actual disease distributions.

This work has highlighted a fundamental disconnect between real-world prevalence estimates and LLM outputs, which appear to track significantly closer to simple co-occurrences in pretraining data. In order to address these discrepancies, the most obvious solution is to curate pretraining data of language models with this knowledge in mind and for a specific context. Furthermore, organizations and regulators can evaluate simple co-occurrences to provide a rough idea of models’ tendencies before deployment and in addition to task performance. This is particularly important when considering multilingual models; if this is to be used across languages, then accurate data in these languages are important at both pretraining and alignment stages.**CONCLUSION** This study conducted a detailed analysis of how corpus co-occurrence and demographic representation influence biases in LLMs within the context of disease prevalence. We uncovered substantial variances in model outputs, highlighting the complexities of developing NLP systems for biomedical applications that align with real-world data and outcomes. Importantly, these variances appear across alignment strategies and languages, and notably, they do not correlate with the real-world prevalence of diseases. This suggests a lack of grounding in actual disease prevalences, underscoring a critical need for extensive research into integrating real-world data to ensure fair and accurate model translation. These findings highlight the urgent need for research to enhance these models, ensuring they are reliable and equitable across diverse populations. Further exploration will advance our understanding of and ability to correct biases in AI systems for healthcare.**REFERENCES**

1. [Guha, N.](http://paperpile.com/b/coPcdt/rn7n)[*et al.*](http://paperpile.com/b/coPcdt/rn7n)[LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models.](http://paperpile.com/b/coPcdt/rn7n)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/rn7n)[(2023).](http://paperpile.com/b/coPcdt/rn7n)

2. [Hendrycks, D.](http://paperpile.com/b/coPcdt/EbvY)[*et al.*](http://paperpile.com/b/coPcdt/EbvY)[Measuring mathematical problem solving with the MATH dataset.](http://paperpile.com/b/coPcdt/EbvY)[*arXiv [cs.LG]*](http://paperpile.com/b/coPcdt/EbvY)[(2021).](http://paperpile.com/b/coPcdt/EbvY)

3. [Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large Language Models are Zero-Shot Reasoners.](http://paperpile.com/b/coPcdt/o7lT)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/o7lT)[(2022).](http://paperpile.com/b/coPcdt/o7lT)

4. [Touvron, H.](http://paperpile.com/b/coPcdt/JxIE)[*et al.*](http://paperpile.com/b/coPcdt/JxIE)[LLaMA: Open and efficient foundation language models.](http://paperpile.com/b/coPcdt/JxIE)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/JxIE)[(2023).](http://paperpile.com/b/coPcdt/JxIE)

5. [Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large language models are few-shot clinical information extractors.](http://paperpile.com/b/coPcdt/h6P0)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/h6P0)[(2022).](http://paperpile.com/b/coPcdt/h6P0)

6. [Wang, A.](http://paperpile.com/b/coPcdt/AOzs)[*et al.*](http://paperpile.com/b/coPcdt/AOzs)[GLUE: A multi-task benchmark and analysis platform for natural language understanding. in](http://paperpile.com/b/coPcdt/AOzs)[*Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*](http://paperpile.com/b/coPcdt/AOzs)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2018). doi:](http://paperpile.com/b/coPcdt/AOzs)[10.18653/v1/w18-5446](http://dx.doi.org/10.18653/v1/w18-5446)[.](http://paperpile.com/b/coPcdt/AOzs)

7. [Wang, A.](http://paperpile.com/b/coPcdt/bUun)[*et al.*](http://paperpile.com/b/coPcdt/bUun)[SuperGLUE: A stickier benchmark for general-purpose language understanding systems.](http://paperpile.com/b/coPcdt/bUun)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/bUun)[(2019).](http://paperpile.com/b/coPcdt/bUun)

8. [Liu, J.](http://paperpile.com/b/coPcdt/V1vd)[*et al.*](http://paperpile.com/b/coPcdt/V1vd)[Benchmarking large language models on CMExam -- A comprehensive Chinese medical exam dataset.](http://paperpile.com/b/coPcdt/V1vd)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/V1vd)[(2023).](http://paperpile.com/b/coPcdt/V1vd)

9. [Singh, S.](http://paperpile.com/b/coPcdt/rh6z)[*et al.*](http://paperpile.com/b/coPcdt/rh6z)[Global MMLU: Understanding and addressing cultural and linguistic biases in multilingual evaluation.](http://paperpile.com/b/coPcdt/rh6z)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/rh6z)[(2024).](http://paperpile.com/b/coPcdt/rh6z)

10. [Wang, Y.](http://paperpile.com/b/coPcdt/xbIg)[*et al.*](http://paperpile.com/b/coPcdt/xbIg)[MMLU-Pro: A more robust and challenging multi-task language understanding benchmark.](http://paperpile.com/b/coPcdt/xbIg)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/xbIg)[(2024).](http://paperpile.com/b/coPcdt/xbIg)

11. [Ye, Q.](http://paperpile.com/b/coPcdt/zMVp)[*et al.*](http://paperpile.com/b/coPcdt/zMVp)[Qilin-Med: Multi-stage knowledge injection advanced medical large language model.](http://paperpile.com/b/coPcdt/zMVp)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/zMVp)[(2023).](http://paperpile.com/b/coPcdt/zMVp)

12. [Jin, D.](http://paperpile.com/b/coPcdt/v2iZ)[*et al.*](http://paperpile.com/b/coPcdt/v2iZ)[What disease does this patient have? A large-scale open domain question answering dataset from medical exams.](http://paperpile.com/b/coPcdt/v2iZ)[*Appl. Sci. (Basel)*](http://paperpile.com/b/coPcdt/v2iZ)[http://paperpile.com/b/coPcdt/v2iZ](http://paperpile.com/b/coPcdt/v2iZ)[**11**](http://paperpile.com/b/coPcdt/v2iZ)[, 6421 (2021).](http://paperpile.com/b/coPcdt/v2iZ)

13. [Mazeika, M.](http://paperpile.com/b/coPcdt/9Bi9)[*et al.*](http://paperpile.com/b/coPcdt/9Bi9)[HarmBench: A standardized evaluation framework for automated red teaming and robust refusal.](http://paperpile.com/b/coPcdt/9Bi9)[*arXiv [cs.LG]*](http://paperpile.com/b/coPcdt/9Bi9)[(2024).](http://paperpile.com/b/coPcdt/9Bi9)

14. [Li, N.](http://paperpile.com/b/coPcdt/CLhj)[*et al.*](http://paperpile.com/b/coPcdt/CLhj)[The WMDP benchmark: Measuring and reducing malicious use with unlearning.](http://paperpile.com/b/coPcdt/CLhj)[*arXiv [cs.LG]*](http://paperpile.com/b/coPcdt/CLhj)[(2024).](http://paperpile.com/b/coPcdt/CLhj)

15. [Wang, B.](http://paperpile.com/b/coPcdt/LMNd)[*et al.*](http://paperpile.com/b/coPcdt/LMNd)[DecodingTrust: A comprehensive assessment of trustworthiness in GPT models.](http://paperpile.com/b/coPcdt/LMNd)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/LMNd)[(2023).](http://paperpile.com/b/coPcdt/LMNd)

16. [Li, L.](http://paperpile.com/b/coPcdt/wt4h)[*et al.*](http://paperpile.com/b/coPcdt/wt4h)[SALAD-Bench: A hierarchical and comprehensive safety benchmark for Large Language Models.](http://paperpile.com/b/coPcdt/wt4h)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/wt4h)[(2024).](http://paperpile.com/b/coPcdt/wt4h)

17. [Liu, T.](http://paperpile.com/b/coPcdt/cNDA)[*et al.*](http://paperpile.com/b/coPcdt/cNDA)[Rumor Detection with a novel graph neural network approach.](http://paperpile.com/b/coPcdt/cNDA)[*arXiv [cs.AI]*](http://paperpile.com/b/coPcdt/cNDA)[(2024).](http://paperpile.com/b/coPcdt/cNDA)

18. [Goodman, K. E., Yi, P. H. & Morgan, D. J. AI-generated clinical summaries require more than accuracy.](http://paperpile.com/b/coPcdt/mWz4)[*JAMA*](http://paperpile.com/b/coPcdt/mWz4)[http://paperpile.com/b/coPcdt/mWz4](http://paperpile.com/b/coPcdt/mWz4)[**331**](http://paperpile.com/b/coPcdt/mWz4)[, 637–638 (2024).](http://paperpile.com/b/coPcdt/mWz4)

19. [Li, J., Cheng, X., Zhao, W. X., Nie, J.-Y. & Wen, J.-R. HaluEval: A large-scale Hallucination Evaluation benchmark for Large Language Models.](http://paperpile.com/b/coPcdt/lt4w)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/lt4w)[(2023).](http://paperpile.com/b/coPcdt/lt4w)

20. [Felkner, V., Chang, H.-C. H., Jang, E. & May, J. WinoQueer: A community-in-the-loop benchmark for anti-LGBTQ+ bias in large language models. in](http://paperpile.com/b/coPcdt/4oGx)[*Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*](http://paperpile.com/b/coPcdt/4oGx)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2023). doi:](http://paperpile.com/b/coPcdt/4oGx)[10.18653/v1/2023.acl-long.507](http://dx.doi.org/10.18653/v1/2023.acl-long.507)[.](http://paperpile.com/b/coPcdt/4oGx)

21. [Guevara, M.](http://paperpile.com/b/coPcdt/RImu)[*et al.*](http://paperpile.com/b/coPcdt/RImu)[Large language models to identify social determinants of health in electronic health records.](http://paperpile.com/b/coPcdt/RImu)[*NPJ Digit. Med.*](http://paperpile.com/b/coPcdt/RImu)[http://paperpile.com/b/coPcdt/RImu](http://paperpile.com/b/coPcdt/RImu)[**7**](http://paperpile.com/b/coPcdt/RImu)[, 6 (2024).](http://paperpile.com/b/coPcdt/RImu)

22. [Nangia, N., Vania, C., Bhalerao, R. & Bowman, S. R. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. in](http://paperpile.com/b/coPcdt/rIap)[*Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*](http://paperpile.com/b/coPcdt/rIap)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2020). doi:](http://paperpile.com/b/coPcdt/rIap)[10.18653/v1/2020.emnlp-main.154](http://dx.doi.org/10.18653/v1/2020.emnlp-main.154)[.](http://paperpile.com/b/coPcdt/rIap)

23. [Parrish, A.](http://paperpile.com/b/coPcdt/jsIO)[*et al.*](http://paperpile.com/b/coPcdt/jsIO)[BBQ: A hand-built bias benchmark for question answering. in](http://paperpile.com/b/coPcdt/jsIO)[*Findings of the Association for Computational Linguistics: ACL 2022*](http://paperpile.com/b/coPcdt/jsIO)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2022). doi:](http://paperpile.com/b/coPcdt/jsIO)[10.18653/v1/2022.findings-acl.165](http://dx.doi.org/10.18653/v1/2022.findings-acl.165)[.](http://paperpile.com/b/coPcdt/jsIO)

24. [Chen, S.](http://paperpile.com/b/coPcdt/uPo4)[*et al.*](http://paperpile.com/b/coPcdt/uPo4)[The effect of using a large language model to respond to patient messages.](http://paperpile.com/b/coPcdt/uPo4)[*Lancet Digit. Health*](http://paperpile.com/b/coPcdt/uPo4)[http://paperpile.com/b/coPcdt/uPo4](http://paperpile.com/b/coPcdt/uPo4)[**6**](http://paperpile.com/b/coPcdt/uPo4)[, e379–e381 (2024).](http://paperpile.com/b/coPcdt/uPo4)

25. [Zack, T.](http://paperpile.com/b/coPcdt/SrkJ)[*et al.*](http://paperpile.com/b/coPcdt/SrkJ)[Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study.](http://paperpile.com/b/coPcdt/SrkJ)[*Lancet Digit. Health*](http://paperpile.com/b/coPcdt/SrkJ)[http://paperpile.com/b/coPcdt/SrkJ](http://paperpile.com/b/coPcdt/SrkJ)[**6**](http://paperpile.com/b/coPcdt/SrkJ)[, e12–e22 (2024).](http://paperpile.com/b/coPcdt/SrkJ)

26. [Pfohl, S. R.](http://paperpile.com/b/coPcdt/94S0)[*et al.*](http://paperpile.com/b/coPcdt/94S0)[A toolbox for surfacing health equity harms and biases in large language models.](http://paperpile.com/b/coPcdt/94S0)[*arXiv [cs.CY]*](http://paperpile.com/b/coPcdt/94S0)[(2024).](http://paperpile.com/b/coPcdt/94S0)

27. [Wei, J.](http://paperpile.com/b/coPcdt/nFS4)[*et al.*](http://paperpile.com/b/coPcdt/nFS4)[Larger language models do in-context learning differently.](http://paperpile.com/b/coPcdt/nFS4)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/nFS4)[(2023).](http://paperpile.com/b/coPcdt/nFS4)

28. [Magnusson, I.](http://paperpile.com/b/coPcdt/UVGC)[*et al.*](http://paperpile.com/b/coPcdt/UVGC)[Paloma: A benchmark for evaluating language model fit.](http://paperpile.com/b/coPcdt/UVGC)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/UVGC)[(2023).](http://paperpile.com/b/coPcdt/UVGC)

29. [Webster, K.](http://paperpile.com/b/coPcdt/K3wJ)[*et al.*](http://paperpile.com/b/coPcdt/K3wJ)[Measuring and reducing gendered correlations in pre-trained models.](http://paperpile.com/b/coPcdt/K3wJ)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/K3wJ)[(2020).](http://paperpile.com/b/coPcdt/K3wJ)

30. [Kaneko, M. & Bollegala, D. Unmasking the mask -- evaluating social biases in masked Language Models.](http://paperpile.com/b/coPcdt/kzHl)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/kzHl)[(2021).](http://paperpile.com/b/coPcdt/kzHl)

31. [Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, R. Large language models propagate race-based medicine.](http://paperpile.com/b/coPcdt/Eekj)[*NPJ Digit. Med.*](http://paperpile.com/b/coPcdt/Eekj)[http://paperpile.com/b/coPcdt/Eekj](http://paperpile.com/b/coPcdt/Eekj)[**6**](http://paperpile.com/b/coPcdt/Eekj)[, 195 (2023).](http://paperpile.com/b/coPcdt/Eekj)

32. [Li, Y.](http://paperpile.com/b/coPcdt/5nHk)[*et al.*](http://paperpile.com/b/coPcdt/5nHk)[Textbooks Are All You Need II: phi-1.5 technical report.](http://paperpile.com/b/coPcdt/5nHk)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/5nHk)[(2023).](http://paperpile.com/b/coPcdt/5nHk)

33. [Biderman, S.](http://paperpile.com/b/coPcdt/vl8a)[*et al.*](http://paperpile.com/b/coPcdt/vl8a)[Pythia: A suite for analyzing large language models across training and scaling.](http://paperpile.com/b/coPcdt/vl8a)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/vl8a)[(2023).](http://paperpile.com/b/coPcdt/vl8a)

34. [Feng, S., Park, C. Y., Liu, Y. & Tsvetkov, Y. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. in](http://paperpile.com/b/coPcdt/UzXQ)[*Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*](http://paperpile.com/b/coPcdt/UzXQ)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2023). doi:](http://paperpile.com/b/coPcdt/UzXQ)[10.18653/v1/2023.acl-long.656](http://dx.doi.org/10.18653/v1/2023.acl-long.656)[.](http://paperpile.com/b/coPcdt/UzXQ)

35. [Lum, K., Anthis, J. R., Robinson, K., Nagpal, C. & D’Amour, A. N. Bias in language models: Beyond trick tests and towards RUTEd evaluation. in](http://paperpile.com/b/coPcdt/yteq)[*Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*](http://paperpile.com/b/coPcdt/yteq)[137–161 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2025).](http://paperpile.com/b/coPcdt/yteq)

36. [Goldfarb-Tarrant, S., Marchant, R., Muñoz Sánchez, R., Pandya, M. & Lopez, A. Intrinsic bias metrics do not correlate with application bias. in](http://paperpile.com/b/coPcdt/pTvN)[*Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*](http://paperpile.com/b/coPcdt/pTvN)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2021). doi:](http://paperpile.com/b/coPcdt/pTvN)[10.18653/v1/2021.acl-long.150](http://dx.doi.org/10.18653/v1/2021.acl-long.150)[.](http://paperpile.com/b/coPcdt/pTvN)

37. [Mikolov, T., Chen, K., Corrado, G. & Dean, J. Efficient estimation of word representations in vector space.](http://paperpile.com/b/coPcdt/z3Ih)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/z3Ih)[(2013).](http://paperpile.com/b/coPcdt/z3Ih)

38. [*fastText: Library for Fast Text Representation and Classification*](http://paperpile.com/b/coPcdt/MMre)[. (Github).](http://paperpile.com/b/coPcdt/MMre)

39. [Guo, W. & Caliskan, A. Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases. in](http://paperpile.com/b/coPcdt/xyIK)[*Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society*](http://paperpile.com/b/coPcdt/xyIK)[(ACM, New York, NY, USA, 2021). doi:](http://paperpile.com/b/coPcdt/xyIK)[10.1145/3461702.3462536](http://dx.doi.org/10.1145/3461702.3462536)[.](http://paperpile.com/b/coPcdt/xyIK)

40. [Nadeem, M., Bethke, A. & Reddy, S. StereoSet: Measuring stereotypical bias in pretrained language models. in](http://paperpile.com/b/coPcdt/QfiW)[*Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*](http://paperpile.com/b/coPcdt/QfiW)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2021). doi:](http://paperpile.com/b/coPcdt/QfiW)[10.18653/v1/2021.acl-long.416](http://dx.doi.org/10.18653/v1/2021.acl-long.416)[.](http://paperpile.com/b/coPcdt/QfiW)

41. [Kurita, K., Vyas, N., Pareek, A., Black, A. W. & Tsvetkov, Y. Measuring bias in contextualized word representations.](http://paperpile.com/b/coPcdt/kPjs)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/kPjs)[(2019).](http://paperpile.com/b/coPcdt/kPjs)

42. [Srivastava, A.](http://paperpile.com/b/coPcdt/DAS6)[*et al.*](http://paperpile.com/b/coPcdt/DAS6)[Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.](http://paperpile.com/b/coPcdt/DAS6)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/DAS6)[(2022).](http://paperpile.com/b/coPcdt/DAS6)

43. [Nicholas, G. & Bhatia, A. Lost in translation: Large language models in non-English content analysis.](http://paperpile.com/b/coPcdt/WOTi)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/WOTi)[(2023).](http://paperpile.com/b/coPcdt/WOTi)

44. [Qi, J., Fernández, R. & Bisazza, A. Cross-lingual consistency of factual knowledge in multilingual language models.](http://paperpile.com/b/coPcdt/Q8T9)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/Q8T9)[(2023).](http://paperpile.com/b/coPcdt/Q8T9)

45. [Ryan, M. J., Held, W. & Yang, D. Unintended impacts of LLM alignment on global representation.](http://paperpile.com/b/coPcdt/YEEb)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/YEEb)[(2024).](http://paperpile.com/b/coPcdt/YEEb)

46. [De-Arteaga, M.](http://paperpile.com/b/coPcdt/uvck)[*et al.*](http://paperpile.com/b/coPcdt/uvck)[Bias in bios: A case study of semantic representation bias in a high-stakes setting.](http://paperpile.com/b/coPcdt/uvck)[*arXiv [cs.IR]*](http://paperpile.com/b/coPcdt/uvck)[(2019).](http://paperpile.com/b/coPcdt/uvck)

47. [Zhao, J., Wang, T., Yatskar, M., Ordonez, V. & Chang, K.-W. Gender bias in coreference resolution: Evaluation and debiasing methods. in](http://paperpile.com/b/coPcdt/rAJt)[*Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)*](http://paperpile.com/b/coPcdt/rAJt)[(Association for Computational Linguistics, Stroudsburg, PA, USA, 2018). doi:](http://paperpile.com/b/coPcdt/rAJt)[10.18653/v1/n18-2003](http://dx.doi.org/10.18653/v1/n18-2003)[.](http://paperpile.com/b/coPcdt/rAJt)

48. [Bai, X., Wang, A., Sucholutsky, I. & Griffiths, T. L. Measuring Implicit Bias in explicitly unbiased large language models.](http://paperpile.com/b/coPcdt/M3HN)[*arXiv [cs.CY]*](http://paperpile.com/b/coPcdt/M3HN)[(2024).](http://paperpile.com/b/coPcdt/M3HN)

49. [Naous, T., Ryan, M. J., Ritter, A. & Xu, W. Having beer after prayer? Measuring cultural bias in large language models. in](http://paperpile.com/b/coPcdt/rhc0)[*Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*](http://paperpile.com/b/coPcdt/rhc0)[16366–16393 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2024).](http://paperpile.com/b/coPcdt/rhc0)

50. [Hartmann, J., Schwenzow, J. & Witte, M. The political ideology of conversational AI: Converging evidence on ChatGPT’s pro-environmental, left-libertarian orientation.](http://paperpile.com/b/coPcdt/CuaR)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/CuaR)[(2023).](http://paperpile.com/b/coPcdt/CuaR)

51. [Yu, H.](http://paperpile.com/b/coPcdt/ZGgZ)[*et al.*](http://paperpile.com/b/coPcdt/ZGgZ)[Large Language Models in biomedical and Health Informatics: A review with bibliometric analysis.](http://paperpile.com/b/coPcdt/ZGgZ)[*arXiv [cs.DL]*](http://paperpile.com/b/coPcdt/ZGgZ)[(2024).](http://paperpile.com/b/coPcdt/ZGgZ)

52. [Fan, L.](http://paperpile.com/b/coPcdt/2r7Z)[*et al.*](http://paperpile.com/b/coPcdt/2r7Z)[A bibliometric review of large language models research from 2017 to 2023.](http://paperpile.com/b/coPcdt/2r7Z)[*arXiv [cs.DL]*](http://paperpile.com/b/coPcdt/2r7Z)[(2023).](http://paperpile.com/b/coPcdt/2r7Z)

53. [Zhou, H.](http://paperpile.com/b/coPcdt/QQaD)[*et al.*](http://paperpile.com/b/coPcdt/QQaD)[A survey of large language models in medicine: Progress, application, and challenge.](http://paperpile.com/b/coPcdt/QQaD)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/QQaD)[(2023).](http://paperpile.com/b/coPcdt/QQaD)

54. [Gao, L.](http://paperpile.com/b/coPcdt/Dq83)[*et al.*](http://paperpile.com/b/coPcdt/Dq83)[The Pile: An 800GB dataset of diverse text for language modeling.](http://paperpile.com/b/coPcdt/Dq83)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/Dq83)[(2020).](http://paperpile.com/b/coPcdt/Dq83)

55. [Gu, A. & Dao, T. Mamba: Linear-time sequence modeling with selective state spaces.](http://paperpile.com/b/coPcdt/cux2)[*arXiv [cs.LG]*](http://paperpile.com/b/coPcdt/cux2)[(2023).](http://paperpile.com/b/coPcdt/cux2)

56. [Hansen, L. H.](http://paperpile.com/b/coPcdt/na2B)[*et al.*](http://paperpile.com/b/coPcdt/na2B)[Seeds of stereotypes: A large-scale textual analysis of race and gender associations with diseases in online sources.](http://paperpile.com/b/coPcdt/na2B)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/na2B)[(2024).](http://paperpile.com/b/coPcdt/na2B)

57. [*I’m Sorry to Hear That": Finding New Biases in Language Models with a Holistic Descriptor Dataset*](http://paperpile.com/b/coPcdt/co6w)[.](http://paperpile.com/b/coPcdt/co6w)

58. [Liu, J., Min, S., Zettlemoyer, L., Choi, Y. & Hajishirzi, H. Infini-gram: Scaling unbounded n-gram language models to a trillion tokens.](http://paperpile.com/b/coPcdt/x6hn)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/x6hn)[(2024).](http://paperpile.com/b/coPcdt/x6hn)

59. [Ouyang, L.](http://paperpile.com/b/coPcdt/3lDg)[*et al.*](http://paperpile.com/b/coPcdt/3lDg)[Training language models to follow instructions with human feedback.](http://paperpile.com/b/coPcdt/3lDg)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/3lDg)[(2022).](http://paperpile.com/b/coPcdt/3lDg)

60. [Dubois, Y.](http://paperpile.com/b/coPcdt/B012)[*et al.*](http://paperpile.com/b/coPcdt/B012)[AlpacaFarm: A simulation framework for methods that learn from human feedback.](http://paperpile.com/b/coPcdt/B012)[*arXiv [cs.LG]*](http://paperpile.com/b/coPcdt/B012)[(2023).](http://paperpile.com/b/coPcdt/B012)

61. [Rafailov, R.](http://paperpile.com/b/coPcdt/ts44)[*et al.*](http://paperpile.com/b/coPcdt/ts44)[Direct Preference Optimization: Your language model is secretly a reward model.](http://paperpile.com/b/coPcdt/ts44)[*arXiv [cs.LG]*](http://paperpile.com/b/coPcdt/ts44)[(2023).](http://paperpile.com/b/coPcdt/ts44)

62. [Hofmann, V., Kalluri, P. R., Jurafsky, D. & King, S. Dialect prejudice predicts AI decisions about people’s character, employability, and criminality.](http://paperpile.com/b/coPcdt/AiWF)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/AiWF)[(2024).](http://paperpile.com/b/coPcdt/AiWF)

63. [Zhao, J.](http://paperpile.com/b/coPcdt/2Zw4)[*et al.*](http://paperpile.com/b/coPcdt/2Zw4)[Unveiling A core linguistic region in large language models.](http://paperpile.com/b/coPcdt/2Zw4)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/2Zw4)[(2023).](http://paperpile.com/b/coPcdt/2Zw4)
