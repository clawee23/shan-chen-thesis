# A1: MedAlpaca - An Open-Source Collection of Medical Conversation AI Models and Training Data

*Available here: *[<u>https://github.com/kbressem/medAlpaca</u>](https://github.com/kbressem/medAlpaca)

**Background**Large language models (LLMs) promise gains in clinical documentation, education, and patient communication, but closed APIs and privacy constraints limit deployment. MedAlpaca addresses this gap by releasing open-source medical instruction-tuned models and datasets that can run on-prem, enabling reproducible research and privacy-preserving use.

**Approach**We compiled ‚ÄúMedical Meadow,‚Äù a >160k-example corpus that reformats medical resources (e.g., flashcards, StackExchange Q&A, WikiDoc chapters, public Medical QA benchmark) into instruction‚Äìresponse pairs. Models are fine-tuned from LLaMA 7B/13B with full-parameter FT and parameter-efficient variants (LoRA; 8-bit optimizer/matmul) using large effective batch sizes and standard schedulers. Zero-shot evaluations target USMLE Steps 1‚Äì3, excluding image items, with strict answer formatting for fair scoring.

**Results**Fine-tuning consistently improves over base models on USMLE. MedAlpaca-13B attains the strongest accuracies (e.g., Step 1 ‚âà0.47, Step 2 ‚âà0.48, Step 3 ‚âà0.60). Parameter-efficient routes (LoRA/8-bit) trade some accuracy for much lower compute, supporting wider academic/clinical use. All models and data are released publicly to catalyze follow-on work.

**Implications**Open instruction-tuned medical LLMs make it feasible to experiment locally, reduce privacy concerns, and accelerate tooling for structured reporting, retrieval/summarization, and training aids‚Äîwhile highlighting the need for guardrails against confabulation.

# A2: Measuring Pointwise ùí±-Usable Information In-Context-ly

*Available here: *[<u>https://aclanthology.org/2023.findings-emnlp.1054/</u>](https://aclanthology.org/2023.findings-emnlp.1054/)

**Background** ‚ÄúPointwise ùí±-usable information‚Äù (PVI) is a hardness measure that tells you, for a fixed model ùí±, how much *usable* signal an individual example contains‚Äîlow PVI means the model has little leverage to solve that instance; high PVI means the signal is there and the model can exploit it. Prior work estimated PVI by training or fine-tuning probes, which is accurate but heavy-weight. This paper asks: can we bring PVI into the in-context learning (ICL) regime‚Äîi.e., diagnose instance-level difficulty purely by prompting, without any gradient updates? That would give practitioners a lightweight, model-specific difficulty score they can compute on the fly when they only have API access.

**Approach**We adapt PVI to an in-context analogue (‚Äúin-context PVI‚Äù) by observing model predictions across *prompted* conditions rather than trained parameters. Concretely, we vary exemplars and shot counts in the prompt, read out the induced distribution of predictions, and compute a PVI-style quantity that reflects how much the model‚Äôs *prompt-conditioned behavior* uses the information present in the input. The key design goal is no fine-tuning and few exemplars, so the metric is cheap to compute and usable in settings where training is impossible.

We run a systematic reliability study: (i) Exemplar robustness‚Äîdoes the in-context PVI for a given instance change if you swap which few-shot examples you show? (ii) Shot robustness‚Äîdoes it change as you move from 1‚Üík shots? (iii) Behavioral parity‚Äîdoes in-context PVI preserve the characteristic relationships known from the original (training-based) PVI? Finally, they demonstrate practical utility: use in-context PVI to surface the ‚Äúhard tail‚Äù of a dataset for targeted evaluation or data curation. Across these lenses, the in-context measure behaves stably and mirrors the spirit of classical PVI, but at a fraction of the computational cost.

**Results** First, you don‚Äôt need fine-tuning to get a useful PVI-like hardness signal: the prompted version tracks difficulty in ways that agree with the training-based formulation. Second, the number and identity of exemplars barely move the needle‚Äîvariance from exemplar choice is *insignificant*‚Äîand scaling the #shots does not destabilize the estimates. Third, it‚Äôs actionable: ranking by in-context PVI cleanly pulls out the cases your model will struggle with, which then lets you allocate annotation or evaluation budget where it matters most. In short, you get a trustworthy, ‚ÄúAPI-only‚Äù difficulty score that‚Äôs cheap and robust.

**Implications** If you rely on ICL (or only have black-box API access), in-context PVI gives you a principled way to diagnose instance-level blind spots without training new heads or models. It‚Äôs useful for: (a) test-set triage‚Äîstress-testing on low-PVI (hard) examples; (b) data curation‚Äîprioritizing new labels where PVI is low; and (c) human-AI collaboration‚Äîrouting borderline cases to humans when the model‚Äôs usable information is small. It extends the PVI toolbox from the fine-tuning world into the prompt-engineering world, while staying faithful to the original theoretical framing of ùí±-usable information.

# A3: Sparse Autoencoder Features for Classifications and Transferability

*Available here: *[<u>https://aclanthology.org/2025.emnlp-main.1521/</u>](https://aclanthology.org/2025.emnlp-main.1521/)

**Background** Hidden states in large language models are rich but notoriously entangled, which makes downstream use brittle and hard to audit. We turn to sparse autoencoders (SAEs) as a way to recover part-based, interpretable features from those states. Our question is pragmatic: if we replace raw hidden states with SAE activations, do we keep enough task signal to be competitive, and do those features transfer‚Äîacross models and across languages‚Äîso they‚Äôre actually useful beyond the lab settings?**Approach** We extract activations from released Gemma-scope SAEs (and keep the pipeline compatible with Neuropedia SAEs) and train simple linear probes for text classification. To stress-test usefulness, we run head-to-head comparisons of SAE features versus same-size slices of raw hidden states on 10+ challenging datasets. We then study two forms of transferability. (i) Model transfer: train on features from base or instruction-tuned LMs and apply them to LLaVA for classification without retraining the representation. (ii) Cross-lingual transfer: train a toxicity classifier on SAE features in one language and evaluate on other languages to see if the features carry over without translation. Throughout, we keep the recipe intentionally lightweight‚Äîno elaborate heads, minimal hyperparameter fuss‚Äîso any improvement is creditable to the representation rather than extra machinery.

![](../figures/image32.png)**Results** Across difficult text classification tasks, SAE features consistently match or outpace baselines that use equivalently sized raw hidden-state slices, while remaining sparse and interpretable. For model transfer, features learned from base or instruction-tuned LMs plug into LLaVA and retain strong classification performance with no feature retraining. For cross-lingual toxicity, a probe trained on SAE features in a single language generalizes to other languages out of the box; in aggregate, it is competitive with ‚Äútranslate-to-English-then-classify‚Äù pipelines shown in our figures. Finally, because our pipeline treats any public SAE as a drop-in module, Neuropedia releases can be used immediately‚Äîno custom retraining required.
