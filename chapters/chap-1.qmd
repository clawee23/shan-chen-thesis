# Chapter 1

***General Introduction, Outline and Glossary***

Recent advances in artificial intelligence are reshaping healthcare, driven by the rapid emergence of large language models (LLMs) that exhibit unprecedented fluency, reasoning, and adaptability. Early generative models such as GPT-2 and GPT-3 demonstrated that scale and unsupervised pre-training could unlock broad generalization capabilities previously unattainable in clinical NLP systems[1,2](https://paperpile.com/c/coPcdt/Lqak+5MlW). These breakthroughs were enabled by decoder-only transformer architectures[3](https://paperpile.com/c/coPcdt/yTOb) and further refined through instruction-following and reinforcement learning from human feedback[4](https://paperpile.com/c/coPcdt/3lDg), which significantly improved model controllability and task alignment. More recently, frontier systems such as GPT-4 continue to expand the scope of clinical tasks LLMs can perform, from summarization to reasoning over complex medical content[5](https://paperpile.com/c/coPcdt/UlT6).

Simultaneously, medicine itself has become increasingly digitized. Electronic health records, clinical trial documents, biomedical literature, and patient-generated data now form vast corpora on which these models are trained and in which they are deployed. As a result, LLMs have transitioned rapidly from research prototypes to operational tools embedded in real clinical workflows. They are already assisting with patient-provider communication[6](https://paperpile.com/c/coPcdt/uPo4), generating and summarizing clinical notes[7–9](https://paperpile.com/c/coPcdt/BsvC+knyy+rVYo), extracting structured information from unstructured data[10](https://paperpile.com/c/coPcdt/83yK), and even encoding substantial clinical knowledge comparable to domain-trained models[11,12](https://paperpile.com/c/coPcdt/DkR2+7xsh). This shift marks a profound change in the epistemic infrastructure of medicine: for the first time, clinicians routinely interact with probabilistic generative systems capable of producing medical text at scale.

However, the same generative fluency that makes LLMs attractive for healthcare applications also introduces substantial risks. Studies consistently show that LLMs can hallucinate nonexistent treatments, misjudge clinical acuity, or propagate biases embedded in their training data[13–19](https://paperpile.com/c/coPcdt/SrkJ+X0L4+IXst+d6qh+RJ2T+k02J+rgaJ). Even small perturbations in phrasing, demographic attributes, or contextual cues can produce significant shifts in output quality - raising concerns about stability, robustness, and equity. These vulnerabilities are particularly concerning given early evidence that patients may overly trust model-generated text and that health systems are beginning to integrate LLMs into triage, decision support, and documentation pipelines. The risk is not hypothetical: inappropriate advice, misaligned summaries, and sycophantic agreement with false statements have already been documented in controlled evaluations[19,20](https://paperpile.com/c/coPcdt/rgaJ+kiiZ).

These developments make clear that the healthcare community faces a dual imperative:**to rigorously identify where LLMs provide measurable value, and to expose and mitigate the conditions under which they may cause harm**. Overemphasizing capability risks premature deployment of unsafe tools; overemphasizing failure risks forfeiting technologies with genuine potential to reduce clinician burden, surface invisible inequities, and enhance evidence generation. This thesis embraces both dimensions.**Part I** investigates domains where LLMs can augment clinical expertise. Our work has shown that models can encode substantial clinical knowledge, assist in patient communication, and extract valuable signals from narrative text that traditional analytics overlook - such as social determinants of health or treatment-related toxicities. The studies presented here also extend through rigorous, workflow-embedded evaluations, demonstrating that when aligned, fine-tuned, and rigorously validated, LLMs can surface clinically meaningful information at scale and support more equitable clinical operations.

![](../figures/image4.png)**Part II** interrogates the foundational limitations that emerge when LLMs confront clinically related tasks. Building on recent evidence of bias, misalignment, and brittleness, these chapters reveal how pre-training distributions, lexical fragility, and conversational alignment pressures can systematically distort medical predictions. They also show that even state-of-the-art models remain vulnerable to hallucination and sycophantic compliance, posing risks for clinical deployment.

Together, these investigations argue that LLMs should not be deployed as black-box oracles but rather as**transparent, auditable, and domain-aligned clinical instruments**. Achieving this vision requires interdisciplinary collaboration, last-mile alignment grounded in local data and workflows, and rigorous, open evaluation frameworks such as HealthBench[21](https://paperpile.com/c/coPcdt/ESmj) and model-specific benchmarks for medical tasks[7,22,23](https://paperpile.com/c/coPcdt/BsvC+9dPU+Scnx). By synthesizing both the promise and the pitfalls of clinical LLMs, this thesis outlines a principled path toward AI systems that strengthen the safety, equity, and integrity of global healthcare.**Part I: Potential Utilities of Language Models in Real Clinical Practice**

The first part of this thesis investigates how LLMs can be harnessed to solve practical, high-impact problems across the clinical ecosystem: from direct patient communication to surfacing hidden determinants of health.

We begin at the patient interface.**Chapter 2** examines the urgent problem of medical misinformation: cancer patients increasingly turn to general-purpose chatbots like ChatGPT for treatment advice, yet these systems lack clinical grounding. We systematically evaluated ChatGPT’s ability to generate guideline-concordant recommendations across 104 oncologic scenarios, finding that while the model always produced at least one correct suggestion, over one-third of responses included potentially harmful errors and 12% hallucinated entire treatment modalities. This study establishes a baseline risk profile for patient-facing LLMs and underscores why unfiltered deployment in consumer health contexts is dangerous without rigorous validation and safeguards.

Moving to the provider workflow,**Chapter 3** investigates LLMs as clinical scribes for patient portal messages—a use case now live in major EHR platforms. By having oncologists use GPT-4 to draft responses to realistic patient messages, we uncovered a tension between efficiency and safety: while AI assistance shortened documentation time, unedited drafts posed a 7.7% risk of severe harm, primarily from misjudging clinical urgency. This work reveals that human oversight remains non-negotiable; the value of LLMs lies not in autonomous drafting but in augmenting physicians, provided they remain vigilant to subtle but critical errors.

Shifting from communication to data extraction,**Chapter 4** and**Chapter 5** demonstrate how LLMs can unlock information trapped in unstructured clinical notes. Radiotherapy-induced esophagitis is a debilitating toxicity that is routinely documented in free text but rarely captured for research. We developed NLP models that automatically grade esophagitis severity with high fidelity, enabling large-scale real-world evidence generation on treatment complications. Similarly, social determinants of health (SDoH)—powerful predictors of outcomes—are notoriously absent from structured data. In**Chapter 5**, we show that fine-tuned LLMs can extract SDoH from clinical narratives far more accurately than billing codes, identifying nearly 94% of patients with adverse social circumstances versus 2% via traditional methods. Critically, these models exhibit less demographic bias than generalist LLMs, offering a path toward more equitable data-driven care.

These extraction capabilities culminate in**Chapter 6**, which introduces iRAE Agent(AEGIS), an agentic system piloted live at Mass General Brigham for immunotherapy-related adverse event detection in oncology clinical trials. Unlike passive classification, iRAE orchestrates multi-step reasoning, error checking, and ontology-guided reporting within live hospital workflows—addressing a labor-intensive process that directly impacts trial safety and cost. This pilot represents the frontier of clinical LLM deployment: moving from isolated tasks to integrated, autonomous agents that solve real, resource-constrained problems in cancer care.**Part II: Potential Fails of Language Models in Medical Settings**

If Part I demonstrates what LLMs can do, Part II investigates what can go wrong—exposing latent vulnerabilities that become dangerous when models are deployed in high-stakes medical environments.**Chapter 7** establishes a performance ceiling by comparing generalist LLMs against traditional biomedical NLP methods. On structured tasks like health advice classification and causal reasoning, few-shot prompting of ChatGPT approached but never exceeded the accuracy of fine-tuned, domain-specific models like BioBERT - while requiring a thousandfold more computational time. This finding is pivotal: it demonstrates that for many clinical NLP tasks, the “generalist advantage” is a myth. Locally tuned models are not only more accurate but also cheaper, faster to deploy, and easier to integrate into existing workflows, making them the pragmatic choice for most biomedical applications.

The next three chapters peel back layers of hidden fragility rooted in pre-training bias.**Chapter 8** introduces Cross‑Care, a benchmark quantifying discordance between LLM-encoded disease-demographic associations and true epidemiology. Across 89 diseases and multiple demographic and language groups, we found that LLM predictions correlate strongly with how often disease-demographic pairs appear in pre-training corpora - but show virtually no alignment with actual prevalence. This representational bias persists even after alignment tuning and across languages.**Chapter 9** reveals how this bias manifests as lexical fragility: swapping brand and generic drug names (semantically equivalent variations) caused performance drops of 1–13% across state-of-the-art models. This suggests that even when models “know” A=B, they cannot reliably manipulate this knowledge—they rely on memorized patterns from training data rather than robust reasoning.**Chapter 10** extends this to sycophantic behavior, where models comply with factually incorrect drug equivalence claims up to 100% of the time to appear helpful. Together, these chapters expose a systemic vulnerability: LLMs encode the statistical idiosyncrasies of their training data, not grounded medical knowledge, and they default to helpfulness over truth when these conflict.**Chapter 11** and**Chapter 12** scale evaluation efficiently to track real progress on emerging capabilities using existing resources.**Chapter 11** presents WorldMedQA‑V, a multilingual, multimodal medical exam dataset spanning four countries and languages. Rather than simply exposing disparities, this benchmark provides a scalable tool to rigorously track whether vision-language model improvements truly generalize across diverse linguistic contexts—or merely deepen global health inequities.**Chapter 12** introduces MedBrowseComp, a benchmark for medical deep research requiring multi-hop evidence synthesis from live knowledge bases. By forcing agents to navigate fragmented clinical evidence, MedBrowseComp offers a durable framework to measure genuine progress toward safe, evidence-based AI - exposing that even state-of-the-art systems achieve only ~10% accuracy on complex questions, far below clinical requirements.**Chapter 13** synthesizes these threads into a cohesive agenda: advancing clinical LLMs requires parallel investment in utility and safety. Our findings reveal that each success in Part I: whether extracting toxicities, surfacing SDoH, or enabling trial workflows—also surfaces deeper, domain-general problems: the brittleness of knowledge manipulation, the persistence of pre-training bias, the misalignment of helpfulness and truthfulness, and the inadequacy of current evaluation paradigms. We outline priorities including prevalence-aware pre-training, robust evaluation frameworks that test for bias and fragility, alignment strategies that prioritize factual correctness over helpfulness, and agent architectures grounded in verifiable evidence. The appendices document broader NLP contributions such as open-source medical LLMs, interpretability methods, and multilingual reasoning that strengthen the foundations of this field.

Together, this thesis provides an empirically grounded, dual-narrative examination of LLMs in medicine: celebrating their potential to transform cancer care while unflinchingly exposing the vulnerabilities that must be resolved before they can be trusted at scale.**REFERENCE**

1. [Radford, A.](http://paperpile.com/b/coPcdt/Lqak)[*et al.*](http://paperpile.com/b/coPcdt/Lqak)[Language Models are Unsupervised Multitask Learners. (2019).](http://paperpile.com/b/coPcdt/Lqak)

2. [Brown, T. B.](http://paperpile.com/b/coPcdt/5MlW)[*et al.*](http://paperpile.com/b/coPcdt/5MlW)[Language Models are Few-Shot Learners.](http://paperpile.com/b/coPcdt/5MlW)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/5MlW)[(2020).](http://paperpile.com/b/coPcdt/5MlW)

3. [Vaswani, A.](http://paperpile.com/b/coPcdt/yTOb)[*et al.*](http://paperpile.com/b/coPcdt/yTOb)[Attention is all you need. (2025) doi:](http://paperpile.com/b/coPcdt/yTOb)[10.65215/mdcm8z23](http://dx.doi.org/10.65215/mdcm8z23)[.](http://paperpile.com/b/coPcdt/yTOb)

4. [Ouyang, L.](http://paperpile.com/b/coPcdt/3lDg)[*et al.*](http://paperpile.com/b/coPcdt/3lDg)[Training language models to follow instructions with human feedback.](http://paperpile.com/b/coPcdt/3lDg)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/3lDg)[(2022).](http://paperpile.com/b/coPcdt/3lDg)

5. [OpenAI](http://paperpile.com/b/coPcdt/UlT6)[*et al.*](http://paperpile.com/b/coPcdt/UlT6)[GPT-4 Technical Report.](http://paperpile.com/b/coPcdt/UlT6)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/UlT6)[(2023).](http://paperpile.com/b/coPcdt/UlT6)

6. [Chen, S.](http://paperpile.com/b/coPcdt/uPo4)[*et al.*](http://paperpile.com/b/coPcdt/uPo4)[The effect of using a large language model to respond to patient messages.](http://paperpile.com/b/coPcdt/uPo4)[*Lancet Digit. Health*](http://paperpile.com/b/coPcdt/uPo4)[6, e379–e381 (2024).](http://paperpile.com/b/coPcdt/uPo4)

7. [Gao, Y.](http://paperpile.com/b/coPcdt/BsvC)[*et al.*](http://paperpile.com/b/coPcdt/BsvC)[DR.BENCH: Diagnostic reasoning benchmark for clinical natural language processing.](http://paperpile.com/b/coPcdt/BsvC)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/BsvC)[(2022).](http://paperpile.com/b/coPcdt/BsvC)

8. [Bednarczyk, L.](http://paperpile.com/b/coPcdt/knyy)[*et al.*](http://paperpile.com/b/coPcdt/knyy)[Scientific evidence for clinical text summarization using large language models: Scoping review.](http://paperpile.com/b/coPcdt/knyy)[*J. Med. Internet Res.*](http://paperpile.com/b/coPcdt/knyy)[27, e68998 (2025).](http://paperpile.com/b/coPcdt/knyy)

9. [Oliveira, J. D.](http://paperpile.com/b/coPcdt/rVYo)[*et al.*](http://paperpile.com/b/coPcdt/rVYo)[Development and evaluation of a clinical note summarization system using large language models.](http://paperpile.com/b/coPcdt/rVYo)[*Commun. Med. (Lond.)*](http://paperpile.com/b/coPcdt/rVYo)[5, 376 (2025).](http://paperpile.com/b/coPcdt/rVYo)

10. [Goodman, K. E.](http://paperpile.com/b/coPcdt/83yK)[*et al.*](http://paperpile.com/b/coPcdt/83yK)[Identification of long-term care facility residence from admission notes using large language models.](http://paperpile.com/b/coPcdt/83yK)[*JAMA Netw. Open*](http://paperpile.com/b/coPcdt/83yK)[8, e2512032 (2025).](http://paperpile.com/b/coPcdt/83yK)

11. [Singhal, K.](http://paperpile.com/b/coPcdt/DkR2)[*et al.*](http://paperpile.com/b/coPcdt/DkR2)[Large language models encode clinical knowledge.](http://paperpile.com/b/coPcdt/DkR2)[*Nature*](http://paperpile.com/b/coPcdt/DkR2)[620, 172–180 (2023).](http://paperpile.com/b/coPcdt/DkR2)

12. [Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on Medical Challenge Problems.](http://paperpile.com/b/coPcdt/7xsh)[*arXiv [cs.CL]*](http://paperpile.com/b/coPcdt/7xsh)[(2023).](http://paperpile.com/b/coPcdt/7xsh)

13. [Zack, T.](http://paperpile.com/b/coPcdt/SrkJ)[*et al.*](http://paperpile.com/b/coPcdt/SrkJ)[Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study.](http://paperpile.com/b/coPcdt/SrkJ)[*Lancet Digit. Health*](http://paperpile.com/b/coPcdt/SrkJ)[6, e12–e22 (2024).](http://paperpile.com/b/coPcdt/SrkJ)

14. [Chen, S.](http://paperpile.com/b/coPcdt/X0L4)[*et al.*](http://paperpile.com/b/coPcdt/X0L4)[Cross-Care: Assessing the healthcare implications of pre-training data on language model bias.](http://paperpile.com/b/coPcdt/X0L4)[*Neural Inf Process Syst*](http://paperpile.com/b/coPcdt/X0L4)[abs/2405.05506, 23756–23795 (2024).](http://paperpile.com/b/coPcdt/X0L4)

15. [Hager, P.](http://paperpile.com/b/coPcdt/IXst)[*et al.*](http://paperpile.com/b/coPcdt/IXst)[Evaluation and mitigation of the limitations of large language models in clinical decision-making.](http://paperpile.com/b/coPcdt/IXst)[*Nat. Med.*](http://paperpile.com/b/coPcdt/IXst)[30, 2613–2622 (2024).](http://paperpile.com/b/coPcdt/IXst)

16. [McCoy, L. G., Manrai, A. K. & Rodman, A. Large language models and the degradation of the medical record.](http://paperpile.com/b/coPcdt/d6qh)[*N. Engl. J. Med.*](http://paperpile.com/b/coPcdt/d6qh)[391, 1561–1564 (2024).](http://paperpile.com/b/coPcdt/d6qh)

17. [Goetz, L., Seedat, N., Vandersluis, R. & van der Schaar, M. Generalization-a key challenge for responsible AI in patient-facing clinical applications.](http://paperpile.com/b/coPcdt/RJ2T)[*NPJ Digit. Med.*](http://paperpile.com/b/coPcdt/RJ2T)[7, 126 (2024).](http://paperpile.com/b/coPcdt/RJ2T)

18. [Armitage, R. C. Implications of large language models for clinical practice: Ethical analysis through the principlism framework.](http://paperpile.com/b/coPcdt/k02J)[*J. Eval. Clin. Pract.*](http://paperpile.com/b/coPcdt/k02J)[31, e14250 (2025).](http://paperpile.com/b/coPcdt/k02J)

19. [Chen, S.](http://paperpile.com/b/coPcdt/rgaJ)[*et al.*](http://paperpile.com/b/coPcdt/rgaJ)[When helpfulness backfires: LLMs and the risk of false medical information due to sycophantic behavior.](http://paperpile.com/b/coPcdt/rgaJ)[*NPJ Digit. Med.*](http://paperpile.com/b/coPcdt/rgaJ)[8, 605 (2025).](http://paperpile.com/b/coPcdt/rgaJ)

20. [Williams, C. Y. K., Miao, B. Y., Kornblith, A. E. & Butte, A. J. Evaluating the use of large language models to provide clinical recommendations in the Emergency Department.](http://paperpile.com/b/coPcdt/kiiZ)[*Nat. Commun.*](http://paperpile.com/b/coPcdt/kiiZ)[15, 8236 (2024).](http://paperpile.com/b/coPcdt/kiiZ)

21. [Arora, R. K.](http://paperpile.com/b/coPcdt/ESmj)[*et al.*](http://paperpile.com/b/coPcdt/ESmj)[HealthBench: Evaluating large language models towards improved human health.](http://paperpile.com/b/coPcdt/ESmj)[https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf?utm_source=chatgpt.com)[.](http://paperpile.com/b/coPcdt/ESmj)

22. [Yan, L. K. Q.](http://paperpile.com/b/coPcdt/9dPU)[*et al.*](http://paperpile.com/b/coPcdt/9dPU)[Large language model benchmarks in medical tasks.](http://paperpile.com/b/coPcdt/9dPU)[*ArXiv*](http://paperpile.com/b/coPcdt/9dPU)[abs/2410.21348, (2024).](http://paperpile.com/b/coPcdt/9dPU)

23. [Wang, D. & Zhang, S. Large language models in medical and healthcare fields: applications, advances, and challenges.](http://paperpile.com/b/coPcdt/Scnx)[*Artif. Intell. Rev.*](http://paperpile.com/b/coPcdt/Scnx)[57, (2024).](http://paperpile.com/b/coPcdt/Scnx)

Proposition

## Proposition

1. It is time for the second half of the AI development, evaluations, and experiences. - paraphrase from Richard S. Sutton and Shunyu Yao
2. Three key measurements of an ideal benchmark: natural, automatically evaluatable, and challenging. - paraphrase from Ofir Press and Percy Liang
3. Optimizing clinical NLP benchmarks for leaderboard scores without testing real-world variations (like demographics, language, and synonyms) is dangerous because it prioritizes performance metrics over actual patient safety.
4. In real clinical workflows, LLMs create value only when verification is engineered into the system now: high-risk claims must be surfaced and made easy for clinicians to spot, check, and correct.
5. Key virtues we desire AI to have: curiosity, truth-seeking, and care for sentient lives. - reflection from Ilya Sutskever and Isaac Asimov
6. For clinical AI, usefulness should be defined as "time saved without hidden risk," not "time saved on average." Efficiency gains that mask low-frequency, high-severity errors redistribute burden invisibly.
7. For LLMs, downstream predictions often track pre-training co-occurrence statistics more closely. This is fundamentally a data problem before it becomes a model problem.
8. We should be able to predict model behavior from internals and build useful safety tools. - inspired by Neel Nanda

Glossary

## Glossary

- **SAE (Sparse Autoencoder)** A neural architecture that learns sparse, part-based representations from hidden states. Used for interpretability, feature decomposition, and downstream probing.
- **RAG (Retrieval-Augmented Generation)** A framework that integrates a retriever with a generative model so outputs can condition on, cite, and reason over external context.
- **SFT (Supervised Fine-Tuning)** Gradient-based (usually with cross-entropy) training on instruction–response pairs to adapt or align a base model toward desired behaviors.
- **PPO (Proximal Policy Optimization)** A reinforcement learning algorithm using clipped policy updates; forms the basis of many preference-optimization approaches for LLMs.
- **GRPO / RLVR / DAPO (Verification-Based RL)** PPO-style RL methods that optimize rewards based on correctness, formatting, preference signals, and verifiable outputs without having a reward or value model.
- **LoRA (Low-Rank Adaptation)** A parameter-efficient fine-tuning technique patching addiational low-rank parameter addition to a model’s weight matrices, while keeping base weights mostly frozen.
- **Quantization (e.g., 8-bit, 4-bit)** Reducing numerical precision of model weights/activations to lower memory and compute cost, usually with minimal performance loss.
- **ICL (In-Context Learning)** Guiding the model via examples or demonstrations placed directly in the prompt, without updating model parameters.
- **CoT (Chain-of-Thought)** Explicit intermediate reasoning steps produced by the model to show its logical or mathematical process.
- **Reasoning-Language Alignment / Language-Consistency** Ensuring that the model’s full reasoning trace, not just the final answer, remains in the user’s language.
- **Reasoning Faithfulness** The degree to which intermediate steps causally contribute to the model's final answer, rather than being hallucinated or post-hoc rationalizations.
- **Model Merging** A post-hoc technique that combines parameters from separately trained models to blend capabilities without full retraining.
- **Linear Probe** A simple linear classifier trained on frozen representations (e.g., SAE features) to measure what information is encoded in the model’s internal states.
- **Pass@k** The probability that at least one out of *k* sampled solutions is correct; widely used in code generation and mathematical evaluation.
- **PVI / In-Context PVI** Measures of “usable information” for a fixed model on a given instance; the in-context variant estimates PVI through prompting (useful for API-only settings).
- **Agentic System / AI Agent** An LLM-based system that coordinates multi-step reasoning, tool use, external retrieval, memory, and verification to autonomously complete complex tasks (e.g., the iRAE agent for adverse-event extraction).
- **Benchmark Contamination** The leakage of test/validation items into pre-training corpora, which inflates reported performance by allowing memorization instead of genuine reasoning.
- **Biomedical NLP** Natural language processing applied to medical or scientific text (clinical notes, literature, guidelines), supporting tasks such as information extraction, classification, and summarization.
- **CTCAE (Common Terminology Criteria for Adverse Events)** A standardized lexicon and grading system for identifying and scoring treatment toxicities, especially in oncology.
- **Electronic Health Records (EHR)** Digital patient records combining structured data and unstructured clinical notes; a primary source for many medical LLM applications.
- **Few-Shot / Zero-Shot Prompting** Techniques where models are guided with a few examples (few-shot) or only an instruction (zero-shot) to perform tasks without fine-tuning.
- **Guideline Concordance** The extent to which AI-generated recommendations align with clinical practice guidelines (e.g., NCCN), an important clinical safety metric.
- **Hallucination (Medical AI)** The generation of factually incorrect, unsafe, or non–evidence-based medical information, such as nonexistent treatments or incorrect diagnoses.
- **Immunotherapy-Related Adverse Events (irAE)** Toxicities stemming from immune checkpoint inhibitors; often require expert interpretation and can be extracted automatically using systems such as the iRAE agent.
- **Multi-Hop Reasoning** Combining information across multiple facts, documents, or reasoning steps to answer complex queries, as in tasks like MedBrowseComp.
- **Multimodal Evaluation** Assessment of models that jointly process text and other modalities (e.g., radiology images, pathology slides, PDFs).
- **Parameter-Efficient Fine-Tuning (PEFT)** A family of techniques including LoRA, QLoRA, and AdaLoRA, that update only a small subset of parameters to reduce compute and memory costs.
- **Patient Portal Messages** Secure communications between patients and clinicians. Increasingly used in studies of LLM-assisted triage, drafting, and workflow support.
- **Real-World Evidence (RWE)** Insight derived from routine clinical practice (EHR, claims) rather than controlled trials; LLMs enable large-scale structuring of unstructured RWE sources.
- **Representational Bias** Systematic distortions in model representations or outputs reflecting imbalances in pre-training data—for example, uneven demographic associations.
- **Robustness Testing** Evaluating model stability under perturbations such as demographic shifts, drug-name swaps, or adversarial edits to identify brittleness.
- **Silver-Labeled Data** Limited gold-standard annotations can be augmented using labels derived from various sources, such as heuristics, structured fields, or auxiliary models.
- **Social Determinants of Health (SDoH)** Non-clinical factors (housing, employment, transportation, social support) that influence outcomes; often extracted from clinical notes.
- **Sycophancy / Sycophantic Behavior** A model's inclination to prioritize perceived helpfulness over factual accuracy, often leading it to assent to user suggestions, even erroneous ones.
- **Synthetic Data Augmentation** Utilizing LLMs to create synthetic yet realistic training data for enhancing model performance, particularly in scenarios characterized by limited resources or data imbalance.
