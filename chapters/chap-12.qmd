# Chapter 12: MedBrowseComp: Benchmarking Medical Deep Research and Computer Use

---
*Shan Chen\****, Pedro Moreira\*, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, Danielle S Bitterman

     Under Review at The Lancet Digital Health
Summary
Background
While general-purpose LLMs are increasingly touted as tools for clinical support, most evaluations focus on single-step fact retrieval rather than the complex multi-hop evidence synthesis required in real-world medicine. This gap raises concerns about their true suitability for clinical decision-making.
Methods
We introduce MedBrowseComp — a benchmark comprising over 1,000 human-curated medical questions reflecting realistic clinical scenarios where practitioners must navigate fragmented evidence from trials, regulations, and cost data to reach updated conclusions. Unlike prior benchmarks, this dataset forces agents to browse live, domain-specific knowledge bases, perform multi-step reasoning, and ground answers in verifiable evidence. We then evaluated state-of-the-art agentic systems on this benchmark to assess performance gaps.
Findings
Agent systems scored as low as \~10% accuracy on the hardest subsets of questions, with overall performance well below the required rigor for clinical settings. Our results expose a substantial capability gap between current LLM-based browsing/retrieval agents and the demands of safe, reliable medical reasoning.
Interpretation
MedBrowseComp underscores that even cutting-edge agentic LLM systems remain far from being dependable tools for medical research or clinical decision support. The benchmark provides a clear testbed and targets for future development of retrieval-reasoning architectures, tool-augmented agents, and evidence-grounded systems in medicine.

## INTRODUCTION

LLMs have now effectively saturated most static, knowledge-based benchmarks, diminishing the value of these tasks to provide new insights and push the field forward[1–5](https://paperpile.com/c/Y40emr/HNVqF+hbXRy+ouQfI+Caw8P+a7HKr). However, this evolution exposes an evaluation gap: legacy leaderboards track static knowledge recall, whereas agentic systems should plan, browse, and synthesize fresh evidence in real time. To move beyond this plateau, the community is pivoting toward agentic systems that actively browse the web, retrieve real-time evidence, and reason over information outside their frozen parameter memories [6–8](https://paperpile.com/c/Y40emr/uMfMN+qMmet+S5hSb). The progression from chatbots to reasoners and ultimately to autonomous agents promises to enable access to real-time data, to allow models to tackle questions outside their pretraining knowledge, and perform complex information gathering tasks previously exclusive to humans[9–11](https://paperpile.com/c/Y40emr/ix5ju+4FuRc+LVq9d).

The potential impact of web-enabled agents is enormous. In principle, a sufficiently-capable AI agent should be able to retrieve any well-specified fact from reliable sources on the open web, even if doing so requires navigating thousands of pages[12–14](https://paperpile.com/c/Y40emr/lHveh+HbwVn+0ttzp). This promise is especially compelling in medicine, where research, clinical decision support, and patient education all depend on the integration of the most current, specific, and accurate information from heterogeneous sources such as journal articles, clinical trial registries, treatment guidelines, and drug databases[15–20](https://paperpile.com/c/Y40emr/peTRl+46ikF+Z4lyR+tMNnU+BaqPg+9E9ZT) . Yet despite rapid progress, the community lacks a unified benchmark for systematically evaluating whether agents can perform such complex, multi-source medical retrieval at scale.

As LLMs become the backbone of autonomous agentic systems, rigorous, domain-specific evaluation is critical. Contemporary agent models frequently “hallucinate,” generating confident but unsubstantiated or factually incorrect statements[21](https://paperpile.com/c/Y40emr/hZkYV). In high-stakes fields like medicine, these errors can misinform clinicians or patients and erode trust. Therefore, benchmarks must test not only a model’s reasoning and navigation skills but also its ability to ground each answer in verifiable evidence[21–24](https://paperpile.com/c/Y40emr/FbPDH+hZkYV+ic5ew+uLbbZ). A robust framework that measures evidence-based accuracy, navigation efficiency, and citation fidelity would directly quantify how well agentic systems incorporate best available information, closing the gap between impressive demos and safe, real-world deployment.

Evaluating an agent’s deep research and computer use competence is fundamentally different from testing its ability to recall static facts. Popular medical benchmarks such as MMLU, MedQA, and WorldMedQA test information that can be memorized during pre-training or retrieved from a single authoritative page. Frontier models now achieve near-ceiling scores on these tasks, so the benchmarks can no longer distinguish state-of-the-art systems or quantify new progress[2,3,25–27](https://paperpile.com/c/Y40emr/hbXRy+XLtUM+Ceu4a+u0LHx+ouQfI). They are usually executed in closed or synthetic environments that sidestep the real-world hurdles of live web interaction—pagination, obsolete links, contradictory evidence, and shifting page layouts.

In medicine, clinicians and researchers must integrate the latest study results, guideline updates, and safety advisories scattered across heterogeneous sites. To understand and compare agents' abilities to augment such tasks, new deep research and computer use benchmarks reflective of real-world, up-to-date tasks are urgently needed. A benchmark that forces agents to conduct multi-hop, evidence-grounded searches on the open Web should therefore serve two complementary purposes: (1) Directly measure whether agents can navigate, filter, and reconcile real-world information. (2) Dynamically stress-test systems as underlying evidence evolves, long after static benchmarks saturate.

To address this gap, we introduce MedBrowseComp, a new benchmark specifically designed to evaluate the capabilities of AI agents performing complex information retrieval tasks within the medical domain via web browsing. MedBrowseComp measures an agent's ability to accurately navigate the web—including general web pages, specialized medical websites, databases, and potential document formats like wikis, PDFs—to locate verifiable medical facts from reliable sources.

MedBrowseComp's design is inspired by the BrowseComp benchmark[28](https://paperpile.com/c/Y40emr/aURUp); it focuses on fact-seeking questions where the answers are short, objective, and easily verifiable, simplifying the evaluation process and enhancing its reliability. We designed this challenging benchmark collaboratively with physicians using HemOnc.org, one of the largest structured wiki information resources maintained weekly by oncologists for the past 6 years. Importantly, the benchmark is designed to enable automated, dynamic updating as information resources and medical evidence evolves. State-of-the-art Deep Research systems and Computer Use Agents (CUA) in May 2025 achieve less than 50% accuracy overall on MedBrowseComp, with less than 10% in the two hardest sets of questions.
Contributions
The MedBrowseComp Dataset: A novel, curated collection of challenging medical fact-seeking questions, each requiring web browsing and resulting in a short, verifiable answer. MedBrowseComp is the pioneer in curating a comprehensive benchmark that utilizes linked domain knowledge.

Baseline Performance Analysis: An empirical evaluation of various state-of-the-art LLMs and agentic systems on MedBrowseComp, providing initial benchmarks and highlighting the specific difficulties encountered in medical information retrieval.

Demonstration of Capability Gaps: Evidence showing the gap between the capabilities of general-purpose browsing agents (given frontier models already saturated general benchmark like SimpleQA) and specialized skills on complex medical information-seeking tasks.
**Figure 1 |** Example question constructions for MedBrowseComp.
![][image66]
RELATED WORK
One of the first comprehensive efforts to advance AI evaluation for general-purpose tool use and web browsing is GAIA[29](https://paperpile.com/c/Y40emr/lPSXk). GAIA is a carefully curated testbed for “General AI Assistants,” combining tasks that require multi-modal input, open-ended reasoning, and the ability to query external tools. It is simple for humans, but it was hard for AI systems at the time. Subsequent work began to highlight the importance of structured, multi-step web traversal. WebWalker introduced a dual-agent framework that separately handles horizontal browsing across different webpages and deep vertical navigation through website hierarchies[30,31](https://paperpile.com/c/Y40emr/pCcJ4+ipOAO). Its companion benchmark, WebWalkerQA, comprises realistic multi-hop questions from domains like education and organizational websites, emphasizing the challenge of distributing information across intricate hyperlink structures. Around the same time, FRAMES took a different angle by systematically evaluating Retrieval-Augmented Generation (RAG) systems for factual correctness, retrieval quality, and reasoning[32](https://paperpile.com/c/Y40emr/MmVUd).  While WebWalker and FRAMES address structured exploration and pipeline analysis, SimpleQA focuses more narrowly on short-answer factual correctness, specifically targeting LLM hallucinations[33](https://paperpile.com/c/Y40emr/JKUaV). SimpleQA’s adversarial question collection and stringent answer verification make it an effective tool for measuring whether models can reliably produce grounded, non-invented responses. Despite its value in exposing factual flaws, SimpleQA’s tasks remain relatively easy to solve with basic web searches, limiting its utility for deeper or more specialized queries.

To test expert-level knowledge well beyond the range of average tasks, Humanity's Last Exam (HLE) provides a set of deeply specialized questions[5](https://paperpile.com/c/Y40emr/a7HKr). HLE intentionally targets areas where frontier models are known to have knowledge gaps, thereby illuminating the upper limits of AI comprehension across diverse academic disciplines. However, while HLE excels at assessing conceptual difficulty and depth of reasoning, it focuses less on web and knowledge-source navigation capabilities.
*Figure 2:** Overall workflow of the curation of MedBrowseComp.

![][image67]

*

BrowseComp and its offshoot BrowseComp-ZH push AI agents to perform complex web searches for difficult-to-find facts[34,35](https://paperpile.com/c/Y40emr/p7K9A+k66Ka). BrowseComp's 1,255 English questions use a reverse-engineered approach to ensure they are not trivially searchable: each was devised and combined from existing short answers. The leading OpenAI models have an overall accuracy lower than 10%, however AI systems with search functionalities perform better. BrowseComp-ZH extends this framework to the Chinese web, accounting for different linguistic structures, censorship constraints, and local data sources. Leading models still achieve below 10–20% on BrowseComp-ZH tasks, suggesting that domain- or region-specific peculiarities are another consideration of the difficulty of high-stakes information retrieval.

Taken together, these benchmarks outline a rapidly evolving space in AI applications, where goal-oriented web navigation, accurate retrieval, and robust reasoning remain underdeveloped in even the most capable models. MedBrowseComp bridges the gap by focusing on the specialized domain of medicine. Its 1000+ questions\[121\*5=605 deep research questions across five hops, 121\*4=484 computer use questions over four hops\] are designed to be both practically useful and challenging, demanding persistent exploration of reputable medical sources, correct interpretation of terminology, and alignment with evidence-based standards. Moreover, MedBrowseComp’s design is expandable, allowing for continual updates that track evolving medical knowledge and new online resources.

## METHODS

To construct the MedBrowseComp dataset, we leverage the comprehensive hematology and oncology database available from HemOnc.org and work with its editors. HemOnc.org is the largest freely available medical wiki in the field of hematology/oncology, established to address the challenge oncologists routinely face navigating complex treatment regimens and rapidly evolving standards of care. This comprehensive resource covers over 1,000 pages of specialized content, including more than 250 hematologic and oncologic conditions, 5,455 detailed treatment regimens, and 6,950 referenced clinical studies, all curated by physicians with verifications. The platform catalogs approved systemic antineoplastic therapy agents, supportive medications, standard-of-care regimens, and references to primary literature, organized within a standardized ontology framework available through the HemOnc Dataverse[36](https://paperpile.com/c/Y40emr/1CV59). Our first step involved cleaning anti-neoplastic regimen efficacy data, linking each case with corresponding PubMed publications, and associated clinical trial information sourced from ClinicalTrials.gov, with data collected up to April 2025\. The fully cleaned and structured version of this dataset has been publicly released on HuggingFace to facilitate broader community engagement, further development, and external validation. To avoid potential dataset contamination, we encoded our final test sets with shifts and byte-wise encoding, which you can decode using the script we provide on GitHub.

To create our specific evaluation questions, we narrowed our dataset by excluding trials linked to multiple PubMed publications to maintain clarity and verifiability. Subsequently, we integrated regimen-specific drug information with FDA Orange Book data as of April 2025\. To maintain data consistency, only trials with regimens containing drugs easily matched through standard generic regular expressions were included. A manual verification and deduplication process was conducted to ensure accuracy and reduce redundancy, culminating in a refined set of 121 trials. Each of these trials has clearly defined trial metadata, verified regimen efficacy data, detailed FDA drug approval information, and the corresponding financial market data obtained from Yahoo Finance API for associated stock pricing, as Figure 2 shows.

From this carefully curated dataset, we developed our benchmark designed explicitly to assess (1) autonomous CUA within one to two hops of HemOnc.org’s webpage, and (2) deep research agents.
Model Selection and other Details
System Selection Rationale for Deep Research Agent
We evaluate a range of systems, from models with easy API access and systems without API access, and one Computer Use Agent (CUA) system. For models with an easy API with accessible cost, we evaluated the full set, which we refer to as MedBrowseComp-605. For models without easy API acccess and/or inaccessible costs, we evaluate against a smaller set which we refer to as MedBrowseComp-50(Authors put each of the queries into each application, copied out the responses, and graded the final outputs). Detailed model/system descriptions are in the appendix. For answer verification we employed an automated judge powered by GPT 4.1 mini-2025-04-14. The judging prompt was adapted from existing refined evaluation. Two annotators manually answered  MedBrowseComp-50 and achieved 100% inter-annotator agreements and 98% agreement with GPT 4.1 mini.
Model Selection Rationale for Computer Use Agent
We select Anthropic's Computer Use (using Claude 3.7 Sonnet) for evaluating browsing capabilities because, among the major commercially available GUI agents at the time, it was the only one that simultaneously offer a documented, programmable API and sandboxed environment that could be reengineered to run large batches without manual oversight, enabling us to run our experiments at scale. Details on Claude's sandbox is provide in the following paragraph and the code is open sourced on our Github repository.

Other UI agents—such as Bytedance’s UI Tars, OpenAI's Operator, and Google's Project Mariner-were considered, but at the time of experimentation they either lacked comparable logging/automation features or required additional deployment effort that was beyond our project timeline.
Evaluation Architecture
CUA is still experimental: every UI action spawns a tool call and screenshot, adding range from 2–10k tokens and extra latency. As recent benchmarks and surveys note, this overhead quickly pushes long tasks toward the model’s context limit and amplifies selector failures, making deep navigation chains unreliable. Accordingly, we restrict our benchmark for UI agents to four tasks \- trial ID, second author, PMID, and start date — that can be retrieved in one or two hops from HemOnc.org, providing a challenging yet attainable benchmark for GUI agents.

We implement a distributed evaluation infrastructure by adapting Anthropic's CUA codebase, transforming it from an interactive web application into a scalable, fault-tolerant parallel processing system that can be easily evaluated on personal laptops.

The evaluation back-end is organized as three cooperating services:
(1) Prompt processor** \- a lightweight Python microservice that streams CSV encoded tasks into Claude 3.7 Sonnet, applying up to five exponential backoff retries for transient errors (HTTP 429, timeouts, 5xx).  **(2) Container orchestrator** \- a Docker / Compose layer that launches N identical Ubuntu22.04 containers, mounts a shared volume, and shards the prompt file so that each container runs an independent batch without crosstalk. We ran our container in a batch of 8 on one of the author's MacBooks throughout the experiment. **(3) Sampling loop** \- A sampling loop that calls each turn is capped at 4096 output tokens while the screenshots that are sent as input are only the three most recent, ensuring that the running context never exceeds 120k tokens \- well under Sonnet's 200k window.

Note that there is no limit on the number of actions the model can perform or time constraints. We retained Anthropic’s default Ubuntu sandbox exactly as shipped, so the model runs in the same environment used for training — maintaining the best navigation accuracy, reproducibility, and runtime stability. The only change was a minor adjustment to the system‑prompt that nudges the agent to act autonomously without asking a human for grounding or clarifications during execution, which is included in Appendix.

Lastly, we verified difficulty by running Gemini-2.0-Flash-001 zero-shot across five runs, where it achieved under 7% accuracy consistently on all sub-partitions of our benchmark questions with model just using its parameter knowledge without accessing any tools.
Model Selection and other Details
System Selection Rationale for Deep Research Agent — We evaluate a range of systems, from models with easy API access and systems without API access, and one Computer Use Agent (CUA) system. For models with an easy API with accessible cost, we evaluated the full set, which we refer to as MedBrowseComp-605. For models without easy API acccess and/or inaccessible costs, we evaluate against a smaller set which we refer to as MedBrowseComp-50. Detailed model/system descriptions are in the appendix. For answer verification we employed an automated judge powered by GPT 4.1 mini-2025-04-14. The judging prompt was adapted from existing refined evaluation templates. Two annotators manually answered MedBrowseComp-50 and achieved 100% inter-annotator agreements and 98% agreement with GPT 4.1 mini.

Model Selection Rationale for Computer Use Agent — We select Anthropic's Computer Use (using Claude 3.7 Sonnet) for evaluating browsing capabilities because, among the major commercially available GUI agents at the time, it was the only one that simultaneously offer a documented, programmable API and sandboxed environment that could be reengineered to run large batches without manual oversight, enabling us to run our experiments at scale. Other UI agents—such as Bytedance’s UI Tars, OpenAI's Operator, and Google's Project Mariner—were considered, but at the time of experimentation they either lacked comparable logging/automation features or required additional deployment effort.

Evaluation Architecture — CUA is still experimental: every UI action spawns a tool call and screenshot, adding range from 2–10k tokens and extra latency. We restrict our benchmark for UI agents to four tasks—trial ID, second author, PMID, and start date—that can be retrieved in one or two hops from HemOnc.org. We implement a distributed evaluation infrastructure by adapting Anthropic's CUA codebase into a scalable, fault-tolerant parallel processing system. The back-end comprises a prompt processor with retries, a Docker/Compose container orchestrator sharding inputs, and a sampling loop that caps context at \~120k tokens while retaining default Ubuntu sandbox settings and a lightly adjusted system prompt.

Lastly, we verified difficulty by running Gemini-2.0-Flash-001 zero-shot across five runs: under 7% accuracy consistently on all sub-partitions without tools.

## RESULTS

Table 1 summarizes accuracy on MedBrowseComp-50. Accuracy decays with hop count; deep research modes outperform single-shot search. Gains are most pronounced at 4–5 hops. System-wise, O3 deepresearch and Gemini-2.5-pro deepsearch trail only the ‘Best of 1’ upper bound (30/50). Cross-model sampling improves accuracy but increases cost, motivating more efficient unified agents.

*Table 1| Notes: “Hop Pattern” shows per-depth correct counts (1-hop → 2-hop → 3-hop → 4-hop → 5-hop; 10 questions each). If fewer numbers are shown, omitted depths had 0\. Cost notes omitted in Word table; see LaTeX for details. ‘Best of 1’ selects the best single answer across systems per question.*
Deep Research Agent Results
Table 1 summarizes accuracy on MedBrowseComp-50. Across all systems, performance decays monotonically with hop count, corroborating prior evidence that long-horizon web navigation remains an open challenge for frontier LLM agents. Nevertheless, deep research variants—agents that allow iterative browsing steps rather than a single query—had improved performance. For example, O3 deepresearch answers 25.5/50 questions correctly, a 34% relative gain over O3 search (19/50); Gemini-2.5-pro deepsearch shows 75% improvement over its single-shot analogue (24.5 vs. 14). These gains are most pronounced on the hardest 4- and 5-hop splits, where deep research agents more than double the baseline accuracy.

Consistent trends are observed in the MedBrowseComp-605 results, where we exclusively evaluate models utilizing parametric memories and the Retrieval-Augmented Generation (RAG). The performance of bare models is notably poor across the majority of tasks, which aligns with our intention to create a challenging benchmark. RAG improves overall performance, but its benefit diminishes with increasing hops. On MedBrowseComp-605, we observe the same core patterns when comparing “bare” parameter-only models—i.e., those relying exclusively on their internal (parametric) memory—with retrieval-augmented variants. In isolation, parameter-only systems struggle across nearly every hop depth, confirming that our benchmark delivers the intended level of difficulty. With RAG, search models demonstrate substantial gains on shallow questions (1- and 2-hop) except GPT4.1, Gemini2Flash holds 30% and 4% boost respectively. And 67% and 7.4% for Gemini2.5Pro. However, the utility of retrieval diminishes beyond the third hop: by the 4- and 5-hop levels, RAG provides virtually no additional benefit over the bare model. The detailed results of MedBrowseComp-50 and MedBrowseComp-605 are in Appendix A.

System-wise, O3 deepresearch and Gemini-2.5-pro deepsearch constitute the frontier, trailing only the upper bound of the 'Best of 1' (30/50) that selects post hoc the single best model/system answer per question. Unlike prior work showing that repeatedly sampling from a single system can boost performance, our cross-model, test-time compute extension demonstrates even greater gains in overall accuracy. However, the computational expense of querying multiple distinct agents for every question is substantial, underscoring the urgent need to develop more efficient, unified systems that deliver comparable or superior results with lower resource overhead. Their advantage over specialized retrieval systems such as Sonar Pro (10/50) or Perplexity deepresearch (20/50) may suggest that contemporary instruction-tuned LLMs can outperform purpose-built agentic pipelines when granted autonomous browsing. However, even the best system falls short of perfect accuracy, underscoring the need for research in planning, tool use, and hallucination suppression in complex biomedical information-seeking tasks. Appendix Table B shows some common error modes in examples.
Computer Use Agent Results
The agent performs best on finding the Second Author of the linked PubMed paper and Trial ID extraction, relatively low-hop tasks that benefit from predictable formatting and shallow navigation. In addition, this information is mostly found in close proximity on the same page, which likely explains their similar results. Start Date questions require navigating to ClinicalTrials.gov and correctly identifying structured metadata. Errors in these settings often stem from greedy field selection: trajectory analysis shows the agent tends to extract the first date it encounters—even if unrelated to trial start—rather than locating the dedicated “Study Start” field.

As we take one step further beyond Hemonc, asking for the matching PMID results in weakest performance, despite appearing structurally simple since the model just need to located the PMID on the page. Interestingly, this is not due to common visual brittleness or interface failures. Instead, the agent often returns a valid PMID for a plausible but incorrect paper. This suggests semantic confusion rather than surface-level noise. You can see a detailed common error modes sorted with explanations in Appendix Table 9\.

| Extraction Task | Accuracy (%) |

| :---- | :---- |

| Clinical Trial IDs | 33.88 |

| Start Date | 30.58 |

| Second Author | 36.36 |

| PMIDs | 11.57 |

*Table 2\. Anthropic Computer-Use performance on four HemOnc information-extraction tasks in the sandbox environment.*

To evaluate the impact of structured entrypoints on CUA performance (and to demonstrate defined workflow can lead to performance improvements), we re-ran the benchmark using the same Claude 3.7 Sonnet system but initialized each task from the HemOnc.org homepage. This strategy avoids ambiguity introduced by external search engines and leverages HemOnc.org's curated structure to simplify task execution.

| Extraction Task | With HemOnc Start | No Defined Start |

| :---- | :---- | :---- |

| PMIDs | 42.98 | 11.57 |

| Second Author | 46.28 | 36.36 |

| NCT | 39.67 | 33.88 |

| Start Date | 31.40 | 30.58 |

*Table 3\. Accuracy (%) of Claude 3.7 Computer-Use Agent on structured extraction tasks, with and without initialization from HemOnc.org homepage.*

Compared to the results reported in Table 2, these scores show notable improvement, particularly on PMID extraction, which increased from 11.57% to 42.98%. By starting from a high-quality structured resource, the agent appears less likely to encounter ambiguous links, irrelevant documents, or noisy intermediate pages.

We hypothesize several contributing factors:

* Reduced ambiguity at the root: Starting on HemOnc.org anchors the agent to a semantically dense, domain-verified hub. This prevents early misrouting or unnecessary detours caused by irrelevant or overly generic search results.

* Fewer hops, higher precision: Structured navigation from HemOnc.org often yields answers in one or two clicks. This minimizes the risk of cumulative tool-call errors, selector mismatches, or hallucinated document interpretations.

* Improved alignment with human workflows: Medical oncologists often use HemOnc.org as a first step for navigating oncology evidence. Our findings suggest that model workflows, like human ones, benefit from strong priors.

These findings suggest that GUI-agent accuracy can benefit significantly from constrained yet clinically meaningful entrypoints like HemOnc.org. Future work should explore formal initialization policies as part of web-agent pipelines.
Conclusion and Future Work
Our work has certain limitations:
System selection:
We evaluated only agents with publicly programmable APIs. As a result, promising but closed systems such as OpenAI Operator and Google’s Project Mariner were not evaluated. Other systems like Bytedance’s UITars were not tested due to time and computational constraints. However, it is not clear that their inclusion would shift the leaderboard substantially given their performance compared to Claude CUA on other benchmarks.
Human supervision:
All answers were machine‑judged with a lightly‑audited LLM rubric. However, a subset of answers were human-verified with good agreement compared to the LLM-as-judge as we described in the methods.
Compute and subscription cost:
Building the benchmark and running baselines is non‑trivial and our experiments cost a total of $3,690: $320 on Perplexity (of which $200 was Deep‑Research API calls), $450 on Gemini 2.5 pro, and about $2,500 on Claude Sonnet 3.7 CUA, $200 for a 1 month ChatGPT Pro subscription, $20 for advanced reasoning, and $200 for GPT‑4.1 mini judging.
Real‑world validation:
Finally, we did not place answers in front of clinicians, and the questions in MedBrowseComp only cover a small portion of the enormous medical knowledge base used for real-world clinical decision-making. However, expert-curated, verifiable knowledge is not available for the entire medical domain. Our focused benchmark enables deeper study of deep research and computer use capabilities and still reveals important gaps and future directions to inform the field.
Future Work
* Broader task suite: Expand beyond single‑field extraction to multi‑paragraph justification, guideline concordance, and financial/regulatory trend analysis, all of which require deeper reasoning.

* Tool‑augmented agents: Test whether lightweight adapters such as PDF parsers, table detectors, ClinicalTrials.gov wrappers boost agent performance.

* Inclusion of closed systems: Collaborate with companies to benchmark Operator, Mariner, and UITars under identical prompts, closing the current comparability gap.

* Test system in AI-IDE environment: Place agents in an AI-IDE sandbox (e.g., Claude Code CLI or Cursor or Windsurf) where they must draft, debug, and reuse their own helper scripts over our horizontal tasks.

* Study agents with a human-in-the-loop and in real-world settings: Compare agentic systems vs human performance on MedBrowseComp, and study top-performing systems in clinical settings.

* Prevent contamination during test set: Because benchmark artifacts are public, use allow-lists to block the repo/demo and permit only vetted domains; maintain a private hold-out, deduplicate across splits, encode released test sets, and require grounded citations to flag leakage as shown in figure 3\.
**Figure 3 |** Latest computer use agent showing direct access to the Hugging Face repo containing gold labels, illustrating the risk of contamination. However, this system did not read directly into the labeled dataset.

![][image68]
CONCLUSION
We introduced MedBrowseComp, the largest verifiable benchmark that evaluates deep research and computer use agents in the medical field. The tasks are clinically meaningful rather than contrived for difficulty. Experiments reveal a clear capability gap: retrieval‑augmented text pipelines answer nearly half as many questions as their deep research counterparts, yet no system, including computer use agents, exceeds 40% accuracy among questions that require more than a single hop. GUI‑centric agents can control browsers and desktops end‑to‑end, but they tend to underperform compared to deep researchers driven by APIs on our benchmark, partly due to context bloat from screenshots. MedBrowseComp offers a realistic, challenging test bed for future systems and a focused subset that remains challenging for CUA.
REFERENCES
1\.	[Guha, N. *et al.* LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. *arXiv \[cs.CL\]* (2023).](http://paperpile.com/b/Y40emr/HNVqF)
2\.	[Wang, Y. *et al.* MMLU-Pro: A more robust and challenging multi-task language understanding benchmark. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/hbXRy)
3\.	[Singh, S. *et al.* Global MMLU: Understanding and addressing cultural and linguistic biases in multilingual evaluation. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/ouQfI)
4\.	[Hendrycks, D. *et al.* Measuring massive multitask language understanding. *arXiv \[cs.CY\]* (2020).](http://paperpile.com/b/Y40emr/Caw8P)
5\.	[Phan, L. Humanity’s Last Exam. *Research Square* (2025) doi:](http://paperpile.com/b/Y40emr/a7HKr)[10.21203/rs.3.rs-6615297/v1](http://dx.doi.org/10.21203/rs.3.rs-6615297/v1)[.](http://paperpile.com/b/Y40emr/a7HKr)
6\.	[Dupplaw, D. P. *et al.* Information extraction from multimedia web documents: an open-source platform and testbed. *Int. J. Multimed. Inf. Retr.* **3**, 97–111 (2014).](http://paperpile.com/b/Y40emr/uMfMN)
7\.	[Vu, T. *et al.* FreshLLMs: Refreshing large language models with search engine augmentation. *arXiv \[cs.CL\]* (2023).](http://paperpile.com/b/Y40emr/qMmet)
8\.	[Alzubi, S. *et al.* Open Deep Search: Democratizing search with open-source reasoning agents. *arXiv \[cs.LG\]* (2025).](http://paperpile.com/b/Y40emr/S5hSb)
9\.	[Nakano, R. *et al.* WebGPT: Browser-assisted question-answering with human feedback. *arXiv \[cs.CL\]* (2021).](http://paperpile.com/b/Y40emr/ix5ju)
10\.	[Kasai, J. *et al.* RealTime QA: What’s the answer right now? *arXiv \[cs.CL\]* (2022).](http://paperpile.com/b/Y40emr/4FuRc)
11\.	[Wang, G. *et al.* Voyager: An open-ended embodied agent with large language models. *arXiv \[cs.AI\]* (2023).](http://paperpile.com/b/Y40emr/LVq9d)
12\.	[Deng, X. *et al.* Mind2Web: Towards a generalist agent for the web. *arXiv \[cs.CL\]* (2023).](http://paperpile.com/b/Y40emr/lHveh)
13\.	[Xu, Y. *et al.* Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/HbwVn)
14\.	[Pahuja, V. *et al.* Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents. *arXiv \[cs.AI\]* (2025).](http://paperpile.com/b/Y40emr/0ttzp)
15\.	[Wornow, M., Thapa, R., Steinberg, E., Fries, J. A. & Shah, N. H. EHRSHOT: An EHR benchmark for few-shot evaluation of foundation models. *arXiv \[cs.LG\]* (2023).](http://paperpile.com/b/Y40emr/peTRl)
16\.	[Chen, S. *et al.* Wait, but Tylenol is acetaminophen... Investigating and improving language models’ ability to resist requests for misinformation. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/46ikF)
17\.	[Chen, S. *et al.* Use of artificial intelligence chatbots for cancer treatment information. *JAMA Oncol.* **9**, 1459–1462 (2023).](http://paperpile.com/b/Y40emr/Z4lyR)
18\.	[Yu, H. *et al.* AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/tMNnU)
19\.	[Tu, T. *et al.* Towards Conversational Diagnostic AI. *arXiv \[cs.AI\]* (2024).](http://paperpile.com/b/Y40emr/BaqPg)
20\.	[Li, X. *et al.* MedGUIDE: Benchmarking clinical decision-making in Large Language Models. *arXiv \[cs.CL\]* (2025).](http://paperpile.com/b/Y40emr/9E9ZT)
21\.	[Kim, Y. *et al.* Medical hallucinations in foundation models and their impact on healthcare. *arXiv \[cs.CL\]* (2025).](http://paperpile.com/b/Y40emr/hZkYV)
22\.	[Holmes, J. *et al.* RadOnc-GPT: An autonomous LLM agent for real-time patient outcomes labeling at scale. *arXiv \[cs.AI\]* (2025).](http://paperpile.com/b/Y40emr/FbPDH)
23\.	[Gallifant, J. & Bitterman, D. S. Humanity’s next medical exam: Preparing to evaluate superhuman systems. *NEJM AI* **2**, (2025).](http://paperpile.com/b/Y40emr/ic5ew)
24\.	[Xiao, Y. *et al.* KScope: A framework for characterizing the knowledge status of language models. *arXiv \[cs.CL\]* (2025) doi:](http://paperpile.com/b/Y40emr/uLbbZ)[10.48550/arXiv.2506.07458](http://dx.doi.org/10.48550/arXiv.2506.07458)[.](http://paperpile.com/b/Y40emr/uLbbZ)
25\.	[Schmidgall, S. *et al.* AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments. *arXiv \[cs.HC\]* (2024).](http://paperpile.com/b/Y40emr/XLtUM)
26\.	[Gallifant, J. *et al.* Language models are surprisingly fragile to drug names in biomedical benchmarks. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/Ceu4a)
27\.	[Matos, J. *et al.* WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation. in *Findings of the Association for Computational Linguistics: NAACL 2025* (eds. Chiruzzo, L., Ritter, A. & Wang, L.) 7203–7216 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2025).](http://paperpile.com/b/Y40emr/u0LHx)
28\.	[Wei, J. *et al.* BrowseComp: A simple yet challenging benchmark for browsing agents. *arXiv \[cs.CL\]* (2025).](http://paperpile.com/b/Y40emr/aURUp)
29\.	[Mialon, G. *et al.* GAIA: a benchmark for General AI Assistants. *arXiv \[cs.CL\]* (2023).](http://paperpile.com/b/Y40emr/lPSXk)
30\.	[Wu, J. *et al.* WebWalker: Benchmarking LLMs in Web Traversal. *arXiv \[cs.CL\]* (2025).](http://paperpile.com/b/Y40emr/pCcJ4)
31\.	[Xie, T. *et al.* OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. *arXiv \[cs.AI\]* (2024).](http://paperpile.com/b/Y40emr/ipOAO)
32\.	[Akkiraju, R. *et al.* FACTS about building retrieval Augmented Generation-based chatbots. *arXiv \[cs.LG\]* (2024).](http://paperpile.com/b/Y40emr/MmVUd)
33\.	[Wei, J. *et al.* Measuring short-form factuality in large language models. *arXiv \[cs.CL\]* (2024).](http://paperpile.com/b/Y40emr/JKUaV)
34\.	[Zhou, P. *et al.* BrowseComp-ZH: Benchmarking web browsing ability of large language models in Chinese. *arXiv \[cs.CL\]* (2025).](http://paperpile.com/b/Y40emr/p7K9A)
35\.	[Chen, Z. *et al.* BrowseComp-Plus: A more fair and transparent evaluation benchmark of deep-research agent. *arXiv \[cs.CL\]* (2025).](http://paperpile.com/b/Y40emr/k66Ka)
36\.	[Warner, J. L. *et al.* HemOnc: A new standard vocabulary for chemotherapy regimen representation in the OMOP common data model. *J. Biomed. Inform.* **96**, 103239 (2019).](http://paperpile.com/b/Y40emr/1CV59)

![][image69]

![][image70]

![][image71]

![][image72]

![][image73]

![][image74]

[image66]: ../figures/image66.png
[image67]: ../figures/image67.png
[image68]: ../figures/image68.png
[image69]: ../figures/image69.png
[image70]: ../figures/image70.png
[image71]: ../figures/image71.png
[image72]: ../figures/image72.png
[image73]: ../figures/image73.png
[image74]: ../figures/image74.png
